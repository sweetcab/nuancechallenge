{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, max_seq_len, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    seq_lens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        sentence=sentence[:max_seq_len]\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        # Padding\n",
    "        data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "        # Store original sequence length\n",
    "        seq_lens.append(len(token_ids))\n",
    "\n",
    "    return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "def process_data(datafile, max_vocab_size,max_seq_len):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, max_seq_len,normalize_digits=True)\n",
    "\n",
    "    return data_as_tokens, seq_lens, vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tar_token_ids, tar_seq_lens, tar_vocab_dict, tar_rev_vocab_dict = \\\n",
    "        process_data('test.p', max_vocab_size=5000,max_seq_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_token_ids = np.zeros(tar_token_ids.shape,dtype=np.int)\n",
    "src_seq_lens=tar_seq_lens.copy()\n",
    "\n",
    "for x in range(tar_token_ids.shape[0]):\n",
    "    for y in range(0, tar_token_ids.shape[1]):\n",
    "        if tar_token_ids[x, y]==6 or tar_token_ids[x, y]==11:\n",
    "            src_token_ids[x,y]=6 if random.random()<0.5 else 11\n",
    "        else:\n",
    "            src_token_ids[x,y]=tar_token_ids[x,y]\n",
    "    tar_token_ids[x,tar_seq_lens[x]]=EOS_ID\n",
    "    tar_seq_lens[x]+=1\n",
    "\n",
    "src_vocab_dict, src_rev_vocab_dict=tar_vocab_dict, tar_rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(en_token_ids, sp_token_ids,\n",
    "    en_seq_lens, sp_seq_len, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split the into train and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "    # Add go token to decoder inputs and create targets\n",
    "    for sentence in sp_token_ids:\n",
    "        decoder_inputs.append(np.array([GO_ID] + list(sentence)))\n",
    "        targets.append(np.array(([GO_ID] + list(sentence))[1:] + [0]))\n",
    "\n",
    "    sp_token_ids = np.array(decoder_inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Splitting index\n",
    "    last_train_index = int(0.8*len(en_token_ids))\n",
    "\n",
    "    train_encoder_inputs = en_token_ids[:last_train_index]\n",
    "    train_decoder_inputs = sp_token_ids[:last_train_index]\n",
    "    train_targets = targets[:last_train_index]\n",
    "    train_en_seq_lens = en_seq_lens[:last_train_index]\n",
    "    train_sp_seq_len = sp_seq_len[:last_train_index]\n",
    "\n",
    "    valid_encoder_inputs = en_token_ids[last_train_index:]\n",
    "    valid_decoder_inputs = sp_token_ids[last_train_index:]\n",
    "    valid_targets = targets[last_train_index:]\n",
    "    valid_en_seq_lens = en_seq_lens[last_train_index:]\n",
    "    valid_sp_seq_len = sp_seq_len[last_train_index:]\n",
    "\n",
    "    print(\"%i training samples and %i validations samples\" % (\n",
    "        len(train_encoder_inputs), len(valid_encoder_inputs)))\n",
    "\n",
    "    return train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "        train_en_seq_lens, train_sp_seq_len, \\\n",
    "        valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "        valid_en_seq_lens, valid_sp_seq_len\n",
    "\n",
    "def generate_epoch(encoder_inputs, decoder_inputs, targets, en_seq_lens, sp_seq_lens,\n",
    "    num_epochs, batch_size):\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        yield generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "            en_seq_lens, sp_seq_lens, batch_size)\n",
    "\n",
    "def generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "    en_seq_lens, sp_seq_lens, batch_size):\n",
    "\n",
    "    data_size = len(encoder_inputs)\n",
    "\n",
    "    num_batches = (data_size // batch_size)\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\n",
    "        yield encoder_inputs[start_index:end_index], \\\n",
    "            decoder_inputs[start_index:end_index], \\\n",
    "            targets[start_index:end_index], \\\n",
    "            en_seq_lens[start_index:end_index], \\\n",
    "            sp_seq_lens[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rnn_cell(FLAGS, dropout, scope):\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        # Get the cell type\n",
    "        if FLAGS.rnn_unit == 'rnn':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.BasicRNNCell\n",
    "        elif FLAGS.rnn_unit == 'gru':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "        elif FLAGS.rnn_unit == 'lstm':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise Exception(\"Choose a valid RNN unit type.\")\n",
    "\n",
    "        # Single cell\n",
    "        single_cell = rnn_cell_type(FLAGS.num_hidden_units)\n",
    "\n",
    "        # Dropout\n",
    "        single_cell = tf.nn.rnn_cell.DropoutWrapper(single_cell,\n",
    "            output_keep_prob=1-dropout)\n",
    "\n",
    "        # Each state as one cell\n",
    "        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [single_cell] * FLAGS.num_layers)\n",
    "\n",
    "    return stacked_cell\n",
    "\n",
    "def rnn_inputs(FLAGS, input_data, vocab_size, scope):\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_input = tf.get_variable(\"W_input\",\n",
    "            [vocab_size, FLAGS.num_hidden_units])\n",
    "\n",
    "    # embeddings will be shape [input_data dimensions, num_hidden units]\n",
    "    embeddings = tf.nn.embedding_lookup(W_input, input_data)\n",
    "    return embeddings\n",
    "\n",
    "def rnn_softmax(FLAGS, outputs, scope):\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_softmax = tf.get_variable(\"W_softmax\",\n",
    "            [FLAGS.num_hidden_units, FLAGS.tar_vocab_size])\n",
    "        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.tar_vocab_size])\n",
    "\n",
    "    logits = tf.matmul(outputs, W_softmax) + b_softmax\n",
    "    return logits\n",
    "\n",
    "class model(object):\n",
    "\n",
    "    def __init__(self, FLAGS):\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoder_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='encoder_inputs')\n",
    "        self.decoder_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='decoder_inputs')\n",
    "        self.inference_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='inference_inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='targets')\n",
    "        self.src_seq_lens = tf.placeholder(tf.int32, shape=[None, ],\n",
    "            name=\"src_seq_lens\")\n",
    "        self.tar_seq_lens = tf.placeholder(tf.int32, shape=[None, ],\n",
    "            name=\"tar_seq_lens\")\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "\n",
    "            # Encoder RNN cell\n",
    "            self.encoder_stacked_cell = rnn_cell(FLAGS, self.dropout,\n",
    "                scope=scope)\n",
    "\n",
    "            # Embed encoder inputs\n",
    "            W_input = tf.get_variable(\"W_input\",\n",
    "                [FLAGS.src_vocab_size, FLAGS.num_hidden_units])\n",
    "            self.embedded_encoder_inputs = rnn_inputs(FLAGS,\n",
    "                self.encoder_inputs, FLAGS.src_vocab_size, scope=scope)\n",
    "            #initial_state = encoder_stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_encoder_outputs, self.encoder_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.encoder_stacked_cell,\n",
    "                inputs=self.embedded_encoder_inputs,\n",
    "                sequence_length=self.src_seq_lens, time_major=False,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        '''\n",
    "        # Convert to list of tensors\n",
    "        self.encoder_outputs = tf.unpack(self.all_outputs, axis=0) # annotations\n",
    "        self.encoder_state = tf.unpack(self.state, axis=0)\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        self.top_states = [tf.reshape(e, [-1, 1,\n",
    "            self.stacked_cell.output_size]) for e in self.encoder_outputs]\n",
    "        self.attention_states = tf.concat(1, self.top_states)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # Decoder (use last relevant state from encoder as initial state)\n",
    "        self.initial_decoder_state = self.encoder_state[0]\n",
    "\n",
    "        '''\n",
    "\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "\n",
    "            # Initial state is last relevant state from encoder\n",
    "            self.decoder_initial_state = self.encoder_state\n",
    "\n",
    "            # Decoder RNN cell\n",
    "            self.decoder_stacked_cell = rnn_cell(FLAGS, self.dropout,\n",
    "                scope=scope)\n",
    "\n",
    "            # Embed decoder RNN inputs\n",
    "            W_input = tf.get_variable(\"W_input\",\n",
    "                [FLAGS.tar_vocab_size, FLAGS.num_hidden_units])\n",
    "            self.embedded_decoder_inputs = rnn_inputs(FLAGS, self.decoder_inputs,\n",
    "                FLAGS.tar_vocab_size, scope=scope)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_decoder_outputs, self.decoder_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.decoder_stacked_cell,\n",
    "                inputs=self.embedded_decoder_inputs,\n",
    "                sequence_length=self.tar_seq_lens, time_major=False,\n",
    "                initial_state=self.decoder_initial_state)\n",
    "\n",
    "            # Softmax on decoder RNN outputs\n",
    "            W_softmax = tf.get_variable(\"W_softmax\",\n",
    "                [FLAGS.num_hidden_units, FLAGS.tar_vocab_size])\n",
    "            b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.tar_vocab_size])\n",
    "\n",
    "            # Logits\n",
    "            self.decoder_outputs_flat = tf.reshape(self.all_decoder_outputs,\n",
    "                [-1, FLAGS.num_hidden_units])\n",
    "            self.logits_flat = rnn_softmax(FLAGS, self.decoder_outputs_flat,\n",
    "                scope=scope)\n",
    "\n",
    "            # Loss with masking\n",
    "            targets_flat = tf.reshape(self.targets, [-1])\n",
    "            losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits_flat, labels=targets_flat)\n",
    "            mask = tf.sign(tf.to_float(targets_flat))\n",
    "            masked_losses = mask * losses_flat\n",
    "            masked_losses = tf.reshape(masked_losses,  tf.shape(self.targets))\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(masked_losses, reduction_indices=1))\n",
    "            \n",
    "        with tf.variable_scope('decoder',reuse=True) as scope:\n",
    "\n",
    "            # Initial state is last relevant state from encoder\n",
    "            self.inference_initial_state = self.encoder_state\n",
    "\n",
    "            self.embedded_inference_inputs = rnn_inputs(FLAGS, self.inference_inputs,\n",
    "                FLAGS.tar_vocab_size, scope=scope)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_inference_outputs, self.inference_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.decoder_stacked_cell,\n",
    "                inputs=self.embedded_inference_inputs,\n",
    "                sequence_length=self.tar_seq_lens, time_major=False,\n",
    "                initial_state=self.inference_initial_state)\n",
    "\n",
    "            # Logits\n",
    "            self.inference_outputs_flat = tf.reshape(self.all_inference_outputs,\n",
    "                [-1, FLAGS.num_hidden_units])\n",
    "            self.inference_logits_flat = rnn_softmax(FLAGS, self.inference_outputs_flat,\n",
    "                scope=scope)\n",
    "\n",
    "        # Optimization\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        # clip the gradient to avoid vanishing or blowing up gradients\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_optimizer = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_vars))\n",
    "\n",
    "    def step(self, sess, FLAGS, batch_encoder_inputs, batch_decoder_inputs,\n",
    "        batch_targets, batch_en_seq_lens, batch_sp_seq_lens, dropout):\n",
    "\n",
    "        input_feed = {self.encoder_inputs: batch_encoder_inputs,\n",
    "            self.decoder_inputs: batch_decoder_inputs,\n",
    "            self.targets: batch_targets,\n",
    "            self.src_seq_lens: batch_en_seq_lens,\n",
    "            self.tar_seq_lens: batch_sp_seq_lens,\n",
    "            self.dropout: dropout}\n",
    "        output_feed = [self.loss, self.train_optimizer]\n",
    "        outputs = sess.run(output_feed, input_feed)\n",
    "\n",
    "        return outputs[0], outputs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parameters(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Holds all the parameters for NMT.\n",
    "        \"\"\"\n",
    "        self.ckpt_dir = 'checkpoints/'\n",
    "        \n",
    "        self.max_src_vocab_size = 5000\n",
    "        self.max_tar_vocab_size = 5000\n",
    "\n",
    "        self.num_epochs = 2\n",
    "        self.batch_size = 4\n",
    "\n",
    "        self.rnn_unit = 'gru'\n",
    "        self.num_hidden_units = 500\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 1e-3\n",
    "        self.learning_rate_decay_factor = 0.99\n",
    "        self.max_gradient_norm = 5.0\n",
    "\n",
    "def create_model(sess, FLAGS):\n",
    "\n",
    "    tf_model = model(FLAGS)\n",
    "    print(\"Created a new model\")\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "def train(FLAGS):\n",
    "\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "        train_src_seq_lens, train_tar_seq_lens, \\\n",
    "        valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "        valid_src_seq_lens, valid_tar_seq_len = \\\n",
    "        split_data(src_token_ids, tar_token_ids, src_seq_lens, tar_seq_lens,\n",
    "            train_ratio=0.8)\n",
    "\n",
    "    # Update parameters\n",
    "    FLAGS.src_vocab_size = len(src_vocab_dict)\n",
    "    FLAGS.tar_vocab_size = len(tar_vocab_dict)\n",
    "\n",
    "    # Start session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Create new model or load old one\n",
    "        model = create_model(sess, FLAGS)\n",
    "\n",
    "        # Training begins\n",
    "        losses = []\n",
    "        for epoch_num, epoch in enumerate(generate_epoch(train_encoder_inputs,\n",
    "            train_decoder_inputs, train_targets,\n",
    "            train_src_seq_lens, train_tar_seq_lens,\n",
    "            FLAGS.num_epochs, FLAGS.batch_size)):\n",
    "\n",
    "            print(\"EPOCH: %i\" % (epoch_num))\n",
    "            # Decay learning rate\n",
    "            sess.run(tf.assign(model.lr, FLAGS.learning_rate * \\\n",
    "                (FLAGS.learning_rate_decay_factor ** epoch_num)))\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for batch_num, (batch_encoder_inputs, batch_decoder_inputs,\n",
    "                batch_targets, batch_src_seq_lens,\n",
    "                batch_tar_seq_lens) in enumerate(epoch):\n",
    "\n",
    "                loss, _ = model.step(sess, FLAGS,\n",
    "                    batch_encoder_inputs, batch_decoder_inputs, batch_targets,\n",
    "                    batch_src_seq_lens, batch_tar_seq_lens,\n",
    "                    FLAGS.dropout)\n",
    "\n",
    "                batch_loss.append(loss)\n",
    "            \n",
    "            print(np.mean(batch_loss))\n",
    "            losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Save checkpoint.\n",
    "        checkpoint = \"./best_model.ckpt\" \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, checkpoint)\n",
    "        print('Model Trained and Saved')\n",
    "        plt.plot(losses, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(FLAGS):\n",
    "\n",
    "    # Change FLAGS parameters\n",
    "    FLAGS.batch_size = 1\n",
    "    FLAGS.src_vocab_size = len(src_vocab_dict)\n",
    "    FLAGS.tar_vocab_size = len(tar_vocab_dict)\n",
    "    FLAGS.tar_max_len = max(src_seq_lens) + 1 # GO token\n",
    "\n",
    "    # Process sample sentence\n",
    "    inference_sentence = [\"I usually eat the very large salad.\"]\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(inference_sentence)):\n",
    "        tokenized.append(basic_tokenizer(inference_sentence[i]))\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, sample_src_seq_lens = data_to_token_ids(\n",
    "        tokenized, src_vocab_dict,max_seq_len=30 ,normalize_digits=True)\n",
    "\n",
    "    # make dummy_sp_inputs\n",
    "    dummy_tar_inputs = np.array([[GO_ID]])\n",
    "    sample_tar_seq_lens = np.array([len(dummy_tar_inputs)])\n",
    "\n",
    "    print(data_as_tokens)\n",
    "    print(sample_src_seq_lens)\n",
    "    print(dummy_tar_inputs)\n",
    "    print(sample_tar_seq_lens)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        infermodel = create_model(sess, FLAGS)\n",
    "        print(FLAGS.ckpt_dir)\n",
    "        ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)\n",
    "        if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Restoring old model parameters from %s\" %\n",
    "                             ckpt.model_checkpoint_path)\n",
    "            infermodel.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"No Model\")\n",
    "            \n",
    "        # Load trained model\n",
    "        input_feed = {infermodel.encoder_inputs: data_as_tokens,\n",
    "            infermodel.inference_inputs: dummy_tar_inputs,\n",
    "            infermodel.src_seq_lens: sample_src_seq_lens,\n",
    "            infermodel.tar_seq_lens: sample_tar_seq_lens,\n",
    "            infermodel.dropout: 1}\n",
    "        output_feed = [infermodel.inference_logits_flat]\n",
    "        outputs = sess.run(output_feed, input_feed)\n",
    "        print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 training samples and 4 validations samples\n",
      "Created a new model\n",
      "WARNING:tensorflow:From C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "EPOCH: 0\n",
      "85.8627\n",
      "EPOCH: 1\n",
      "82.6542\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27a807bb080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 41 13 14 72 89 92  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0]]\n",
      "[8]\n",
      "[[1]]\n",
      "[1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable encoder/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cb5d491bb465>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-aa289e71f140>\u001b[0m in \u001b[0;36minference\u001b[1;34m(FLAGS)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0minfermodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-59e0a55d7761>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(sess, FLAGS)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Created a new model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b4b6c93b3347>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, FLAGS)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m# Embed encoder inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             W_input = tf.get_variable(\"W_input\",\n\u001b[1;32m---> 77\u001b[1;33m                 [FLAGS.src_vocab_size, FLAGS.num_hidden_units])\n\u001b[0m\u001b[0;32m     78\u001b[0m             self.embedded_encoder_inputs = rnn_inputs(FLAGS,\n\u001b[0;32m     79\u001b[0m                 self.encoder_inputs, FLAGS.src_vocab_size, scope=scope)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m           use_resource=use_resource)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    662\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 664\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable encoder/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "inference(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
