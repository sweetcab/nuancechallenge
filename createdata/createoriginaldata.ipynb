{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ygao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sentences(datafile):\n",
    "    \"\"\"\n",
    "    We will break the raw text in datafile into\n",
    "    sentences. The nltk package will handle edge cases\n",
    "    (ex. Mr. Potter) and will give us the list of sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(datafile, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(filename,savename):\n",
    "\n",
    "    original_data = filename\n",
    "    # Break into sentences\n",
    "    original_sentences = make_sentences(original_data)\n",
    "    print(\"We have %i sentences.\" %(len(original_sentences)))\n",
    "    \n",
    "    print(original_sentences[0])\n",
    "    # Store sentences into data file\n",
    "    with open(savename, 'wb') as f:\n",
    "        pickle.dump(original_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5809 sentences.\n",
      "\"She was mightily pleased to have your message, when I gave it her.\n"
     ]
    }
   ],
   "source": [
    "prepare_data('A_tale_of_two_cities.100kwords.original.txt','originaltwotale.p',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5809 sentences.\n",
      "\"She was mightily pleased to have your message, when I gave it her.\n"
     ]
    }
   ],
   "source": [
    "prepare_data('A_tale_of_two_cities.100kwords.obfuscated.txt','obfuscatedtwotale.p',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict,normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    seq_lens = []\n",
    "    max_len = max(len(sentence) for sentence in tokenized)# \n",
    "    for sentence in tokenized:\n",
    "        sentence=sentence[:max_len]\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        # Padding\n",
    "        data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "        # Store original sequence length\n",
    "        seq_lens.append(len(token_ids))\n",
    "\n",
    "    return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "def process_data(datafile, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, normalize_digits=True)\n",
    "\n",
    "    return tokenized, seq_lens, vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text):\n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file_by_dict(filename,vocab_dict):\n",
    "    with open(filename, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "    \n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, normalize_digits=True)\n",
    "    \n",
    "    return tokenized,seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_original,len_original, vocab_to_int, int_to_vocab = \\\n",
    "        process_data('originaltwotale.p', max_vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_obfused,len_obfused=process_file_by_dict(\"obfuscatedtwotale.p\",vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5809, 5809)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_original), len(token_obfused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['\"',\n",
       "   'She',\n",
       "   'was',\n",
       "   'mightily',\n",
       "   'pleased',\n",
       "   'to',\n",
       "   'have',\n",
       "   'your',\n",
       "   'message',\n",
       "   ',',\n",
       "   'when',\n",
       "   'I',\n",
       "   'gave',\n",
       "   'it',\n",
       "   'her',\n",
       "   '.'],\n",
       "  ['Not',\n",
       "   'that',\n",
       "   'she',\n",
       "   'showed',\n",
       "   'she',\n",
       "   'was',\n",
       "   'pleased',\n",
       "   ',',\n",
       "   'but',\n",
       "   'I',\n",
       "   'suppose',\n",
       "   'she',\n",
       "   'was',\n",
       "   '.',\n",
       "   '\"']],\n",
       " [['\"',\n",
       "   'She',\n",
       "   'was',\n",
       "   'mightily',\n",
       "   'pleased',\n",
       "   'to',\n",
       "   'have',\n",
       "   'your',\n",
       "   'message',\n",
       "   ',',\n",
       "   'when',\n",
       "   'I',\n",
       "   'gave',\n",
       "   'it',\n",
       "   'her',\n",
       "   '.'],\n",
       "  ['Not',\n",
       "   'that',\n",
       "   'she',\n",
       "   'showed',\n",
       "   'she',\n",
       "   'was',\n",
       "   'pleased',\n",
       "   ',',\n",
       "   'but',\n",
       "   'I',\n",
       "   'suppose',\n",
       "   'she',\n",
       "   'was',\n",
       "   '.',\n",
       "   '\"']],\n",
       " array([16, 15, 30, 14, 17, 27, 12, 13, 10, 25]),\n",
       " array([16, 15, 30, 14, 17, 27, 12, 13, 10, 25]),\n",
       " 190)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_label[:2], token_obfused[:2],len_original[:10], len_obfused[:10],max(len_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('twotaledumped.p', 'wb') as f:\n",
    "        pickle.dump((token_original,len_original,token_obfused,len_obfused), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-fd1ce06d116c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompare_a_the\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmix\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-d03f5221cf14>\u001b[0m in \u001b[0;36mcompare_a_the\u001b[1;34m(x_text, y_text)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mtheid\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0maid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mtotal\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0my_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "mix,total=compare_a_the(int_label,int_text)\n",
    "mix/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
