{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Seq2seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, max_seq_len, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    seq_lens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        sentence=sentence[:max_seq_len]\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        # Padding\n",
    "        data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "        # Store original sequence length\n",
    "        seq_lens.append(len(token_ids))\n",
    "\n",
    "    return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "def process_data(datafile, max_vocab_size,max_seq_len):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, max_seq_len,normalize_digits=True)\n",
    "\n",
    "    return data_as_tokens, seq_lens, vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tar_token_ids, tar_seq_lens, tar_vocab_dict, tar_rev_vocab_dict = \\\n",
    "        process_data('original.p', max_vocab_size=8000,max_seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9129"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tar_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_token_ids = np.zeros(tar_token_ids.shape,dtype=np.int)\n",
    "src_seq_lens=tar_seq_lens.copy()\n",
    "\n",
    "for x in range(tar_token_ids.shape[0]):\n",
    "    for y in range(0, tar_token_ids.shape[1]):\n",
    "        if tar_token_ids[x, y]==tar_vocab_dict[\"a\"] or tar_token_ids[x, y]==tar_vocab_dict[\"the\"]:\n",
    "            src_token_ids[x,y]=tar_vocab_dict[\"a\"] if random.random()<0.5 else tar_vocab_dict[\"the\"]\n",
    "        else:\n",
    "            src_token_ids[x,y]=tar_token_ids[x,y]\n",
    "    tar_token_ids[x,tar_seq_lens[x]]=EOS_ID\n",
    "    tar_seq_lens[x]+=1\n",
    "src_vocab_dict, src_rev_vocab_dict=tar_vocab_dict, tar_rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source sequence\n",
      "[[   3  102  869 3344   14    6  725  483    4   33   27  170 1483   19\n",
      "    92   36 2728    9    3   45 5874    4    8    9   33   17   92 5332\n",
      "    63 7396  268    4   72   42   67    3  685    9  194    3    4  106\n",
      "    53  267   44    9 6713    4    6  491    0]\n",
      " [ 528   11  151   81  135   19   18 4175   62   40  466   10 1479    8\n",
      "   764    4   37   11  568 1090    4   19  504    6  348   10 1072  798\n",
      "   345   11  195   68    3    9  537  107  268   32   49   12   14   33\n",
      "   646   19   42 1240   76  101 2203   16    0]\n",
      " [1505   17  118   39  752    9 4867   16    6  128  857   14   11  491\n",
      "     4   42   14  570   11  194 2682    8    3 1249   16  108 1534    3\n",
      "    11 1488  128    4   17   82  301    9  134   16   14   40  699 2332\n",
      "     4   19   18    6  382  349   27   34    0]]\n",
      "\n",
      "\n",
      "Example target sequence\n",
      "[[   3  102  869 3344   14   11  725  483    4   33   27  170 1483   19\n",
      "    92   36 2728    9    3   45 5874    4    8    9   33   17   92 5332\n",
      "    63 7396  268    4   72   42   67    3  685    9  194    3    4  106\n",
      "    53  267   44    9 6713    4   11  491    2]\n",
      " [ 528   11  151   81  135   19   18 4175   62   40  466   10 1479    8\n",
      "   764    4   37    6  568 1090    4   19  504   11  348   10 1072  798\n",
      "   345    6  195   68    3    9  537  107  268   32   49   12   14   33\n",
      "   646   19   42 1240   76  101 2203   16    2]\n",
      " [1505   17  118   39  752    9 4867   16    6  128  857   14   11  491\n",
      "     4   42   14  570    6  194 2682    8    3 1249   16  108 1534    3\n",
      "    11 1488  128    4   17   82  301    9  134   16   14   40  699 2332\n",
      "     4   19   18    6  382  349   27   34    2]]\n",
      "(9129, 51)\n"
     ]
    }
   ],
   "source": [
    "source_letter_ids=src_token_ids\n",
    "target_letter_ids=tar_token_ids\n",
    "source_letter_to_int=src_vocab_dict\n",
    "target_letter_to_int=tar_vocab_dict\n",
    "source_int_to_letter=src_rev_vocab_dict\n",
    "target_int_to_letter=tar_rev_vocab_dict   \n",
    "\n",
    "print(\"Example source sequence\")\n",
    "print(source_letter_ids[:3])\n",
    "print(\"\\n\")\n",
    "print(\"Example target sequence\")\n",
    "print(target_letter_ids[:3])\n",
    "\n",
    "print(source_letter_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = max(\n",
    "        [len(sentence) for sentence in source_letter_ids] + [len(sentence) for sentence in target_letter_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 100\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the input we'll feed to the decoder\n",
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int[_GO]), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    # 1. Decoder Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    # 3. Dense layer to translate the decoder's output at each time \n",
    "    # step into a choice from the target vocabulary\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Set up a training decoder and an inference decoder\n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        # Basic decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           enc_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_sequence_length)[0]\n",
    "    # 5. Inference Decoder\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int[_GO]], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        # Helper for the inference process.\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                target_letter_to_int[_EOS])\n",
    "\n",
    "        # Basic decoder\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        enc_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
    "         \n",
    "\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    # Pass the input data through the encoder. We'll ignore the encoder output, but use the state\n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_letter_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "           \n",
    "        yield targets_batch, sources_batch, [51]*len(sources_batch), [51]*len(targets_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch   10/70 - Loss:  5.708  - Validation loss:  5.823\n",
      "Epoch   1/100 Batch   20/70 - Loss:  3.868  - Validation loss:  4.038\n",
      "Epoch   1/100 Batch   30/70 - Loss:  3.001  - Validation loss:  3.614\n",
      "Epoch   1/100 Batch   40/70 - Loss:  3.582  - Validation loss:  3.377\n",
      "Epoch   1/100 Batch   50/70 - Loss:  2.844  - Validation loss:  3.262\n",
      "Epoch   1/100 Batch   60/70 - Loss:  2.383  - Validation loss:  3.249\n",
      "Epoch   2/100 Batch   10/70 - Loss:  2.499  - Validation loss:  3.000\n",
      "Epoch   2/100 Batch   20/70 - Loss:  2.650  - Validation loss:  2.920\n",
      "Epoch   2/100 Batch   30/70 - Loss:  2.286  - Validation loss:  2.864\n",
      "Epoch   2/100 Batch   40/70 - Loss:  2.956  - Validation loss:  2.812\n",
      "Epoch   2/100 Batch   50/70 - Loss:  2.362  - Validation loss:  2.772\n",
      "Epoch   2/100 Batch   60/70 - Loss:  1.947  - Validation loss:  2.789\n",
      "Epoch   3/100 Batch   10/70 - Loss:  2.195  - Validation loss:  2.736\n",
      "Epoch   3/100 Batch   20/70 - Loss:  2.449  - Validation loss:  2.732\n",
      "Epoch   3/100 Batch   30/70 - Loss:  2.136  - Validation loss:  2.719\n",
      "Epoch   3/100 Batch   40/70 - Loss:  2.837  - Validation loss:  2.695\n",
      "Epoch   3/100 Batch   50/70 - Loss:  2.261  - Validation loss:  2.685\n",
      "Epoch   3/100 Batch   60/70 - Loss:  1.879  - Validation loss:  2.696\n",
      "Epoch   4/100 Batch   10/70 - Loss:  2.123  - Validation loss:  2.676\n",
      "Epoch   4/100 Batch   20/70 - Loss:  2.380  - Validation loss:  2.664\n",
      "Epoch   4/100 Batch   30/70 - Loss:  2.082  - Validation loss:  2.684\n",
      "Epoch   4/100 Batch   40/70 - Loss:  2.780  - Validation loss:  2.651\n",
      "Epoch   4/100 Batch   50/70 - Loss:  2.220  - Validation loss:  2.651\n",
      "Epoch   4/100 Batch   60/70 - Loss:  1.830  - Validation loss:  2.646\n",
      "Epoch   5/100 Batch   10/70 - Loss:  2.110  - Validation loss:  2.650\n",
      "Epoch   5/100 Batch   20/70 - Loss:  2.349  - Validation loss:  2.641\n",
      "Epoch   5/100 Batch   30/70 - Loss:  2.054  - Validation loss:  2.641\n",
      "Epoch   5/100 Batch   40/70 - Loss:  2.745  - Validation loss:  2.626\n",
      "Epoch   5/100 Batch   50/70 - Loss:  2.183  - Validation loss:  2.619\n",
      "Epoch   5/100 Batch   60/70 - Loss:  1.789  - Validation loss:  2.629\n",
      "Epoch   6/100 Batch   10/70 - Loss:  2.064  - Validation loss:  2.629\n",
      "Epoch   6/100 Batch   20/70 - Loss:  2.316  - Validation loss:  2.629\n",
      "Epoch   6/100 Batch   30/70 - Loss:  2.023  - Validation loss:  2.634\n",
      "Epoch   6/100 Batch   40/70 - Loss:  2.724  - Validation loss:  2.631\n",
      "Epoch   6/100 Batch   50/70 - Loss:  2.176  - Validation loss:  2.609\n",
      "Epoch   6/100 Batch   60/70 - Loss:  1.785  - Validation loss:  2.637\n",
      "Epoch   7/100 Batch   10/70 - Loss:  2.053  - Validation loss:  2.628\n",
      "Epoch   7/100 Batch   20/70 - Loss:  2.319  - Validation loss:  2.618\n",
      "Epoch   7/100 Batch   30/70 - Loss:  2.010  - Validation loss:  2.608\n",
      "Epoch   7/100 Batch   40/70 - Loss:  2.698  - Validation loss:  2.601\n",
      "Epoch   7/100 Batch   50/70 - Loss:  2.144  - Validation loss:  2.595\n",
      "Epoch   7/100 Batch   60/70 - Loss:  1.758  - Validation loss:  2.599\n",
      "Epoch   8/100 Batch   10/70 - Loss:  2.013  - Validation loss:  2.616\n",
      "Epoch   8/100 Batch   20/70 - Loss:  2.283  - Validation loss:  2.590\n",
      "Epoch   8/100 Batch   30/70 - Loss:  1.978  - Validation loss:  2.584\n",
      "Epoch   8/100 Batch   40/70 - Loss:  2.674  - Validation loss:  2.573\n",
      "Epoch   8/100 Batch   50/70 - Loss:  2.116  - Validation loss:  2.565\n",
      "Epoch   8/100 Batch   60/70 - Loss:  1.733  - Validation loss:  2.571\n",
      "Epoch   9/100 Batch   10/70 - Loss:  1.977  - Validation loss:  2.586\n",
      "Epoch   9/100 Batch   20/70 - Loss:  2.238  - Validation loss:  2.555\n",
      "Epoch   9/100 Batch   30/70 - Loss:  1.948  - Validation loss:  2.548\n",
      "Epoch   9/100 Batch   40/70 - Loss:  2.633  - Validation loss:  2.540\n",
      "Epoch   9/100 Batch   50/70 - Loss:  2.085  - Validation loss:  2.529\n",
      "Epoch   9/100 Batch   60/70 - Loss:  1.699  - Validation loss:  2.533\n",
      "Epoch  10/100 Batch   10/70 - Loss:  1.941  - Validation loss:  2.551\n",
      "Epoch  10/100 Batch   20/70 - Loss:  2.213  - Validation loss:  2.524\n",
      "Epoch  10/100 Batch   30/70 - Loss:  1.914  - Validation loss:  2.513\n",
      "Epoch  10/100 Batch   40/70 - Loss:  2.591  - Validation loss:  2.510\n",
      "Epoch  10/100 Batch   50/70 - Loss:  2.054  - Validation loss:  2.500\n",
      "Epoch  10/100 Batch   60/70 - Loss:  1.670  - Validation loss:  2.516\n",
      "Epoch  11/100 Batch   10/70 - Loss:  1.931  - Validation loss:  2.512\n",
      "Epoch  11/100 Batch   20/70 - Loss:  2.164  - Validation loss:  2.501\n",
      "Epoch  11/100 Batch   30/70 - Loss:  1.879  - Validation loss:  2.484\n",
      "Epoch  11/100 Batch   40/70 - Loss:  2.563  - Validation loss:  2.482\n",
      "Epoch  11/100 Batch   50/70 - Loss:  2.015  - Validation loss:  2.465\n",
      "Epoch  11/100 Batch   60/70 - Loss:  1.647  - Validation loss:  2.464\n",
      "Epoch  12/100 Batch   10/70 - Loss:  1.867  - Validation loss:  2.456\n",
      "Epoch  12/100 Batch   20/70 - Loss:  2.113  - Validation loss:  2.460\n",
      "Epoch  12/100 Batch   30/70 - Loss:  1.837  - Validation loss:  2.448\n",
      "Epoch  12/100 Batch   40/70 - Loss:  2.513  - Validation loss:  2.438\n",
      "Epoch  12/100 Batch   50/70 - Loss:  1.984  - Validation loss:  2.444\n",
      "Epoch  12/100 Batch   60/70 - Loss:  1.617  - Validation loss:  2.433\n",
      "Epoch  13/100 Batch   10/70 - Loss:  1.832  - Validation loss:  2.418\n",
      "Epoch  13/100 Batch   20/70 - Loss:  2.085  - Validation loss:  2.425\n",
      "Epoch  13/100 Batch   30/70 - Loss:  1.811  - Validation loss:  2.422\n",
      "Epoch  13/100 Batch   40/70 - Loss:  2.470  - Validation loss:  2.407\n",
      "Epoch  13/100 Batch   50/70 - Loss:  1.948  - Validation loss:  2.423\n",
      "Epoch  13/100 Batch   60/70 - Loss:  1.578  - Validation loss:  2.401\n",
      "Epoch  14/100 Batch   10/70 - Loss:  1.797  - Validation loss:  2.389\n",
      "Epoch  14/100 Batch   20/70 - Loss:  2.056  - Validation loss:  2.398\n",
      "Epoch  14/100 Batch   30/70 - Loss:  1.773  - Validation loss:  2.381\n",
      "Epoch  14/100 Batch   40/70 - Loss:  2.445  - Validation loss:  2.388\n",
      "Epoch  14/100 Batch   50/70 - Loss:  1.915  - Validation loss:  2.370\n",
      "Epoch  14/100 Batch   60/70 - Loss:  1.552  - Validation loss:  2.379\n",
      "Epoch  15/100 Batch   10/70 - Loss:  1.752  - Validation loss:  2.369\n",
      "Epoch  15/100 Batch   20/70 - Loss:  1.988  - Validation loss:  2.361\n",
      "Epoch  15/100 Batch   30/70 - Loss:  1.731  - Validation loss:  2.354\n",
      "Epoch  15/100 Batch   40/70 - Loss:  2.401  - Validation loss:  2.361\n",
      "Epoch  15/100 Batch   50/70 - Loss:  1.882  - Validation loss:  2.343\n",
      "Epoch  15/100 Batch   60/70 - Loss:  1.520  - Validation loss:  2.352\n",
      "Epoch  16/100 Batch   10/70 - Loss:  1.722  - Validation loss:  2.344\n",
      "Epoch  16/100 Batch   20/70 - Loss:  1.951  - Validation loss:  2.336\n",
      "Epoch  16/100 Batch   30/70 - Loss:  1.699  - Validation loss:  2.331\n",
      "Epoch  16/100 Batch   40/70 - Loss:  2.364  - Validation loss:  2.339\n",
      "Epoch  16/100 Batch   50/70 - Loss:  1.853  - Validation loss:  2.322\n",
      "Epoch  16/100 Batch   60/70 - Loss:  1.494  - Validation loss:  2.328\n",
      "Epoch  17/100 Batch   10/70 - Loss:  1.695  - Validation loss:  2.321\n",
      "Epoch  17/100 Batch   20/70 - Loss:  1.916  - Validation loss:  2.314\n",
      "Epoch  17/100 Batch   30/70 - Loss:  1.673  - Validation loss:  2.308\n",
      "Epoch  17/100 Batch   40/70 - Loss:  2.331  - Validation loss:  2.319\n",
      "Epoch  17/100 Batch   50/70 - Loss:  1.827  - Validation loss:  2.304\n",
      "Epoch  17/100 Batch   60/70 - Loss:  1.472  - Validation loss:  2.312\n",
      "Epoch  18/100 Batch   10/70 - Loss:  1.670  - Validation loss:  2.299\n",
      "Epoch  18/100 Batch   20/70 - Loss:  1.888  - Validation loss:  2.296\n",
      "Epoch  18/100 Batch   30/70 - Loss:  1.650  - Validation loss:  2.291\n",
      "Epoch  18/100 Batch   40/70 - Loss:  2.304  - Validation loss:  2.305\n",
      "Epoch  18/100 Batch   50/70 - Loss:  1.803  - Validation loss:  2.281\n",
      "Epoch  18/100 Batch   60/70 - Loss:  1.448  - Validation loss:  2.290\n",
      "Epoch  19/100 Batch   10/70 - Loss:  1.662  - Validation loss:  2.279\n",
      "Epoch  19/100 Batch   20/70 - Loss:  1.864  - Validation loss:  2.292\n",
      "Epoch  19/100 Batch   30/70 - Loss:  1.629  - Validation loss:  2.288\n",
      "Epoch  19/100 Batch   40/70 - Loss:  2.321  - Validation loss:  2.278\n",
      "Epoch  19/100 Batch   50/70 - Loss:  1.798  - Validation loss:  2.271\n",
      "Epoch  19/100 Batch   60/70 - Loss:  1.441  - Validation loss:  2.272\n",
      "Epoch  20/100 Batch   10/70 - Loss:  1.629  - Validation loss:  2.270\n",
      "Epoch  20/100 Batch   20/70 - Loss:  1.832  - Validation loss:  2.267\n",
      "Epoch  20/100 Batch   30/70 - Loss:  1.604  - Validation loss:  2.258\n",
      "Epoch  20/100 Batch   40/70 - Loss:  2.267  - Validation loss:  2.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/100 Batch   50/70 - Loss:  1.763  - Validation loss:  2.246\n",
      "Epoch  20/100 Batch   60/70 - Loss:  1.414  - Validation loss:  2.250\n",
      "Epoch  21/100 Batch   10/70 - Loss:  1.601  - Validation loss:  2.248\n",
      "Epoch  21/100 Batch   20/70 - Loss:  1.805  - Validation loss:  2.248\n",
      "Epoch  21/100 Batch   30/70 - Loss:  1.582  - Validation loss:  2.242\n",
      "Epoch  21/100 Batch   40/70 - Loss:  2.229  - Validation loss:  2.239\n",
      "Epoch  21/100 Batch   50/70 - Loss:  1.740  - Validation loss:  2.230\n",
      "Epoch  21/100 Batch   60/70 - Loss:  1.394  - Validation loss:  2.234\n",
      "Epoch  22/100 Batch   10/70 - Loss:  1.581  - Validation loss:  2.233\n",
      "Epoch  22/100 Batch   20/70 - Loss:  1.779  - Validation loss:  2.235\n",
      "Epoch  22/100 Batch   30/70 - Loss:  1.564  - Validation loss:  2.229\n",
      "Epoch  22/100 Batch   40/70 - Loss:  2.201  - Validation loss:  2.226\n",
      "Epoch  22/100 Batch   50/70 - Loss:  1.721  - Validation loss:  2.218\n",
      "Epoch  22/100 Batch   60/70 - Loss:  1.378  - Validation loss:  2.223\n",
      "Epoch  23/100 Batch   10/70 - Loss:  1.560  - Validation loss:  2.221\n",
      "Epoch  23/100 Batch   20/70 - Loss:  1.754  - Validation loss:  2.223\n",
      "Epoch  23/100 Batch   30/70 - Loss:  1.546  - Validation loss:  2.217\n",
      "Epoch  23/100 Batch   40/70 - Loss:  2.177  - Validation loss:  2.213\n",
      "Epoch  23/100 Batch   50/70 - Loss:  1.705  - Validation loss:  2.208\n",
      "Epoch  23/100 Batch   60/70 - Loss:  1.362  - Validation loss:  2.212\n",
      "Epoch  24/100 Batch   10/70 - Loss:  1.543  - Validation loss:  2.213\n",
      "Epoch  24/100 Batch   20/70 - Loss:  1.733  - Validation loss:  2.213\n",
      "Epoch  24/100 Batch   30/70 - Loss:  1.529  - Validation loss:  2.208\n",
      "Epoch  24/100 Batch   40/70 - Loss:  2.156  - Validation loss:  2.203\n",
      "Epoch  24/100 Batch   50/70 - Loss:  1.690  - Validation loss:  2.198\n",
      "Epoch  24/100 Batch   60/70 - Loss:  1.348  - Validation loss:  2.204\n",
      "Epoch  25/100 Batch   10/70 - Loss:  1.525  - Validation loss:  2.206\n",
      "Epoch  25/100 Batch   20/70 - Loss:  1.717  - Validation loss:  2.206\n",
      "Epoch  25/100 Batch   30/70 - Loss:  1.517  - Validation loss:  2.201\n",
      "Epoch  25/100 Batch   40/70 - Loss:  2.139  - Validation loss:  2.196\n",
      "Epoch  25/100 Batch   50/70 - Loss:  1.679  - Validation loss:  2.190\n",
      "Epoch  25/100 Batch   60/70 - Loss:  1.337  - Validation loss:  2.198\n",
      "Epoch  26/100 Batch   10/70 - Loss:  1.511  - Validation loss:  2.199\n",
      "Epoch  26/100 Batch   20/70 - Loss:  1.696  - Validation loss:  2.205\n",
      "Epoch  26/100 Batch   30/70 - Loss:  1.502  - Validation loss:  2.194\n",
      "Epoch  26/100 Batch   40/70 - Loss:  2.127  - Validation loss:  2.196\n",
      "Epoch  26/100 Batch   50/70 - Loss:  1.665  - Validation loss:  2.188\n",
      "Epoch  26/100 Batch   60/70 - Loss:  1.327  - Validation loss:  2.200\n",
      "Epoch  27/100 Batch   10/70 - Loss:  1.505  - Validation loss:  2.192\n",
      "Epoch  27/100 Batch   20/70 - Loss:  1.679  - Validation loss:  2.193\n",
      "Epoch  27/100 Batch   30/70 - Loss:  1.487  - Validation loss:  2.188\n",
      "Epoch  27/100 Batch   40/70 - Loss:  2.115  - Validation loss:  2.186\n",
      "Epoch  27/100 Batch   50/70 - Loss:  1.652  - Validation loss:  2.178\n",
      "Epoch  27/100 Batch   60/70 - Loss:  1.316  - Validation loss:  2.198\n",
      "Epoch  28/100 Batch   10/70 - Loss:  1.492  - Validation loss:  2.198\n",
      "Epoch  28/100 Batch   20/70 - Loss:  1.674  - Validation loss:  2.210\n",
      "Epoch  28/100 Batch   30/70 - Loss:  1.478  - Validation loss:  2.181\n",
      "Epoch  28/100 Batch   40/70 - Loss:  2.106  - Validation loss:  2.176\n",
      "Epoch  28/100 Batch   50/70 - Loss:  1.661  - Validation loss:  2.218\n",
      "Epoch  28/100 Batch   60/70 - Loss:  1.354  - Validation loss:  2.200\n",
      "Epoch  29/100 Batch   10/70 - Loss:  1.483  - Validation loss:  2.197\n",
      "Epoch  29/100 Batch   20/70 - Loss:  1.686  - Validation loss:  2.181\n",
      "Epoch  29/100 Batch   30/70 - Loss:  1.487  - Validation loss:  2.187\n",
      "Epoch  29/100 Batch   40/70 - Loss:  2.085  - Validation loss:  2.176\n",
      "Epoch  29/100 Batch   50/70 - Loss:  1.644  - Validation loss:  2.174\n",
      "Epoch  29/100 Batch   60/70 - Loss:  1.302  - Validation loss:  2.176\n",
      "Epoch  30/100 Batch   10/70 - Loss:  1.470  - Validation loss:  2.169\n",
      "Epoch  30/100 Batch   20/70 - Loss:  1.659  - Validation loss:  2.180\n",
      "Epoch  30/100 Batch   30/70 - Loss:  1.470  - Validation loss:  2.186\n",
      "Epoch  30/100 Batch   40/70 - Loss:  2.066  - Validation loss:  2.172\n",
      "Epoch  30/100 Batch   50/70 - Loss:  1.623  - Validation loss:  2.164\n",
      "Epoch  30/100 Batch   60/70 - Loss:  1.289  - Validation loss:  2.169\n",
      "Epoch  31/100 Batch   10/70 - Loss:  1.456  - Validation loss:  2.160\n",
      "Epoch  31/100 Batch   20/70 - Loss:  1.635  - Validation loss:  2.170\n",
      "Epoch  31/100 Batch   30/70 - Loss:  1.447  - Validation loss:  2.174\n",
      "Epoch  31/100 Batch   40/70 - Loss:  2.050  - Validation loss:  2.166\n",
      "Epoch  31/100 Batch   50/70 - Loss:  1.607  - Validation loss:  2.164\n",
      "Epoch  31/100 Batch   60/70 - Loss:  1.279  - Validation loss:  2.165\n",
      "Epoch  32/100 Batch   10/70 - Loss:  1.443  - Validation loss:  2.155\n",
      "Epoch  32/100 Batch   20/70 - Loss:  1.618  - Validation loss:  2.163\n",
      "Epoch  32/100 Batch   30/70 - Loss:  1.430  - Validation loss:  2.165\n",
      "Epoch  32/100 Batch   40/70 - Loss:  2.035  - Validation loss:  2.162\n",
      "Epoch  32/100 Batch   50/70 - Loss:  1.596  - Validation loss:  2.161\n",
      "Epoch  32/100 Batch   60/70 - Loss:  1.268  - Validation loss:  2.160\n",
      "Epoch  33/100 Batch   10/70 - Loss:  1.430  - Validation loss:  2.153\n",
      "Epoch  33/100 Batch   20/70 - Loss:  1.604  - Validation loss:  2.158\n",
      "Epoch  33/100 Batch   30/70 - Loss:  1.418  - Validation loss:  2.162\n",
      "Epoch  33/100 Batch   40/70 - Loss:  2.024  - Validation loss:  2.163\n",
      "Epoch  33/100 Batch   50/70 - Loss:  1.587  - Validation loss:  2.158\n",
      "Epoch  33/100 Batch   60/70 - Loss:  1.263  - Validation loss:  2.161\n",
      "Epoch  34/100 Batch   10/70 - Loss:  1.421  - Validation loss:  2.151\n",
      "Epoch  34/100 Batch   20/70 - Loss:  1.592  - Validation loss:  2.158\n",
      "Epoch  34/100 Batch   30/70 - Loss:  1.408  - Validation loss:  2.156\n",
      "Epoch  34/100 Batch   40/70 - Loss:  2.019  - Validation loss:  2.164\n",
      "Epoch  34/100 Batch   50/70 - Loss:  1.579  - Validation loss:  2.158\n",
      "Epoch  34/100 Batch   60/70 - Loss:  1.257  - Validation loss:  2.154\n",
      "Epoch  35/100 Batch   10/70 - Loss:  1.414  - Validation loss:  2.148\n",
      "Epoch  35/100 Batch   20/70 - Loss:  1.584  - Validation loss:  2.158\n",
      "Epoch  35/100 Batch   30/70 - Loss:  1.400  - Validation loss:  2.157\n",
      "Epoch  35/100 Batch   40/70 - Loss:  2.000  - Validation loss:  2.163\n",
      "Epoch  35/100 Batch   50/70 - Loss:  1.571  - Validation loss:  2.152\n",
      "Epoch  35/100 Batch   60/70 - Loss:  1.261  - Validation loss:  2.156\n",
      "Epoch  36/100 Batch   10/70 - Loss:  1.409  - Validation loss:  2.152\n",
      "Epoch  36/100 Batch   20/70 - Loss:  1.570  - Validation loss:  2.151\n",
      "Epoch  36/100 Batch   30/70 - Loss:  1.386  - Validation loss:  2.150\n",
      "Epoch  36/100 Batch   40/70 - Loss:  1.991  - Validation loss:  2.147\n",
      "Epoch  36/100 Batch   50/70 - Loss:  1.565  - Validation loss:  2.150\n",
      "Epoch  36/100 Batch   60/70 - Loss:  1.246  - Validation loss:  2.155\n",
      "Epoch  37/100 Batch   10/70 - Loss:  1.395  - Validation loss:  2.148\n",
      "Epoch  37/100 Batch   20/70 - Loss:  1.556  - Validation loss:  2.145\n",
      "Epoch  37/100 Batch   30/70 - Loss:  1.377  - Validation loss:  2.150\n",
      "Epoch  37/100 Batch   40/70 - Loss:  1.971  - Validation loss:  2.146\n",
      "Epoch  37/100 Batch   50/70 - Loss:  1.553  - Validation loss:  2.149\n",
      "Epoch  37/100 Batch   60/70 - Loss:  1.236  - Validation loss:  2.152\n",
      "Epoch  38/100 Batch   10/70 - Loss:  1.387  - Validation loss:  2.148\n",
      "Epoch  38/100 Batch   20/70 - Loss:  1.549  - Validation loss:  2.147\n",
      "Epoch  38/100 Batch   30/70 - Loss:  1.366  - Validation loss:  2.146\n",
      "Epoch  38/100 Batch   40/70 - Loss:  1.960  - Validation loss:  2.137\n",
      "Epoch  38/100 Batch   50/70 - Loss:  1.548  - Validation loss:  2.144\n",
      "Epoch  38/100 Batch   60/70 - Loss:  1.228  - Validation loss:  2.153\n",
      "Epoch  39/100 Batch   10/70 - Loss:  1.385  - Validation loss:  2.153\n",
      "Epoch  39/100 Batch   20/70 - Loss:  1.553  - Validation loss:  2.151\n",
      "Epoch  39/100 Batch   30/70 - Loss:  1.368  - Validation loss:  2.160\n",
      "Epoch  39/100 Batch   40/70 - Loss:  1.968  - Validation loss:  2.142\n",
      "Epoch  39/100 Batch   50/70 - Loss:  1.542  - Validation loss:  2.142\n",
      "Epoch  39/100 Batch   60/70 - Loss:  1.223  - Validation loss:  2.146\n",
      "Epoch  40/100 Batch   10/70 - Loss:  1.406  - Validation loss:  2.176\n",
      "Epoch  40/100 Batch   20/70 - Loss:  1.550  - Validation loss:  2.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/100 Batch   30/70 - Loss:  1.356  - Validation loss:  2.159\n",
      "Epoch  40/100 Batch   40/70 - Loss:  1.952  - Validation loss:  2.153\n",
      "Epoch  40/100 Batch   50/70 - Loss:  1.543  - Validation loss:  2.159\n",
      "Epoch  40/100 Batch   60/70 - Loss:  1.232  - Validation loss:  2.157\n",
      "Epoch  41/100 Batch   10/70 - Loss:  1.366  - Validation loss:  2.137\n",
      "Epoch  41/100 Batch   20/70 - Loss:  1.529  - Validation loss:  2.144\n",
      "Epoch  41/100 Batch   30/70 - Loss:  1.342  - Validation loss:  2.149\n",
      "Epoch  41/100 Batch   40/70 - Loss:  1.941  - Validation loss:  2.150\n",
      "Epoch  41/100 Batch   50/70 - Loss:  1.528  - Validation loss:  2.148\n",
      "Epoch  41/100 Batch   60/70 - Loss:  1.221  - Validation loss:  2.159\n",
      "Epoch  42/100 Batch   10/70 - Loss:  1.355  - Validation loss:  2.129\n",
      "Epoch  42/100 Batch   20/70 - Loss:  1.512  - Validation loss:  2.138\n",
      "Epoch  42/100 Batch   30/70 - Loss:  1.331  - Validation loss:  2.140\n",
      "Epoch  42/100 Batch   40/70 - Loss:  1.926  - Validation loss:  2.148\n",
      "Epoch  42/100 Batch   50/70 - Loss:  1.533  - Validation loss:  2.154\n",
      "Epoch  42/100 Batch   60/70 - Loss:  1.212  - Validation loss:  2.165\n",
      "Epoch  43/100 Batch   10/70 - Loss:  1.363  - Validation loss:  2.150\n",
      "Epoch  43/100 Batch   20/70 - Loss:  1.506  - Validation loss:  2.136\n",
      "Epoch  43/100 Batch   30/70 - Loss:  1.322  - Validation loss:  2.134\n",
      "Epoch  43/100 Batch   40/70 - Loss:  1.917  - Validation loss:  2.133\n",
      "Epoch  43/100 Batch   50/70 - Loss:  1.516  - Validation loss:  2.153\n",
      "Epoch  43/100 Batch   60/70 - Loss:  1.197  - Validation loss:  2.159\n",
      "Epoch  44/100 Batch   10/70 - Loss:  1.352  - Validation loss:  2.179\n",
      "Epoch  44/100 Batch   20/70 - Loss:  1.513  - Validation loss:  2.157\n",
      "Epoch  44/100 Batch   30/70 - Loss:  1.314  - Validation loss:  2.147\n",
      "Epoch  44/100 Batch   40/70 - Loss:  1.907  - Validation loss:  2.132\n",
      "Epoch  44/100 Batch   50/70 - Loss:  1.511  - Validation loss:  2.132\n",
      "Epoch  44/100 Batch   60/70 - Loss:  1.204  - Validation loss:  2.141\n",
      "Epoch  45/100 Batch   10/70 - Loss:  1.344  - Validation loss:  2.130\n",
      "Epoch  45/100 Batch   20/70 - Loss:  1.498  - Validation loss:  2.134\n",
      "Epoch  45/100 Batch   30/70 - Loss:  1.314  - Validation loss:  2.145\n",
      "Epoch  45/100 Batch   40/70 - Loss:  1.907  - Validation loss:  2.129\n",
      "Epoch  45/100 Batch   50/70 - Loss:  1.508  - Validation loss:  2.132\n",
      "Epoch  45/100 Batch   60/70 - Loss:  1.199  - Validation loss:  2.144\n",
      "Epoch  46/100 Batch   10/70 - Loss:  1.329  - Validation loss:  2.131\n",
      "Epoch  46/100 Batch   20/70 - Loss:  1.483  - Validation loss:  2.132\n",
      "Epoch  46/100 Batch   30/70 - Loss:  1.303  - Validation loss:  2.146\n",
      "Epoch  46/100 Batch   40/70 - Loss:  1.903  - Validation loss:  2.136\n",
      "Epoch  46/100 Batch   50/70 - Loss:  1.491  - Validation loss:  2.141\n",
      "Epoch  46/100 Batch   60/70 - Loss:  1.183  - Validation loss:  2.144\n",
      "Epoch  47/100 Batch   10/70 - Loss:  1.314  - Validation loss:  2.134\n",
      "Epoch  47/100 Batch   20/70 - Loss:  1.477  - Validation loss:  2.134\n",
      "Epoch  47/100 Batch   30/70 - Loss:  1.287  - Validation loss:  2.142\n",
      "Epoch  47/100 Batch   40/70 - Loss:  1.884  - Validation loss:  2.145\n",
      "Epoch  47/100 Batch   50/70 - Loss:  1.486  - Validation loss:  2.137\n",
      "Epoch  47/100 Batch   60/70 - Loss:  1.173  - Validation loss:  2.140\n",
      "Epoch  48/100 Batch   10/70 - Loss:  1.303  - Validation loss:  2.134\n",
      "Epoch  48/100 Batch   20/70 - Loss:  1.467  - Validation loss:  2.136\n",
      "Epoch  48/100 Batch   30/70 - Loss:  1.281  - Validation loss:  2.136\n",
      "Epoch  48/100 Batch   40/70 - Loss:  1.867  - Validation loss:  2.137\n",
      "Epoch  48/100 Batch   50/70 - Loss:  1.477  - Validation loss:  2.142\n",
      "Epoch  48/100 Batch   60/70 - Loss:  1.170  - Validation loss:  2.145\n",
      "Epoch  49/100 Batch   10/70 - Loss:  1.298  - Validation loss:  2.133\n",
      "Epoch  49/100 Batch   20/70 - Loss:  1.452  - Validation loss:  2.129\n",
      "Epoch  49/100 Batch   30/70 - Loss:  1.271  - Validation loss:  2.136\n",
      "Epoch  49/100 Batch   40/70 - Loss:  1.855  - Validation loss:  2.129\n",
      "Epoch  49/100 Batch   50/70 - Loss:  1.460  - Validation loss:  2.134\n",
      "Epoch  49/100 Batch   60/70 - Loss:  1.159  - Validation loss:  2.141\n",
      "Epoch  50/100 Batch   10/70 - Loss:  1.289  - Validation loss:  2.135\n",
      "Epoch  50/100 Batch   20/70 - Loss:  1.444  - Validation loss:  2.127\n",
      "Epoch  50/100 Batch   30/70 - Loss:  1.259  - Validation loss:  2.136\n",
      "Epoch  50/100 Batch   40/70 - Loss:  1.846  - Validation loss:  2.127\n",
      "Epoch  50/100 Batch   50/70 - Loss:  1.449  - Validation loss:  2.129\n",
      "Epoch  50/100 Batch   60/70 - Loss:  1.149  - Validation loss:  2.137\n",
      "Epoch  51/100 Batch   10/70 - Loss:  1.281  - Validation loss:  2.138\n",
      "Epoch  51/100 Batch   20/70 - Loss:  1.436  - Validation loss:  2.127\n",
      "Epoch  51/100 Batch   30/70 - Loss:  1.251  - Validation loss:  2.136\n",
      "Epoch  51/100 Batch   40/70 - Loss:  1.836  - Validation loss:  2.131\n",
      "Epoch  51/100 Batch   50/70 - Loss:  1.443  - Validation loss:  2.128\n",
      "Epoch  51/100 Batch   60/70 - Loss:  1.142  - Validation loss:  2.130\n",
      "Epoch  52/100 Batch   10/70 - Loss:  1.273  - Validation loss:  2.141\n",
      "Epoch  52/100 Batch   20/70 - Loss:  1.428  - Validation loss:  2.129\n",
      "Epoch  52/100 Batch   30/70 - Loss:  1.246  - Validation loss:  2.135\n",
      "Epoch  52/100 Batch   40/70 - Loss:  1.821  - Validation loss:  2.133\n",
      "Epoch  52/100 Batch   50/70 - Loss:  1.438  - Validation loss:  2.133\n",
      "Epoch  52/100 Batch   60/70 - Loss:  1.139  - Validation loss:  2.129\n",
      "Epoch  53/100 Batch   10/70 - Loss:  1.267  - Validation loss:  2.146\n",
      "Epoch  53/100 Batch   20/70 - Loss:  1.423  - Validation loss:  2.134\n",
      "Epoch  53/100 Batch   30/70 - Loss:  1.239  - Validation loss:  2.137\n",
      "Epoch  53/100 Batch   40/70 - Loss:  1.814  - Validation loss:  2.133\n",
      "Epoch  53/100 Batch   50/70 - Loss:  1.429  - Validation loss:  2.134\n",
      "Epoch  53/100 Batch   60/70 - Loss:  1.132  - Validation loss:  2.131\n",
      "Epoch  54/100 Batch   10/70 - Loss:  1.258  - Validation loss:  2.147\n",
      "Epoch  54/100 Batch   20/70 - Loss:  1.420  - Validation loss:  2.144\n",
      "Epoch  54/100 Batch   30/70 - Loss:  1.231  - Validation loss:  2.136\n",
      "Epoch  54/100 Batch   40/70 - Loss:  1.807  - Validation loss:  2.135\n",
      "Epoch  54/100 Batch   50/70 - Loss:  1.421  - Validation loss:  2.133\n",
      "Epoch  54/100 Batch   60/70 - Loss:  1.127  - Validation loss:  2.136\n",
      "Epoch  55/100 Batch   10/70 - Loss:  1.247  - Validation loss:  2.144\n",
      "Epoch  55/100 Batch   20/70 - Loss:  1.414  - Validation loss:  2.150\n",
      "Epoch  55/100 Batch   30/70 - Loss:  1.229  - Validation loss:  2.141\n",
      "Epoch  55/100 Batch   40/70 - Loss:  1.796  - Validation loss:  2.138\n",
      "Epoch  55/100 Batch   50/70 - Loss:  1.414  - Validation loss:  2.129\n",
      "Epoch  55/100 Batch   60/70 - Loss:  1.125  - Validation loss:  2.141\n",
      "Epoch  56/100 Batch   10/70 - Loss:  1.239  - Validation loss:  2.131\n",
      "Epoch  56/100 Batch   20/70 - Loss:  1.401  - Validation loss:  2.159\n",
      "Epoch  56/100 Batch   30/70 - Loss:  1.240  - Validation loss:  2.158\n",
      "Epoch  56/100 Batch   40/70 - Loss:  1.805  - Validation loss:  2.146\n",
      "Epoch  56/100 Batch   50/70 - Loss:  1.407  - Validation loss:  2.133\n",
      "Epoch  56/100 Batch   60/70 - Loss:  1.116  - Validation loss:  2.142\n",
      "Epoch  57/100 Batch   10/70 - Loss:  1.247  - Validation loss:  2.147\n",
      "Epoch  57/100 Batch   20/70 - Loss:  1.388  - Validation loss:  2.134\n",
      "Epoch  57/100 Batch   30/70 - Loss:  1.222  - Validation loss:  2.167\n",
      "Epoch  57/100 Batch   40/70 - Loss:  1.807  - Validation loss:  2.148\n",
      "Epoch  57/100 Batch   50/70 - Loss:  1.414  - Validation loss:  2.134\n",
      "Epoch  57/100 Batch   60/70 - Loss:  1.118  - Validation loss:  2.139\n",
      "Epoch  58/100 Batch   10/70 - Loss:  1.238  - Validation loss:  2.168\n",
      "Epoch  58/100 Batch   20/70 - Loss:  1.383  - Validation loss:  2.147\n",
      "Epoch  58/100 Batch   30/70 - Loss:  1.215  - Validation loss:  2.133\n",
      "Epoch  58/100 Batch   40/70 - Loss:  1.809  - Validation loss:  2.138\n",
      "Epoch  58/100 Batch   50/70 - Loss:  1.413  - Validation loss:  2.130\n",
      "Epoch  58/100 Batch   60/70 - Loss:  1.112  - Validation loss:  2.138\n",
      "Epoch  59/100 Batch   10/70 - Loss:  1.229  - Validation loss:  2.140\n",
      "Epoch  59/100 Batch   20/70 - Loss:  1.394  - Validation loss:  2.128\n",
      "Epoch  59/100 Batch   30/70 - Loss:  1.217  - Validation loss:  2.141\n",
      "Epoch  59/100 Batch   40/70 - Loss:  1.769  - Validation loss:  2.128\n",
      "Epoch  59/100 Batch   50/70 - Loss:  1.397  - Validation loss:  2.129\n",
      "Epoch  59/100 Batch   60/70 - Loss:  1.103  - Validation loss:  2.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60/100 Batch   10/70 - Loss:  1.218  - Validation loss:  2.139\n",
      "Epoch  60/100 Batch   20/70 - Loss:  1.375  - Validation loss:  2.129\n",
      "Epoch  60/100 Batch   30/70 - Loss:  1.192  - Validation loss:  2.133\n",
      "Epoch  60/100 Batch   40/70 - Loss:  1.757  - Validation loss:  2.129\n",
      "Epoch  60/100 Batch   50/70 - Loss:  1.386  - Validation loss:  2.124\n",
      "Epoch  60/100 Batch   60/70 - Loss:  1.093  - Validation loss:  2.141\n",
      "Epoch  61/100 Batch   10/70 - Loss:  1.209  - Validation loss:  2.136\n",
      "Epoch  61/100 Batch   20/70 - Loss:  1.365  - Validation loss:  2.133\n",
      "Epoch  61/100 Batch   30/70 - Loss:  1.183  - Validation loss:  2.131\n",
      "Epoch  61/100 Batch   40/70 - Loss:  1.741  - Validation loss:  2.131\n",
      "Epoch  61/100 Batch   50/70 - Loss:  1.378  - Validation loss:  2.123\n",
      "Epoch  61/100 Batch   60/70 - Loss:  1.086  - Validation loss:  2.141\n",
      "Epoch  62/100 Batch   10/70 - Loss:  1.205  - Validation loss:  2.139\n",
      "Epoch  62/100 Batch   20/70 - Loss:  1.356  - Validation loss:  2.130\n",
      "Epoch  62/100 Batch   30/70 - Loss:  1.175  - Validation loss:  2.131\n",
      "Epoch  62/100 Batch   40/70 - Loss:  1.738  - Validation loss:  2.131\n",
      "Epoch  62/100 Batch   50/70 - Loss:  1.367  - Validation loss:  2.122\n",
      "Epoch  62/100 Batch   60/70 - Loss:  1.078  - Validation loss:  2.144\n",
      "Epoch  63/100 Batch   10/70 - Loss:  1.194  - Validation loss:  2.145\n",
      "Epoch  63/100 Batch   20/70 - Loss:  1.348  - Validation loss:  2.129\n",
      "Epoch  63/100 Batch   30/70 - Loss:  1.165  - Validation loss:  2.129\n",
      "Epoch  63/100 Batch   40/70 - Loss:  1.728  - Validation loss:  2.135\n",
      "Epoch  63/100 Batch   50/70 - Loss:  1.363  - Validation loss:  2.125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c5463a1ca524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                  \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                  \u001b[0mtarget_sequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_targets_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                  source_sequence_length: valid_sources_lengths})\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_letter_ids[batch_size:]\n",
    "train_target = target_letter_ids[batch_size:]\n",
    "valid_source = source_letter_ids[:batch_size]\n",
    "valid_target = target_letter_ids[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = \\\n",
    "                  next(get_batches(valid_target, valid_source, batch_size))\n",
    "\n",
    "display_step = 10 # Check training loss after every 20 batches\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(train_target, train_source, batch_size)):\n",
    "            \n",
    "            # Training step\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data:sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths})\n",
    "\n",
    "            # Debug message updating us on the status of the training\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                # Calculate validation cost\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    sequence_length = 20\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int[_UNK]) for word in text]+ [source_letter_to_int[_PAD]]*(sequence_length-len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentence = 'I usually eat the very large salad.'\n",
    "text = []\n",
    "text.append(basic_tokenizer(input_sentence))\n",
    "text, sample_src_seq_lens = data_to_token_ids(\n",
    "        text, source_letter_to_int,max_seq_len=20 ,normalize_digits=True)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "text=text[0]\n",
    "print(text)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_sequence_length: [len(text)]*batch_size, \n",
    "                                      source_sequence_length: [len(text)]*batch_size})[0] \n",
    "\n",
    "\n",
    "pad = source_letter_to_int[_PAD] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "128*51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
