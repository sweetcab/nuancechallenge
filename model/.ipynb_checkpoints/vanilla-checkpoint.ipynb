{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Special token definition, define the regular expression to split the sentences and punctuations.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    print(\"original vocab_len: \"+str(len(vocab_list)))\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    #max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        data_as_tokens.extend(token_ids)\n",
    "    return data_as_tokens\n",
    "\n",
    "def process_data(datafile, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens= data_to_token_ids(tokenized, vocab_dict, normalize_digits=True)\n",
    "    \n",
    "    data_mixed=[]\n",
    "    for word in data_as_tokens:\n",
    "        if word==vocab_dict[\"the\"] or word==vocab_dict[\"a\"]:\n",
    "            data_mixed.append(vocab_dict[\"the\"] if random.random()<0.5 else vocab_dict[\"a\"])\n",
    "        else:\n",
    "            data_mixed.append(word)\n",
    "            \n",
    "    pickle.dump((data_as_tokens, vocab_dict,  rev_vocab_dict), open('preprocess.p', 'wb'))\n",
    "    \n",
    "    return data_as_tokens, data_mixed,vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text):\n",
    "    # Compare the total a/the pair and obfuscated a/the pair\n",
    "    \n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab_len: 12251\n"
     ]
    }
   ],
   "source": [
    "## Process the original data for training, which is the novel Oliver Twist\n",
    "int_label,int_text, vocab_to_int, int_to_vocab = \\\n",
    "        process_data('original.p', max_vocab_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49532031628207196"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial accuracy\n",
    "mix,total=compare_a_the(int_label,int_text)\n",
    "mix/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data size\n",
    "len(int_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Tensorflow version and GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "\n",
    "    inputs=tf.placeholder(tf.int32,[None,None],name='input')\n",
    "    targets=tf.placeholder(tf.int32,[None,None],name='targets')\n",
    "    learning_rate=tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, learning_rate,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    def single_cell(keepprob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keepprob)\n",
    "        return drop\n",
    "                                         \n",
    "    cell_fw = tf.contrib.rnn.MultiRNNCell([single_cell(keep_prob) for _ in range(1)])\n",
    "    cell_bw = tf.contrib.rnn.MultiRNNCell([single_cell(keep_prob) for _ in range(1)])\n",
    "    return cell_fw,cell_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_data, name='embed')\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    (cell_fw,cell_bw)=cell\n",
    "    outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,cell_bw=cell_bw, inputs=inputs, dtype=tf.float32)\n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    final_state=tf.concat(final_state, 2)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return (outputs, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs,vocab_size,activation_fn=None)\n",
    "    return (logits, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text,label_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    num_batches = len(int_text) // (batch_size * seq_length)\n",
    "    xtext = int_text[:num_batches * batch_size * seq_length]\n",
    "    ytext = label_text[:num_batches * batch_size * seq_length]\n",
    "    \n",
    "        \n",
    "    xreshape=np.reshape(xtext,[batch_size,-1])\n",
    "    yreshape=np.reshape(ytext,[batch_size,-1])\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0,xreshape.shape[1],seq_length):\n",
    "        xx=xreshape[:,i:i+seq_length]\n",
    "        yy=yreshape[:,i:i+seq_length]\n",
    "        batches.append([xx,yy])\n",
    "        \n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 120\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 200\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 200\n",
    "# Sequence Length\n",
    "seq_length = 30\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'\n",
    "\n",
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr,keep_prob = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell= get_init_cell(input_data_shape[0], rnn_size,keep_prob)\n",
    "    logits,final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/41   train_loss = 9.394  valid_loss = 9.384\n",
      "Epoch   0 Batch   10/41   train_loss = 8.234  valid_loss = 7.902\n",
      "Epoch   0 Batch   20/41   train_loss = 6.547  valid_loss = 6.632\n",
      "Epoch   0 Batch   30/41   train_loss = 6.445  valid_loss = 6.599\n",
      "Epoch   0 Batch   40/41   train_loss = 6.405  valid_loss = 6.455\n",
      "Epoch   1 Batch    9/41   train_loss = 6.186  valid_loss = 6.303\n",
      "Epoch   1 Batch   19/41   train_loss = 6.088  valid_loss = 6.163\n",
      "Epoch   1 Batch   29/41   train_loss = 5.858  valid_loss = 5.991\n",
      "Epoch   1 Batch   39/41   train_loss = 5.646  valid_loss = 5.806\n",
      "Epoch   2 Batch    8/41   train_loss = 5.494  valid_loss = 5.620\n",
      "Epoch   2 Batch   18/41   train_loss = 5.334  valid_loss = 5.463\n",
      "Epoch   2 Batch   28/41   train_loss = 5.186  valid_loss = 5.324\n",
      "Epoch   2 Batch   38/41   train_loss = 4.985  valid_loss = 5.188\n",
      "Epoch   3 Batch    7/41   train_loss = 4.914  valid_loss = 5.041\n",
      "Epoch   3 Batch   17/41   train_loss = 4.849  valid_loss = 4.895\n",
      "Epoch   3 Batch   27/41   train_loss = 4.562  valid_loss = 4.750\n",
      "Epoch   3 Batch   37/41   train_loss = 4.392  valid_loss = 4.598\n",
      "Epoch   4 Batch    6/41   train_loss = 4.227  valid_loss = 4.437\n",
      "Epoch   4 Batch   16/41   train_loss = 4.140  valid_loss = 4.274\n",
      "Epoch   4 Batch   26/41   train_loss = 3.919  valid_loss = 4.120\n",
      "Epoch   4 Batch   36/41   train_loss = 3.864  valid_loss = 3.968\n",
      "Epoch   5 Batch    5/41   train_loss = 3.655  valid_loss = 3.821\n",
      "Epoch   5 Batch   15/41   train_loss = 3.431  valid_loss = 3.682\n",
      "Epoch   5 Batch   25/41   train_loss = 3.338  valid_loss = 3.559\n",
      "Epoch   5 Batch   35/41   train_loss = 3.178  valid_loss = 3.441\n",
      "Epoch   6 Batch    4/41   train_loss = 3.039  valid_loss = 3.331\n",
      "Epoch   6 Batch   14/41   train_loss = 2.961  valid_loss = 3.224\n",
      "Epoch   6 Batch   24/41   train_loss = 2.943  valid_loss = 3.129\n",
      "Epoch   6 Batch   34/41   train_loss = 2.711  valid_loss = 3.036\n",
      "Epoch   7 Batch    3/41   train_loss = 2.707  valid_loss = 2.949\n",
      "Epoch   7 Batch   13/41   train_loss = 2.732  valid_loss = 2.863\n",
      "Epoch   7 Batch   23/41   train_loss = 2.657  valid_loss = 2.788\n",
      "Epoch   7 Batch   33/41   train_loss = 2.378  valid_loss = 2.713\n",
      "Epoch   8 Batch    2/41   train_loss = 2.308  valid_loss = 2.643\n",
      "Epoch   8 Batch   12/41   train_loss = 2.318  valid_loss = 2.574\n",
      "Epoch   8 Batch   22/41   train_loss = 2.301  valid_loss = 2.517\n",
      "Epoch   8 Batch   32/41   train_loss = 2.035  valid_loss = 2.455\n",
      "Epoch   9 Batch    1/41   train_loss = 2.214  valid_loss = 2.400\n",
      "Epoch   9 Batch   11/41   train_loss = 2.030  valid_loss = 2.344\n",
      "Epoch   9 Batch   21/41   train_loss = 2.051  valid_loss = 2.301\n",
      "Epoch   9 Batch   31/41   train_loss = 1.867  valid_loss = 2.250\n",
      "Epoch  10 Batch    0/41   train_loss = 1.975  valid_loss = 2.207\n",
      "Epoch  10 Batch   10/41   train_loss = 1.808  valid_loss = 2.157\n",
      "Epoch  10 Batch   20/41   train_loss = 1.853  valid_loss = 2.123\n",
      "Epoch  10 Batch   30/41   train_loss = 1.706  valid_loss = 2.082\n",
      "Epoch  10 Batch   40/41   train_loss = 1.702  valid_loss = 2.046\n",
      "Epoch  11 Batch    9/41   train_loss = 1.703  valid_loss = 2.005\n",
      "Epoch  11 Batch   19/41   train_loss = 1.748  valid_loss = 1.973\n",
      "Epoch  11 Batch   29/41   train_loss = 1.580  valid_loss = 1.942\n",
      "Epoch  11 Batch   39/41   train_loss = 1.534  valid_loss = 1.909\n",
      "Epoch  12 Batch    8/41   train_loss = 1.476  valid_loss = 1.875\n",
      "Epoch  12 Batch   18/41   train_loss = 1.538  valid_loss = 1.846\n",
      "Epoch  12 Batch   28/41   train_loss = 1.516  valid_loss = 1.824\n",
      "Epoch  12 Batch   38/41   train_loss = 1.404  valid_loss = 1.793\n",
      "Epoch  13 Batch    7/41   train_loss = 1.433  valid_loss = 1.763\n",
      "Epoch  13 Batch   17/41   train_loss = 1.435  valid_loss = 1.737\n",
      "Epoch  13 Batch   27/41   train_loss = 1.315  valid_loss = 1.721\n",
      "Epoch  13 Batch   37/41   train_loss = 1.230  valid_loss = 1.693\n",
      "Epoch  14 Batch    6/41   train_loss = 1.252  valid_loss = 1.666\n",
      "Epoch  14 Batch   16/41   train_loss = 1.236  valid_loss = 1.642\n",
      "Epoch  14 Batch   26/41   train_loss = 1.186  valid_loss = 1.630\n",
      "Epoch  14 Batch   36/41   train_loss = 1.248  valid_loss = 1.605\n",
      "Epoch  15 Batch    5/41   train_loss = 1.170  valid_loss = 1.584\n",
      "Epoch  15 Batch   15/41   train_loss = 1.118  valid_loss = 1.560\n",
      "Epoch  15 Batch   25/41   train_loss = 1.050  valid_loss = 1.551\n",
      "Epoch  15 Batch   35/41   train_loss = 1.070  valid_loss = 1.533\n",
      "Epoch  16 Batch    4/41   train_loss = 1.004  valid_loss = 1.512\n",
      "Epoch  16 Batch   14/41   train_loss = 0.971  valid_loss = 1.489\n",
      "Epoch  16 Batch   24/41   train_loss = 1.011  valid_loss = 1.481\n",
      "Epoch  16 Batch   34/41   train_loss = 0.931  valid_loss = 1.472\n",
      "Epoch  17 Batch    3/41   train_loss = 0.951  valid_loss = 1.449\n",
      "Epoch  17 Batch   13/41   train_loss = 0.994  valid_loss = 1.430\n",
      "Epoch  17 Batch   23/41   train_loss = 0.951  valid_loss = 1.421\n",
      "Epoch  17 Batch   33/41   train_loss = 0.819  valid_loss = 1.415\n",
      "Epoch  18 Batch    2/41   train_loss = 0.776  valid_loss = 1.397\n",
      "Epoch  18 Batch   12/41   train_loss = 0.834  valid_loss = 1.376\n",
      "Epoch  18 Batch   22/41   train_loss = 0.813  valid_loss = 1.368\n",
      "Epoch  18 Batch   32/41   train_loss = 0.713  valid_loss = 1.366\n",
      "Epoch  19 Batch    1/41   train_loss = 0.889  valid_loss = 1.353\n",
      "Epoch  19 Batch   11/41   train_loss = 0.700  valid_loss = 1.329\n",
      "Epoch  19 Batch   21/41   train_loss = 0.718  valid_loss = 1.323\n",
      "Epoch  19 Batch   31/41   train_loss = 0.680  valid_loss = 1.320\n",
      "Epoch  20 Batch    0/41   train_loss = 0.768  valid_loss = 1.314\n",
      "Epoch  20 Batch   10/41   train_loss = 0.651  valid_loss = 1.291\n",
      "Epoch  20 Batch   20/41   train_loss = 0.670  valid_loss = 1.282\n",
      "Epoch  20 Batch   30/41   train_loss = 0.646  valid_loss = 1.283\n",
      "Epoch  20 Batch   40/41   train_loss = 0.639  valid_loss = 1.277\n",
      "Epoch  21 Batch    9/41   train_loss = 0.617  valid_loss = 1.260\n",
      "Epoch  21 Batch   19/41   train_loss = 0.664  valid_loss = 1.245\n",
      "Epoch  21 Batch   29/41   train_loss = 0.574  valid_loss = 1.252\n",
      "Epoch  21 Batch   39/41   train_loss = 0.583  valid_loss = 1.245\n",
      "Epoch  22 Batch    8/41   train_loss = 0.519  valid_loss = 1.234\n",
      "Epoch  22 Batch   18/41   train_loss = 0.567  valid_loss = 1.215\n",
      "Epoch  22 Batch   28/41   train_loss = 0.606  valid_loss = 1.222\n",
      "Epoch  22 Batch   38/41   train_loss = 0.536  valid_loss = 1.216\n",
      "Epoch  23 Batch    7/41   train_loss = 0.555  valid_loss = 1.207\n",
      "Epoch  23 Batch   17/41   train_loss = 0.520  valid_loss = 1.189\n",
      "Epoch  23 Batch   27/41   train_loss = 0.514  valid_loss = 1.192\n",
      "Epoch  23 Batch   37/41   train_loss = 0.454  valid_loss = 1.193\n",
      "Epoch  24 Batch    6/41   train_loss = 0.477  valid_loss = 1.182\n",
      "Epoch  24 Batch   16/41   train_loss = 0.463  valid_loss = 1.170\n",
      "Epoch  24 Batch   26/41   train_loss = 0.439  valid_loss = 1.164\n",
      "Epoch  24 Batch   36/41   train_loss = 0.453  valid_loss = 1.172\n",
      "Epoch  25 Batch    5/41   train_loss = 0.452  valid_loss = 1.161\n",
      "Epoch  25 Batch   15/41   train_loss = 0.412  valid_loss = 1.150\n",
      "Epoch  25 Batch   25/41   train_loss = 0.389  valid_loss = 1.139\n",
      "Epoch  25 Batch   35/41   train_loss = 0.402  valid_loss = 1.148\n",
      "Epoch  26 Batch    4/41   train_loss = 0.373  valid_loss = 1.143\n",
      "Epoch  26 Batch   14/41   train_loss = 0.345  valid_loss = 1.132\n",
      "Epoch  26 Batch   24/41   train_loss = 0.384  valid_loss = 1.119\n",
      "Epoch  26 Batch   34/41   train_loss = 0.345  valid_loss = 1.125\n",
      "Epoch  27 Batch    3/41   train_loss = 0.355  valid_loss = 1.125\n",
      "Epoch  27 Batch   13/41   train_loss = 0.342  valid_loss = 1.115\n",
      "Epoch  27 Batch   23/41   train_loss = 0.335  valid_loss = 1.102\n",
      "Epoch  27 Batch   33/41   train_loss = 0.293  valid_loss = 1.102\n",
      "Epoch  28 Batch    2/41   train_loss = 0.269  valid_loss = 1.107\n",
      "Epoch  28 Batch   12/41   train_loss = 0.299  valid_loss = 1.098\n",
      "Epoch  28 Batch   22/41   train_loss = 0.281  valid_loss = 1.086\n",
      "Epoch  28 Batch   32/41   train_loss = 0.254  valid_loss = 1.083\n",
      "Epoch  29 Batch    1/41   train_loss = 0.320  valid_loss = 1.090\n",
      "Epoch  29 Batch   11/41   train_loss = 0.243  valid_loss = 1.083\n",
      "Epoch  29 Batch   21/41   train_loss = 0.259  valid_loss = 1.071\n",
      "Epoch  29 Batch   31/41   train_loss = 0.227  valid_loss = 1.067\n",
      "Epoch  30 Batch    0/41   train_loss = 0.262  valid_loss = 1.071\n",
      "Epoch  30 Batch   10/41   train_loss = 0.222  valid_loss = 1.071\n",
      "Epoch  30 Batch   20/41   train_loss = 0.220  valid_loss = 1.056\n",
      "Epoch  30 Batch   30/41   train_loss = 0.229  valid_loss = 1.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30 Batch   40/41   train_loss = 0.204  valid_loss = 1.053\n",
      "Epoch  31 Batch    9/41   train_loss = 0.195  valid_loss = 1.055\n",
      "Epoch  31 Batch   19/41   train_loss = 0.217  valid_loss = 1.046\n",
      "Epoch  31 Batch   29/41   train_loss = 0.190  valid_loss = 1.037\n",
      "Epoch  31 Batch   39/41   train_loss = 0.186  valid_loss = 1.036\n",
      "Epoch  32 Batch    8/41   train_loss = 0.153  valid_loss = 1.038\n",
      "Epoch  32 Batch   18/41   train_loss = 0.185  valid_loss = 1.035\n",
      "Epoch  32 Batch   28/41   train_loss = 0.200  valid_loss = 1.025\n",
      "Epoch  32 Batch   38/41   train_loss = 0.165  valid_loss = 1.022\n",
      "Epoch  33 Batch    7/41   train_loss = 0.170  valid_loss = 1.022\n",
      "Epoch  33 Batch   17/41   train_loss = 0.148  valid_loss = 1.022\n",
      "Epoch  33 Batch   27/41   train_loss = 0.165  valid_loss = 1.016\n",
      "Epoch  33 Batch   37/41   train_loss = 0.144  valid_loss = 1.011\n",
      "Epoch  34 Batch    6/41   train_loss = 0.134  valid_loss = 1.011\n",
      "Epoch  34 Batch   16/41   train_loss = 0.139  valid_loss = 1.010\n",
      "Epoch  34 Batch   26/41   train_loss = 0.138  valid_loss = 1.006\n",
      "Epoch  34 Batch   36/41   train_loss = 0.132  valid_loss = 1.002\n",
      "Epoch  35 Batch    5/41   train_loss = 0.131  valid_loss = 0.999\n",
      "Epoch  35 Batch   15/41   train_loss = 0.122  valid_loss = 0.996\n",
      "Epoch  35 Batch   25/41   train_loss = 0.119  valid_loss = 0.995\n",
      "Epoch  35 Batch   35/41   train_loss = 0.116  valid_loss = 0.995\n",
      "Epoch  36 Batch    4/41   train_loss = 0.102  valid_loss = 0.991\n",
      "Epoch  36 Batch   14/41   train_loss = 0.093  valid_loss = 0.985\n",
      "Epoch  36 Batch   24/41   train_loss = 0.113  valid_loss = 0.983\n",
      "Epoch  36 Batch   34/41   train_loss = 0.089  valid_loss = 0.985\n",
      "Epoch  37 Batch    3/41   train_loss = 0.092  valid_loss = 0.984\n",
      "Epoch  37 Batch   13/41   train_loss = 0.086  valid_loss = 0.978\n",
      "Epoch  37 Batch   23/41   train_loss = 0.092  valid_loss = 0.972\n",
      "Epoch  37 Batch   33/41   train_loss = 0.080  valid_loss = 0.973\n",
      "Epoch  38 Batch    2/41   train_loss = 0.071  valid_loss = 0.975\n",
      "Epoch  38 Batch   12/41   train_loss = 0.078  valid_loss = 0.973\n",
      "Epoch  38 Batch   22/41   train_loss = 0.077  valid_loss = 0.964\n",
      "Epoch  38 Batch   32/41   train_loss = 0.070  valid_loss = 0.963\n",
      "Epoch  39 Batch    1/41   train_loss = 0.084  valid_loss = 0.965\n",
      "Epoch  39 Batch   11/41   train_loss = 0.062  valid_loss = 0.966\n",
      "Epoch  39 Batch   21/41   train_loss = 0.064  valid_loss = 0.960\n",
      "Epoch  39 Batch   31/41   train_loss = 0.061  valid_loss = 0.956\n",
      "Epoch  40 Batch    0/41   train_loss = 0.065  valid_loss = 0.956\n",
      "Epoch  40 Batch   10/41   train_loss = 0.058  valid_loss = 0.958\n",
      "Epoch  40 Batch   20/41   train_loss = 0.059  valid_loss = 0.954\n",
      "Epoch  40 Batch   30/41   train_loss = 0.056  valid_loss = 0.950\n",
      "Epoch  40 Batch   40/41   train_loss = 0.049  valid_loss = 0.948\n",
      "Epoch  41 Batch    9/41   train_loss = 0.049  valid_loss = 0.951\n",
      "Epoch  41 Batch   19/41   train_loss = 0.057  valid_loss = 0.950\n",
      "Epoch  41 Batch   29/41   train_loss = 0.044  valid_loss = 0.946\n",
      "Epoch  41 Batch   39/41   train_loss = 0.047  valid_loss = 0.943\n",
      "Epoch  42 Batch    8/41   train_loss = 0.041  valid_loss = 0.943\n",
      "Epoch  42 Batch   18/41   train_loss = 0.047  valid_loss = 0.943\n",
      "Epoch  42 Batch   28/41   train_loss = 0.050  valid_loss = 0.941\n",
      "Epoch  42 Batch   38/41   train_loss = 0.045  valid_loss = 0.938\n",
      "Epoch  43 Batch    7/41   train_loss = 0.045  valid_loss = 0.938\n",
      "Epoch  43 Batch   17/41   train_loss = 0.041  valid_loss = 0.939\n",
      "Epoch  43 Batch   27/41   train_loss = 0.045  valid_loss = 0.936\n",
      "Epoch  43 Batch   37/41   train_loss = 0.039  valid_loss = 0.934\n",
      "Epoch  44 Batch    6/41   train_loss = 0.039  valid_loss = 0.933\n",
      "Epoch  44 Batch   16/41   train_loss = 0.039  valid_loss = 0.933\n",
      "Epoch  44 Batch   26/41   train_loss = 0.040  valid_loss = 0.932\n",
      "Epoch  44 Batch   36/41   train_loss = 0.038  valid_loss = 0.930\n",
      "Epoch  45 Batch    5/41   train_loss = 0.034  valid_loss = 0.928\n",
      "Epoch  45 Batch   15/41   train_loss = 0.037  valid_loss = 0.927\n",
      "Epoch  45 Batch   25/41   train_loss = 0.032  valid_loss = 0.927\n",
      "Epoch  45 Batch   35/41   train_loss = 0.037  valid_loss = 0.927\n",
      "Epoch  46 Batch    4/41   train_loss = 0.034  valid_loss = 0.925\n",
      "Epoch  46 Batch   14/41   train_loss = 0.030  valid_loss = 0.923\n",
      "Epoch  46 Batch   24/41   train_loss = 0.033  valid_loss = 0.922\n",
      "Epoch  46 Batch   34/41   train_loss = 0.029  valid_loss = 0.923\n",
      "Epoch  47 Batch    3/41   train_loss = 0.030  valid_loss = 0.921\n",
      "Epoch  47 Batch   13/41   train_loss = 0.032  valid_loss = 0.921\n",
      "Epoch  47 Batch   23/41   train_loss = 0.029  valid_loss = 0.921\n",
      "Epoch  47 Batch   33/41   train_loss = 0.027  valid_loss = 0.921\n",
      "Epoch  48 Batch    2/41   train_loss = 0.026  valid_loss = 0.919\n",
      "Epoch  48 Batch   12/41   train_loss = 0.030  valid_loss = 0.919\n",
      "Epoch  48 Batch   22/41   train_loss = 0.031  valid_loss = 0.917\n",
      "Epoch  48 Batch   32/41   train_loss = 0.025  valid_loss = 0.917\n",
      "Epoch  49 Batch    1/41   train_loss = 0.032  valid_loss = 0.917\n",
      "Epoch  49 Batch   11/41   train_loss = 0.024  valid_loss = 0.916\n",
      "Epoch  49 Batch   21/41   train_loss = 0.024  valid_loss = 0.915\n",
      "Epoch  49 Batch   31/41   train_loss = 0.023  valid_loss = 0.913\n",
      "Epoch  50 Batch    0/41   train_loss = 0.025  valid_loss = 0.913\n",
      "Epoch  50 Batch   10/41   train_loss = 0.024  valid_loss = 0.913\n",
      "Epoch  50 Batch   20/41   train_loss = 0.025  valid_loss = 0.912\n",
      "Epoch  50 Batch   30/41   train_loss = 0.021  valid_loss = 0.912\n",
      "Epoch  50 Batch   40/41   train_loss = 0.022  valid_loss = 0.911\n",
      "Epoch  51 Batch    9/41   train_loss = 0.022  valid_loss = 0.911\n",
      "Epoch  51 Batch   19/41   train_loss = 0.027  valid_loss = 0.910\n",
      "Epoch  51 Batch   29/41   train_loss = 0.019  valid_loss = 0.910\n",
      "Epoch  51 Batch   39/41   train_loss = 0.020  valid_loss = 0.910\n",
      "Epoch  52 Batch    8/41   train_loss = 0.020  valid_loss = 0.909\n",
      "Epoch  52 Batch   18/41   train_loss = 0.023  valid_loss = 0.907\n",
      "Epoch  52 Batch   28/41   train_loss = 0.023  valid_loss = 0.907\n",
      "Epoch  52 Batch   38/41   train_loss = 0.021  valid_loss = 0.907\n",
      "Epoch  53 Batch    7/41   train_loss = 0.022  valid_loss = 0.907\n",
      "Epoch  53 Batch   17/41   train_loss = 0.020  valid_loss = 0.906\n",
      "Epoch  53 Batch   27/41   train_loss = 0.021  valid_loss = 0.906\n",
      "Epoch  53 Batch   37/41   train_loss = 0.018  valid_loss = 0.906\n",
      "Epoch  54 Batch    6/41   train_loss = 0.018  valid_loss = 0.906\n",
      "Epoch  54 Batch   16/41   train_loss = 0.019  valid_loss = 0.905\n",
      "Epoch  54 Batch   26/41   train_loss = 0.018  valid_loss = 0.906\n",
      "Epoch  54 Batch   36/41   train_loss = 0.018  valid_loss = 0.905\n",
      "Epoch  55 Batch    5/41   train_loss = 0.017  valid_loss = 0.905\n",
      "Epoch  55 Batch   15/41   train_loss = 0.017  valid_loss = 0.904\n",
      "Epoch  55 Batch   25/41   train_loss = 0.016  valid_loss = 0.902\n",
      "Epoch  55 Batch   35/41   train_loss = 0.017  valid_loss = 0.903\n",
      "Epoch  56 Batch    4/41   train_loss = 0.016  valid_loss = 0.903\n",
      "Epoch  56 Batch   14/41   train_loss = 0.014  valid_loss = 0.902\n",
      "Epoch  56 Batch   24/41   train_loss = 0.016  valid_loss = 0.902\n",
      "Epoch  56 Batch   34/41   train_loss = 0.015  valid_loss = 0.902\n",
      "Epoch  57 Batch    3/41   train_loss = 0.014  valid_loss = 0.901\n",
      "Epoch  57 Batch   13/41   train_loss = 0.018  valid_loss = 0.901\n",
      "Epoch  57 Batch   23/41   train_loss = 0.016  valid_loss = 0.900\n",
      "Epoch  57 Batch   33/41   train_loss = 0.014  valid_loss = 0.900\n",
      "Epoch  58 Batch    2/41   train_loss = 0.014  valid_loss = 0.900\n",
      "Epoch  58 Batch   12/41   train_loss = 0.016  valid_loss = 0.901\n",
      "Epoch  58 Batch   22/41   train_loss = 0.016  valid_loss = 0.900\n",
      "Epoch  58 Batch   32/41   train_loss = 0.013  valid_loss = 0.901\n",
      "Epoch  59 Batch    1/41   train_loss = 0.018  valid_loss = 0.900\n",
      "Epoch  59 Batch   11/41   train_loss = 0.015  valid_loss = 0.901\n",
      "Epoch  59 Batch   21/41   train_loss = 0.014  valid_loss = 0.899\n",
      "Epoch  59 Batch   31/41   train_loss = 0.012  valid_loss = 0.900\n",
      "Epoch  60 Batch    0/41   train_loss = 0.014  valid_loss = 0.899\n",
      "Epoch  60 Batch   10/41   train_loss = 0.012  valid_loss = 0.899\n",
      "Epoch  60 Batch   20/41   train_loss = 0.015  valid_loss = 0.898\n",
      "Epoch  60 Batch   30/41   train_loss = 0.013  valid_loss = 0.898\n",
      "Epoch  60 Batch   40/41   train_loss = 0.013  valid_loss = 0.898\n",
      "Epoch  61 Batch    9/41   train_loss = 0.013  valid_loss = 0.899\n",
      "Epoch  61 Batch   19/41   train_loss = 0.017  valid_loss = 0.897\n",
      "Epoch  61 Batch   29/41   train_loss = 0.010  valid_loss = 0.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  61 Batch   39/41   train_loss = 0.012  valid_loss = 0.897\n",
      "Epoch  62 Batch    8/41   train_loss = 0.011  valid_loss = 0.897\n",
      "Epoch  62 Batch   18/41   train_loss = 0.013  valid_loss = 0.897\n",
      "Epoch  62 Batch   28/41   train_loss = 0.012  valid_loss = 0.897\n",
      "Epoch  62 Batch   38/41   train_loss = 0.013  valid_loss = 0.897\n",
      "Epoch  63 Batch    7/41   train_loss = 0.012  valid_loss = 0.897\n",
      "Epoch  63 Batch   17/41   train_loss = 0.010  valid_loss = 0.896\n",
      "Epoch  63 Batch   27/41   train_loss = 0.013  valid_loss = 0.895\n",
      "Epoch  63 Batch   37/41   train_loss = 0.011  valid_loss = 0.896\n",
      "Epoch  64 Batch    6/41   train_loss = 0.010  valid_loss = 0.896\n",
      "Epoch  64 Batch   16/41   train_loss = 0.012  valid_loss = 0.895\n",
      "Epoch  64 Batch   26/41   train_loss = 0.012  valid_loss = 0.895\n",
      "Epoch  64 Batch   36/41   train_loss = 0.011  valid_loss = 0.895\n",
      "Epoch  65 Batch    5/41   train_loss = 0.009  valid_loss = 0.895\n",
      "Epoch  65 Batch   15/41   train_loss = 0.010  valid_loss = 0.895\n",
      "Epoch  65 Batch   25/41   train_loss = 0.009  valid_loss = 0.894\n",
      "Epoch  65 Batch   35/41   train_loss = 0.010  valid_loss = 0.895\n",
      "Epoch  66 Batch    4/41   train_loss = 0.011  valid_loss = 0.894\n",
      "Epoch  66 Batch   14/41   train_loss = 0.009  valid_loss = 0.895\n",
      "Epoch  66 Batch   24/41   train_loss = 0.011  valid_loss = 0.894\n",
      "Epoch  66 Batch   34/41   train_loss = 0.009  valid_loss = 0.895\n",
      "Epoch  67 Batch    3/41   train_loss = 0.009  valid_loss = 0.893\n",
      "Epoch  67 Batch   13/41   train_loss = 0.010  valid_loss = 0.895\n",
      "Epoch  67 Batch   23/41   train_loss = 0.011  valid_loss = 0.895\n",
      "Epoch  67 Batch   33/41   train_loss = 0.009  valid_loss = 0.894\n",
      "Epoch  68 Batch    2/41   train_loss = 0.008  valid_loss = 0.893\n",
      "Epoch  68 Batch   12/41   train_loss = 0.009  valid_loss = 0.893\n",
      "Epoch  68 Batch   22/41   train_loss = 0.012  valid_loss = 0.893\n",
      "Epoch  68 Batch   32/41   train_loss = 0.008  valid_loss = 0.892\n",
      "Epoch  69 Batch    1/41   train_loss = 0.012  valid_loss = 0.893\n",
      "Epoch  69 Batch   11/41   train_loss = 0.009  valid_loss = 0.892\n",
      "Epoch  69 Batch   21/41   train_loss = 0.010  valid_loss = 0.892\n",
      "Epoch  69 Batch   31/41   train_loss = 0.007  valid_loss = 0.890\n",
      "Epoch  70 Batch    0/41   train_loss = 0.009  valid_loss = 0.891\n",
      "Epoch  70 Batch   10/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  70 Batch   20/41   train_loss = 0.009  valid_loss = 0.890\n",
      "Epoch  70 Batch   30/41   train_loss = 0.009  valid_loss = 0.891\n",
      "Epoch  70 Batch   40/41   train_loss = 0.009  valid_loss = 0.895\n",
      "Epoch  71 Batch    9/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  71 Batch   19/41   train_loss = 0.010  valid_loss = 0.890\n",
      "Epoch  71 Batch   29/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  71 Batch   39/41   train_loss = 0.009  valid_loss = 0.891\n",
      "Epoch  72 Batch    8/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  72 Batch   18/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  72 Batch   28/41   train_loss = 0.008  valid_loss = 0.891\n",
      "Epoch  72 Batch   38/41   train_loss = 0.007  valid_loss = 0.892\n",
      "Epoch  73 Batch    7/41   train_loss = 0.008  valid_loss = 0.891\n",
      "Epoch  73 Batch   17/41   train_loss = 0.008  valid_loss = 0.891\n",
      "Epoch  73 Batch   27/41   train_loss = 0.010  valid_loss = 0.890\n",
      "Epoch  73 Batch   37/41   train_loss = 0.008  valid_loss = 0.890\n",
      "Epoch  74 Batch    6/41   train_loss = 0.008  valid_loss = 0.891\n",
      "Epoch  74 Batch   16/41   train_loss = 0.008  valid_loss = 0.891\n",
      "Epoch  74 Batch   26/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  74 Batch   36/41   train_loss = 0.007  valid_loss = 0.893\n",
      "Epoch  75 Batch    5/41   train_loss = 0.006  valid_loss = 0.892\n",
      "Epoch  75 Batch   15/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  75 Batch   25/41   train_loss = 0.006  valid_loss = 0.890\n",
      "Epoch  75 Batch   35/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  76 Batch    4/41   train_loss = 0.008  valid_loss = 0.892\n",
      "Epoch  76 Batch   14/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  76 Batch   24/41   train_loss = 0.007  valid_loss = 0.890\n",
      "Epoch  76 Batch   34/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  77 Batch    3/41   train_loss = 0.005  valid_loss = 0.892\n",
      "Epoch  77 Batch   13/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  77 Batch   23/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  77 Batch   33/41   train_loss = 0.006  valid_loss = 0.890\n",
      "Epoch  78 Batch    2/41   train_loss = 0.005  valid_loss = 0.891\n",
      "Epoch  78 Batch   12/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  78 Batch   22/41   train_loss = 0.008  valid_loss = 0.891\n",
      "Epoch  78 Batch   32/41   train_loss = 0.006  valid_loss = 0.889\n",
      "Epoch  79 Batch    1/41   train_loss = 0.007  valid_loss = 0.890\n",
      "Epoch  79 Batch   11/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  79 Batch   21/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  79 Batch   31/41   train_loss = 0.006  valid_loss = 0.890\n",
      "Epoch  80 Batch    0/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  80 Batch   10/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  80 Batch   20/41   train_loss = 0.006  valid_loss = 0.890\n",
      "Epoch  80 Batch   30/41   train_loss = 0.006  valid_loss = 0.889\n",
      "Epoch  80 Batch   40/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  81 Batch    9/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  81 Batch   19/41   train_loss = 0.007  valid_loss = 0.889\n",
      "Epoch  81 Batch   29/41   train_loss = 0.006  valid_loss = 0.888\n",
      "Epoch  81 Batch   39/41   train_loss = 0.005  valid_loss = 0.890\n",
      "Epoch  82 Batch    8/41   train_loss = 0.006  valid_loss = 0.890\n",
      "Epoch  82 Batch   18/41   train_loss = 0.005  valid_loss = 0.890\n",
      "Epoch  82 Batch   28/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  82 Batch   38/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  83 Batch    7/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  83 Batch   17/41   train_loss = 0.005  valid_loss = 0.888\n",
      "Epoch  83 Batch   27/41   train_loss = 0.007  valid_loss = 0.888\n",
      "Epoch  83 Batch   37/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  84 Batch    6/41   train_loss = 0.006  valid_loss = 0.889\n",
      "Epoch  84 Batch   16/41   train_loss = 0.006  valid_loss = 0.890\n",
      "Epoch  84 Batch   26/41   train_loss = 0.005  valid_loss = 0.891\n",
      "Epoch  84 Batch   36/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  85 Batch    5/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  85 Batch   15/41   train_loss = 0.005  valid_loss = 0.890\n",
      "Epoch  85 Batch   25/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  85 Batch   35/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  86 Batch    4/41   train_loss = 0.005  valid_loss = 0.890\n",
      "Epoch  86 Batch   14/41   train_loss = 0.004  valid_loss = 0.891\n",
      "Epoch  86 Batch   24/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  86 Batch   34/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  87 Batch    3/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  87 Batch   13/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  87 Batch   23/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  87 Batch   33/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  88 Batch    2/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  88 Batch   12/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  88 Batch   22/41   train_loss = 0.007  valid_loss = 0.888\n",
      "Epoch  88 Batch   32/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  89 Batch    1/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  89 Batch   11/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch  89 Batch   21/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  89 Batch   31/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  90 Batch    0/41   train_loss = 0.005  valid_loss = 0.891\n",
      "Epoch  90 Batch   10/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  90 Batch   20/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  90 Batch   30/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  90 Batch   40/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  91 Batch    9/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  91 Batch   19/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  91 Batch   29/41   train_loss = 0.005  valid_loss = 0.888\n",
      "Epoch  91 Batch   39/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  92 Batch    8/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  92 Batch   18/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  92 Batch   28/41   train_loss = 0.004  valid_loss = 0.891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  92 Batch   38/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  93 Batch    7/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  93 Batch   17/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch  93 Batch   27/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  93 Batch   37/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  94 Batch    6/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  94 Batch   16/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  94 Batch   26/41   train_loss = 0.004  valid_loss = 0.893\n",
      "Epoch  94 Batch   36/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  95 Batch    5/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch  95 Batch   15/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  95 Batch   25/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch  95 Batch   35/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  96 Batch    4/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  96 Batch   14/41   train_loss = 0.003  valid_loss = 0.890\n",
      "Epoch  96 Batch   24/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  96 Batch   34/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch  97 Batch    3/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  97 Batch   13/41   train_loss = 0.003  valid_loss = 0.890\n",
      "Epoch  97 Batch   23/41   train_loss = 0.003  valid_loss = 0.892\n",
      "Epoch  97 Batch   33/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch  98 Batch    2/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  98 Batch   12/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch  98 Batch   22/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch  98 Batch   32/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch  99 Batch    1/41   train_loss = 0.005  valid_loss = 0.886\n",
      "Epoch  99 Batch   11/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch  99 Batch   21/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch  99 Batch   31/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch 100 Batch    0/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 100 Batch   10/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 100 Batch   20/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 100 Batch   30/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch 100 Batch   40/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 101 Batch    9/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch 101 Batch   19/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch 101 Batch   29/41   train_loss = 0.003  valid_loss = 0.893\n",
      "Epoch 101 Batch   39/41   train_loss = 0.002  valid_loss = 0.888\n",
      "Epoch 102 Batch    8/41   train_loss = 0.003  valid_loss = 0.890\n",
      "Epoch 102 Batch   18/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 102 Batch   28/41   train_loss = 0.003  valid_loss = 0.897\n",
      "Epoch 102 Batch   38/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch 103 Batch    7/41   train_loss = 0.003  valid_loss = 0.894\n",
      "Epoch 103 Batch   17/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch 103 Batch   27/41   train_loss = 0.005  valid_loss = 0.892\n",
      "Epoch 103 Batch   37/41   train_loss = 0.002  valid_loss = 0.887\n",
      "Epoch 104 Batch    6/41   train_loss = 0.003  valid_loss = 0.890\n",
      "Epoch 104 Batch   16/41   train_loss = 0.002  valid_loss = 0.888\n",
      "Epoch 104 Batch   26/41   train_loss = 0.003  valid_loss = 0.896\n",
      "Epoch 104 Batch   36/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch 105 Batch    5/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch 105 Batch   15/41   train_loss = 0.002  valid_loss = 0.888\n",
      "Epoch 105 Batch   25/41   train_loss = 0.002  valid_loss = 0.889\n",
      "Epoch 105 Batch   35/41   train_loss = 0.002  valid_loss = 0.887\n",
      "Epoch 106 Batch    4/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch 106 Batch   14/41   train_loss = 0.002  valid_loss = 0.889\n",
      "Epoch 106 Batch   24/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch 106 Batch   34/41   train_loss = 0.002  valid_loss = 0.888\n",
      "Epoch 107 Batch    3/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch 107 Batch   13/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch 107 Batch   23/41   train_loss = 0.002  valid_loss = 0.883\n",
      "Epoch 107 Batch   33/41   train_loss = 0.002  valid_loss = 0.886\n",
      "Epoch 108 Batch    2/41   train_loss = 0.002  valid_loss = 0.885\n",
      "Epoch 108 Batch   12/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 108 Batch   22/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 108 Batch   32/41   train_loss = 0.002  valid_loss = 0.898\n",
      "Epoch 109 Batch    1/41   train_loss = 0.004  valid_loss = 0.891\n",
      "Epoch 109 Batch   11/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch 109 Batch   21/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch 109 Batch   31/41   train_loss = 0.002  valid_loss = 0.906\n",
      "Epoch 110 Batch    0/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch 110 Batch   10/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 110 Batch   20/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch 110 Batch   30/41   train_loss = 0.003  valid_loss = 0.898\n",
      "Epoch 110 Batch   40/41   train_loss = 0.002  valid_loss = 0.893\n",
      "Epoch 111 Batch    9/41   train_loss = 0.003  valid_loss = 0.894\n",
      "Epoch 111 Batch   19/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch 111 Batch   29/41   train_loss = 0.003  valid_loss = 0.892\n",
      "Epoch 111 Batch   39/41   train_loss = 0.002  valid_loss = 0.886\n",
      "Epoch 112 Batch    8/41   train_loss = 0.002  valid_loss = 0.886\n",
      "Epoch 112 Batch   18/41   train_loss = 0.002  valid_loss = 0.889\n",
      "Epoch 112 Batch   28/41   train_loss = 0.002  valid_loss = 0.898\n",
      "Epoch 112 Batch   38/41   train_loss = 0.002  valid_loss = 0.886\n",
      "Epoch 113 Batch    7/41   train_loss = 0.002  valid_loss = 0.887\n",
      "Epoch 113 Batch   17/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 113 Batch   27/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch 113 Batch   37/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 114 Batch    6/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 114 Batch   16/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 114 Batch   26/41   train_loss = 0.002  valid_loss = 0.884\n",
      "Epoch 114 Batch   36/41   train_loss = 0.002  valid_loss = 0.890\n",
      "Epoch 115 Batch    5/41   train_loss = 0.002  valid_loss = 0.884\n",
      "Epoch 115 Batch   15/41   train_loss = 0.002  valid_loss = 0.890\n",
      "Epoch 115 Batch   25/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch 115 Batch   35/41   train_loss = 0.005  valid_loss = 0.910\n",
      "Epoch 116 Batch    4/41   train_loss = 0.004  valid_loss = 0.892\n",
      "Epoch 116 Batch   14/41   train_loss = 0.002  valid_loss = 0.896\n",
      "Epoch 116 Batch   24/41   train_loss = 0.004  valid_loss = 0.894\n",
      "Epoch 116 Batch   34/41   train_loss = 0.003  valid_loss = 0.917\n",
      "Epoch 117 Batch    3/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 117 Batch   13/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 117 Batch   23/41   train_loss = 0.002  valid_loss = 0.882\n",
      "Epoch 117 Batch   33/41   train_loss = 0.002  valid_loss = 0.894\n",
      "Epoch 118 Batch    2/41   train_loss = 0.002  valid_loss = 0.890\n",
      "Epoch 118 Batch   12/41   train_loss = 0.002  valid_loss = 0.897\n",
      "Epoch 118 Batch   22/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch 118 Batch   32/41   train_loss = 0.002  valid_loss = 0.883\n",
      "Epoch 119 Batch    1/41   train_loss = 0.003  valid_loss = 0.879\n",
      "Epoch 119 Batch   11/41   train_loss = 0.002  valid_loss = 0.882\n",
      "Epoch 119 Batch   21/41   train_loss = 0.002  valid_loss = 0.885\n",
      "Epoch 119 Batch   31/41   train_loss = 0.002  valid_loss = 0.889\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuEVEsoyCW0Qkyyi4RUSyzP8BGbsfM4sS8VAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f561e2f1278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_index=int(0.8*len(int_text))\n",
    "train_source = int_text[:split_index]\n",
    "valid_source = int_text[split_index:]\n",
    "\n",
    "train_label= int_label[:split_index]\n",
    "valid_label= int_label[split_index:]\n",
    "\n",
    "#print(len(valid_label))\n",
    "train_batches = get_batches(train_source,train_label, batch_size, seq_length)\n",
    "valid_batches=get_batches(valid_source,valid_label, len(valid_label)//seq_length, seq_length)\n",
    "valid_x=valid_batches[0][0]\n",
    "valid_y=valid_batches[0][1]\n",
    "#print(valid_x)\n",
    "#print(valid_y)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_train_loss=[]\n",
    "    batch_valid_loss=[]\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(train_batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                lr: learning_rate,\n",
    "                keep_prob:0.5}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            batch_train_loss.append(train_loss)\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(train_batches) + batch_i) % show_every_n_batches == 0:\n",
    "                feed_valid={\n",
    "                input_text: valid_x,\n",
    "                keep_prob:1,\n",
    "                targets: valid_y}\n",
    "                valid_loss,= sess.run([cost], feed_valid)\n",
    "                batch_valid_loss.append(valid_loss)\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}  valid_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(train_batches),\n",
    "                    train_loss,\n",
    "                    valid_loss))\n",
    "                \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    \n",
    "    plt.plot(batch_train_loss, label='train loss')\n",
    "    plt.plot(batch_valid_loss, label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restore the data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab= pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))\n",
    "token_original,len_original,token_obfused,len_obfused=pickle.load(open('twotaledumped.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, ProbsTensor, KeepprobTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return loaded_graph.get_tensor_by_name(\"input:0\"), \\\n",
    "           loaded_graph.get_tensor_by_name(\"probs:0\"),\\\n",
    "           loaded_graph.get_tensor_by_name(\"keep_prob:0\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text,vocab_to_int):\n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the original novel \"A Tale of Two Cities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Proceeding 1/5809\n",
      "Proceeding 11/5809\n",
      "Proceeding 21/5809\n",
      "Proceeding 31/5809\n",
      "Proceeding 41/5809\n",
      "Proceeding 51/5809\n",
      "Proceeding 61/5809\n",
      "Proceeding 71/5809\n",
      "Proceeding 81/5809\n",
      "Proceeding 91/5809\n",
      "Proceeding 101/5809\n",
      "Proceeding 111/5809\n",
      "Proceeding 121/5809\n",
      "Proceeding 131/5809\n",
      "Proceeding 141/5809\n",
      "Proceeding 151/5809\n",
      "Proceeding 161/5809\n",
      "Proceeding 171/5809\n",
      "Proceeding 181/5809\n",
      "Proceeding 191/5809\n",
      "Proceeding 201/5809\n",
      "Proceeding 211/5809\n",
      "Proceeding 221/5809\n",
      "Proceeding 231/5809\n",
      "Proceeding 241/5809\n",
      "Proceeding 251/5809\n",
      "Proceeding 261/5809\n",
      "Proceeding 271/5809\n",
      "Proceeding 281/5809\n",
      "Proceeding 291/5809\n",
      "Proceeding 301/5809\n",
      "Proceeding 311/5809\n",
      "Proceeding 321/5809\n",
      "Proceeding 331/5809\n",
      "Proceeding 341/5809\n",
      "Proceeding 351/5809\n",
      "Proceeding 361/5809\n",
      "Proceeding 371/5809\n",
      "Proceeding 381/5809\n",
      "Proceeding 391/5809\n",
      "Proceeding 401/5809\n",
      "Proceeding 411/5809\n",
      "Proceeding 421/5809\n",
      "Proceeding 431/5809\n",
      "Proceeding 441/5809\n",
      "Proceeding 451/5809\n",
      "Proceeding 461/5809\n",
      "Proceeding 471/5809\n",
      "Proceeding 481/5809\n",
      "Proceeding 491/5809\n",
      "Proceeding 501/5809\n",
      "Proceeding 511/5809\n",
      "Proceeding 521/5809\n",
      "Proceeding 531/5809\n",
      "Proceeding 541/5809\n",
      "Proceeding 551/5809\n",
      "Proceeding 561/5809\n",
      "Proceeding 571/5809\n",
      "Proceeding 581/5809\n",
      "Proceeding 591/5809\n",
      "Proceeding 601/5809\n",
      "Proceeding 611/5809\n",
      "Proceeding 621/5809\n",
      "Proceeding 631/5809\n",
      "Proceeding 641/5809\n",
      "Proceeding 651/5809\n",
      "Proceeding 661/5809\n",
      "Proceeding 671/5809\n",
      "Proceeding 681/5809\n",
      "Proceeding 691/5809\n",
      "Proceeding 701/5809\n",
      "Proceeding 711/5809\n",
      "Proceeding 721/5809\n",
      "Proceeding 731/5809\n",
      "Proceeding 741/5809\n",
      "Proceeding 751/5809\n",
      "Proceeding 761/5809\n",
      "Proceeding 771/5809\n",
      "Proceeding 781/5809\n",
      "Proceeding 791/5809\n",
      "Proceeding 801/5809\n",
      "Proceeding 811/5809\n",
      "Proceeding 821/5809\n",
      "Proceeding 831/5809\n",
      "Proceeding 841/5809\n",
      "Proceeding 851/5809\n",
      "Proceeding 861/5809\n",
      "Proceeding 871/5809\n",
      "Proceeding 881/5809\n",
      "Proceeding 891/5809\n",
      "Proceeding 901/5809\n",
      "Proceeding 911/5809\n",
      "Proceeding 921/5809\n",
      "Proceeding 931/5809\n",
      "Proceeding 941/5809\n",
      "Proceeding 951/5809\n",
      "Proceeding 961/5809\n",
      "Proceeding 971/5809\n",
      "Proceeding 981/5809\n",
      "Proceeding 991/5809\n",
      "Proceeding 1001/5809\n",
      "Proceeding 1011/5809\n",
      "Proceeding 1021/5809\n",
      "Proceeding 1031/5809\n",
      "Proceeding 1041/5809\n",
      "Proceeding 1051/5809\n",
      "Proceeding 1061/5809\n",
      "Proceeding 1071/5809\n",
      "Proceeding 1081/5809\n",
      "Proceeding 1091/5809\n",
      "Proceeding 1101/5809\n",
      "Proceeding 1111/5809\n",
      "Proceeding 1121/5809\n",
      "Proceeding 1131/5809\n",
      "Proceeding 1141/5809\n",
      "Proceeding 1151/5809\n",
      "Proceeding 1161/5809\n",
      "Proceeding 1171/5809\n",
      "Proceeding 1181/5809\n",
      "Proceeding 1191/5809\n",
      "Proceeding 1201/5809\n",
      "Proceeding 1211/5809\n",
      "Proceeding 1221/5809\n",
      "Proceeding 1231/5809\n",
      "Proceeding 1241/5809\n",
      "Proceeding 1251/5809\n",
      "Proceeding 1261/5809\n",
      "Proceeding 1271/5809\n",
      "Proceeding 1281/5809\n",
      "Proceeding 1291/5809\n",
      "Proceeding 1301/5809\n",
      "Proceeding 1311/5809\n",
      "Proceeding 1321/5809\n",
      "Proceeding 1331/5809\n",
      "Proceeding 1341/5809\n",
      "Proceeding 1351/5809\n",
      "Proceeding 1361/5809\n",
      "Proceeding 1371/5809\n",
      "Proceeding 1381/5809\n",
      "Proceeding 1391/5809\n",
      "Proceeding 1401/5809\n",
      "Proceeding 1411/5809\n",
      "Proceeding 1421/5809\n",
      "Proceeding 1431/5809\n",
      "Proceeding 1441/5809\n",
      "Proceeding 1451/5809\n",
      "Proceeding 1461/5809\n",
      "Proceeding 1471/5809\n",
      "Proceeding 1481/5809\n",
      "Proceeding 1491/5809\n",
      "Proceeding 1501/5809\n",
      "Proceeding 1511/5809\n",
      "Proceeding 1521/5809\n",
      "Proceeding 1531/5809\n",
      "Proceeding 1541/5809\n",
      "Proceeding 1551/5809\n",
      "Proceeding 1561/5809\n",
      "Proceeding 1571/5809\n",
      "Proceeding 1581/5809\n",
      "Proceeding 1591/5809\n",
      "Proceeding 1601/5809\n",
      "Proceeding 1611/5809\n",
      "Proceeding 1621/5809\n",
      "Proceeding 1631/5809\n",
      "Proceeding 1641/5809\n",
      "Proceeding 1651/5809\n",
      "Proceeding 1661/5809\n",
      "Proceeding 1671/5809\n",
      "Proceeding 1681/5809\n",
      "Proceeding 1691/5809\n",
      "Proceeding 1701/5809\n",
      "Proceeding 1711/5809\n",
      "Proceeding 1721/5809\n",
      "Proceeding 1731/5809\n",
      "Proceeding 1741/5809\n",
      "Proceeding 1751/5809\n",
      "Proceeding 1761/5809\n",
      "Proceeding 1771/5809\n",
      "Proceeding 1781/5809\n",
      "Proceeding 1791/5809\n",
      "Proceeding 1801/5809\n",
      "Proceeding 1811/5809\n",
      "Proceeding 1821/5809\n",
      "Proceeding 1831/5809\n",
      "Proceeding 1841/5809\n",
      "Proceeding 1851/5809\n",
      "Proceeding 1861/5809\n",
      "Proceeding 1871/5809\n",
      "Proceeding 1881/5809\n",
      "Proceeding 1891/5809\n",
      "Proceeding 1901/5809\n",
      "Proceeding 1911/5809\n",
      "Proceeding 1921/5809\n",
      "Proceeding 1931/5809\n",
      "Proceeding 1941/5809\n",
      "Proceeding 1951/5809\n",
      "Proceeding 1961/5809\n",
      "Proceeding 1971/5809\n",
      "Proceeding 1981/5809\n",
      "Proceeding 1991/5809\n",
      "Proceeding 2001/5809\n",
      "Proceeding 2011/5809\n",
      "Proceeding 2021/5809\n",
      "Proceeding 2031/5809\n",
      "Proceeding 2041/5809\n",
      "Proceeding 2051/5809\n",
      "Proceeding 2061/5809\n",
      "Proceeding 2071/5809\n",
      "Proceeding 2081/5809\n",
      "Proceeding 2091/5809\n",
      "Proceeding 2101/5809\n",
      "Proceeding 2111/5809\n",
      "Proceeding 2121/5809\n",
      "Proceeding 2131/5809\n",
      "Proceeding 2141/5809\n",
      "Proceeding 2151/5809\n",
      "Proceeding 2161/5809\n",
      "Proceeding 2171/5809\n",
      "Proceeding 2181/5809\n",
      "Proceeding 2191/5809\n",
      "Proceeding 2201/5809\n",
      "Proceeding 2211/5809\n",
      "Proceeding 2221/5809\n",
      "Proceeding 2231/5809\n",
      "Proceeding 2241/5809\n",
      "Proceeding 2251/5809\n",
      "Proceeding 2261/5809\n",
      "Proceeding 2271/5809\n",
      "Proceeding 2281/5809\n",
      "Proceeding 2291/5809\n",
      "Proceeding 2301/5809\n",
      "Proceeding 2311/5809\n",
      "Proceeding 2321/5809\n",
      "Proceeding 2331/5809\n",
      "Proceeding 2341/5809\n",
      "Proceeding 2351/5809\n",
      "Proceeding 2361/5809\n",
      "Proceeding 2371/5809\n",
      "Proceeding 2381/5809\n",
      "Proceeding 2391/5809\n",
      "Proceeding 2401/5809\n",
      "Proceeding 2411/5809\n",
      "Proceeding 2421/5809\n",
      "Proceeding 2431/5809\n",
      "Proceeding 2441/5809\n",
      "Proceeding 2451/5809\n",
      "Proceeding 2461/5809\n",
      "Proceeding 2471/5809\n",
      "Proceeding 2481/5809\n",
      "Proceeding 2491/5809\n",
      "Proceeding 2501/5809\n",
      "Proceeding 2511/5809\n",
      "Proceeding 2521/5809\n",
      "Proceeding 2531/5809\n",
      "Proceeding 2541/5809\n",
      "Proceeding 2551/5809\n",
      "Proceeding 2561/5809\n",
      "Proceeding 2571/5809\n",
      "Proceeding 2581/5809\n",
      "Proceeding 2591/5809\n",
      "Proceeding 2601/5809\n",
      "Proceeding 2611/5809\n",
      "Proceeding 2621/5809\n",
      "Proceeding 2631/5809\n",
      "Proceeding 2641/5809\n",
      "Proceeding 2651/5809\n",
      "Proceeding 2661/5809\n",
      "Proceeding 2671/5809\n",
      "Proceeding 2681/5809\n",
      "Proceeding 2691/5809\n",
      "Proceeding 2701/5809\n",
      "Proceeding 2711/5809\n",
      "Proceeding 2721/5809\n",
      "Proceeding 2731/5809\n",
      "Proceeding 2741/5809\n",
      "Proceeding 2751/5809\n",
      "Proceeding 2761/5809\n",
      "Proceeding 2771/5809\n",
      "Proceeding 2781/5809\n",
      "Proceeding 2791/5809\n",
      "Proceeding 2801/5809\n",
      "Proceeding 2811/5809\n",
      "Proceeding 2821/5809\n",
      "Proceeding 2831/5809\n",
      "Proceeding 2841/5809\n",
      "Proceeding 2851/5809\n",
      "Proceeding 2861/5809\n",
      "Proceeding 2871/5809\n",
      "Proceeding 2881/5809\n",
      "Proceeding 2891/5809\n",
      "Proceeding 2901/5809\n",
      "Proceeding 2911/5809\n",
      "Proceeding 2921/5809\n",
      "Proceeding 2931/5809\n",
      "Proceeding 2941/5809\n",
      "Proceeding 2951/5809\n",
      "Proceeding 2961/5809\n",
      "Proceeding 2971/5809\n",
      "Proceeding 2981/5809\n",
      "Proceeding 2991/5809\n",
      "Proceeding 3001/5809\n",
      "Proceeding 3011/5809\n",
      "Proceeding 3021/5809\n",
      "Proceeding 3031/5809\n",
      "Proceeding 3041/5809\n",
      "Proceeding 3051/5809\n",
      "Proceeding 3061/5809\n",
      "Proceeding 3071/5809\n",
      "Proceeding 3081/5809\n",
      "Proceeding 3091/5809\n",
      "Proceeding 3101/5809\n",
      "Proceeding 3111/5809\n",
      "Proceeding 3121/5809\n",
      "Proceeding 3131/5809\n",
      "Proceeding 3141/5809\n",
      "Proceeding 3151/5809\n",
      "Proceeding 3161/5809\n",
      "Proceeding 3171/5809\n",
      "Proceeding 3181/5809\n",
      "Proceeding 3191/5809\n",
      "Proceeding 3201/5809\n",
      "Proceeding 3211/5809\n",
      "Proceeding 3221/5809\n",
      "Proceeding 3231/5809\n",
      "Proceeding 3241/5809\n",
      "Proceeding 3251/5809\n",
      "Proceeding 3261/5809\n",
      "Proceeding 3271/5809\n",
      "Proceeding 3281/5809\n",
      "Proceeding 3291/5809\n",
      "Proceeding 3301/5809\n",
      "Proceeding 3311/5809\n",
      "Proceeding 3321/5809\n",
      "Proceeding 3331/5809\n",
      "Proceeding 3341/5809\n",
      "Proceeding 3351/5809\n",
      "Proceeding 3361/5809\n",
      "Proceeding 3371/5809\n",
      "Proceeding 3381/5809\n",
      "Proceeding 3391/5809\n",
      "Proceeding 3401/5809\n",
      "Proceeding 3411/5809\n",
      "Proceeding 3421/5809\n",
      "Proceeding 3431/5809\n",
      "Proceeding 3441/5809\n",
      "Proceeding 3451/5809\n",
      "Proceeding 3461/5809\n",
      "Proceeding 3471/5809\n",
      "Proceeding 3481/5809\n",
      "Proceeding 3491/5809\n",
      "Proceeding 3501/5809\n",
      "Proceeding 3511/5809\n",
      "Proceeding 3521/5809\n",
      "Proceeding 3531/5809\n",
      "Proceeding 3541/5809\n",
      "Proceeding 3551/5809\n",
      "Proceeding 3561/5809\n",
      "Proceeding 3571/5809\n",
      "Proceeding 3581/5809\n",
      "Proceeding 3591/5809\n",
      "Proceeding 3601/5809\n",
      "Proceeding 3611/5809\n",
      "Proceeding 3621/5809\n",
      "Proceeding 3631/5809\n",
      "Proceeding 3641/5809\n",
      "Proceeding 3651/5809\n",
      "Proceeding 3661/5809\n",
      "Proceeding 3671/5809\n",
      "Proceeding 3681/5809\n",
      "Proceeding 3691/5809\n",
      "Proceeding 3701/5809\n",
      "Proceeding 3711/5809\n",
      "Proceeding 3721/5809\n",
      "Proceeding 3731/5809\n",
      "Proceeding 3741/5809\n",
      "Proceeding 3751/5809\n",
      "Proceeding 3761/5809\n",
      "Proceeding 3771/5809\n",
      "Proceeding 3781/5809\n",
      "Proceeding 3791/5809\n",
      "Proceeding 3801/5809\n",
      "Proceeding 3811/5809\n",
      "Proceeding 3821/5809\n",
      "Proceeding 3831/5809\n",
      "Proceeding 3841/5809\n",
      "Proceeding 3851/5809\n",
      "Proceeding 3861/5809\n",
      "Proceeding 3871/5809\n",
      "Proceeding 3881/5809\n",
      "Proceeding 3891/5809\n",
      "Proceeding 3901/5809\n",
      "Proceeding 3911/5809\n",
      "Proceeding 3921/5809\n",
      "Proceeding 3931/5809\n",
      "Proceeding 3941/5809\n",
      "Proceeding 3951/5809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding 3961/5809\n",
      "Proceeding 3971/5809\n",
      "Proceeding 3981/5809\n",
      "Proceeding 3991/5809\n",
      "Proceeding 4001/5809\n",
      "Proceeding 4011/5809\n",
      "Proceeding 4021/5809\n",
      "Proceeding 4031/5809\n",
      "Proceeding 4041/5809\n",
      "Proceeding 4051/5809\n",
      "Proceeding 4061/5809\n",
      "Proceeding 4071/5809\n",
      "Proceeding 4081/5809\n",
      "Proceeding 4091/5809\n",
      "Proceeding 4101/5809\n",
      "Proceeding 4111/5809\n",
      "Proceeding 4121/5809\n",
      "Proceeding 4131/5809\n",
      "Proceeding 4141/5809\n",
      "Proceeding 4151/5809\n",
      "Proceeding 4161/5809\n",
      "Proceeding 4171/5809\n",
      "Proceeding 4181/5809\n",
      "Proceeding 4191/5809\n",
      "Proceeding 4201/5809\n",
      "Proceeding 4211/5809\n",
      "Proceeding 4221/5809\n",
      "Proceeding 4231/5809\n",
      "Proceeding 4241/5809\n",
      "Proceeding 4251/5809\n",
      "Proceeding 4261/5809\n",
      "Proceeding 4271/5809\n",
      "Proceeding 4281/5809\n",
      "Proceeding 4291/5809\n",
      "Proceeding 4301/5809\n",
      "Proceeding 4311/5809\n",
      "Proceeding 4321/5809\n",
      "Proceeding 4331/5809\n",
      "Proceeding 4341/5809\n",
      "Proceeding 4351/5809\n",
      "Proceeding 4361/5809\n",
      "Proceeding 4371/5809\n",
      "Proceeding 4381/5809\n",
      "Proceeding 4391/5809\n",
      "Proceeding 4401/5809\n",
      "Proceeding 4411/5809\n",
      "Proceeding 4421/5809\n",
      "Proceeding 4431/5809\n",
      "Proceeding 4441/5809\n",
      "Proceeding 4451/5809\n",
      "Proceeding 4461/5809\n",
      "Proceeding 4471/5809\n",
      "Proceeding 4481/5809\n",
      "Proceeding 4491/5809\n",
      "Proceeding 4501/5809\n",
      "Proceeding 4511/5809\n",
      "Proceeding 4521/5809\n",
      "Proceeding 4531/5809\n",
      "Proceeding 4541/5809\n",
      "Proceeding 4551/5809\n",
      "Proceeding 4561/5809\n",
      "Proceeding 4571/5809\n",
      "Proceeding 4581/5809\n",
      "Proceeding 4591/5809\n",
      "Proceeding 4601/5809\n",
      "Proceeding 4611/5809\n",
      "Proceeding 4621/5809\n",
      "Proceeding 4631/5809\n",
      "Proceeding 4641/5809\n",
      "Proceeding 4651/5809\n",
      "Proceeding 4661/5809\n",
      "Proceeding 4671/5809\n",
      "Proceeding 4681/5809\n",
      "Proceeding 4691/5809\n",
      "Proceeding 4701/5809\n",
      "Proceeding 4711/5809\n",
      "Proceeding 4721/5809\n",
      "Proceeding 4731/5809\n",
      "Proceeding 4741/5809\n",
      "Proceeding 4751/5809\n",
      "Proceeding 4761/5809\n",
      "Proceeding 4771/5809\n",
      "Proceeding 4781/5809\n",
      "Proceeding 4791/5809\n",
      "Proceeding 4801/5809\n",
      "Proceeding 4811/5809\n",
      "Proceeding 4821/5809\n",
      "Proceeding 4831/5809\n",
      "Proceeding 4841/5809\n",
      "Proceeding 4851/5809\n",
      "Proceeding 4861/5809\n",
      "Proceeding 4871/5809\n",
      "Proceeding 4881/5809\n",
      "Proceeding 4891/5809\n",
      "Proceeding 4901/5809\n",
      "Proceeding 4911/5809\n",
      "Proceeding 4921/5809\n",
      "Proceeding 4931/5809\n",
      "Proceeding 4941/5809\n",
      "Proceeding 4951/5809\n",
      "Proceeding 4961/5809\n",
      "Proceeding 4971/5809\n",
      "Proceeding 4981/5809\n",
      "Proceeding 4991/5809\n",
      "Proceeding 5001/5809\n",
      "Proceeding 5011/5809\n",
      "Proceeding 5021/5809\n",
      "Proceeding 5031/5809\n",
      "Proceeding 5041/5809\n",
      "Proceeding 5051/5809\n",
      "Proceeding 5061/5809\n",
      "Proceeding 5071/5809\n",
      "Proceeding 5081/5809\n",
      "Proceeding 5091/5809\n",
      "Proceeding 5101/5809\n",
      "Proceeding 5111/5809\n",
      "Proceeding 5121/5809\n",
      "Proceeding 5131/5809\n",
      "Proceeding 5141/5809\n",
      "Proceeding 5151/5809\n",
      "Proceeding 5161/5809\n",
      "Proceeding 5171/5809\n",
      "Proceeding 5181/5809\n",
      "Proceeding 5191/5809\n",
      "Proceeding 5201/5809\n",
      "Proceeding 5211/5809\n",
      "Proceeding 5221/5809\n",
      "Proceeding 5231/5809\n",
      "Proceeding 5241/5809\n",
      "Proceeding 5251/5809\n",
      "Proceeding 5261/5809\n",
      "Proceeding 5271/5809\n",
      "Proceeding 5281/5809\n",
      "Proceeding 5291/5809\n",
      "Proceeding 5301/5809\n",
      "Proceeding 5311/5809\n",
      "Proceeding 5321/5809\n",
      "Proceeding 5331/5809\n",
      "Proceeding 5341/5809\n",
      "Proceeding 5351/5809\n",
      "Proceeding 5361/5809\n",
      "Proceeding 5371/5809\n",
      "Proceeding 5381/5809\n",
      "Proceeding 5391/5809\n",
      "Proceeding 5401/5809\n",
      "Proceeding 5411/5809\n",
      "Proceeding 5421/5809\n",
      "Proceeding 5431/5809\n",
      "Proceeding 5441/5809\n",
      "Proceeding 5451/5809\n",
      "Proceeding 5461/5809\n",
      "Proceeding 5471/5809\n",
      "Proceeding 5481/5809\n",
      "Proceeding 5491/5809\n",
      "Proceeding 5501/5809\n",
      "Proceeding 5511/5809\n",
      "Proceeding 5521/5809\n",
      "Proceeding 5531/5809\n",
      "Proceeding 5541/5809\n",
      "Proceeding 5551/5809\n",
      "Proceeding 5561/5809\n",
      "Proceeding 5571/5809\n",
      "Proceeding 5581/5809\n",
      "Proceeding 5591/5809\n",
      "Proceeding 5601/5809\n",
      "Proceeding 5611/5809\n",
      "Proceeding 5621/5809\n",
      "Proceeding 5631/5809\n",
      "Proceeding 5641/5809\n",
      "Proceeding 5651/5809\n",
      "Proceeding 5661/5809\n",
      "Proceeding 5671/5809\n",
      "Proceeding 5681/5809\n",
      "Proceeding 5691/5809\n",
      "Proceeding 5701/5809\n",
      "Proceeding 5711/5809\n",
      "Proceeding 5721/5809\n",
      "Proceeding 5731/5809\n",
      "Proceeding 5741/5809\n",
      "Proceeding 5751/5809\n",
      "Proceeding 5761/5809\n",
      "Proceeding 5771/5809\n",
      "Proceeding 5781/5809\n",
      "Proceeding 5791/5809\n",
      "Proceeding 5801/5809\n",
      "0.7345013477088949\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, probs,keep_prob = get_tensors(loaded_graph)\n",
    "        \n",
    "    mixnum=0\n",
    "    totalnum=0\n",
    "    \n",
    "    obfused_words=[]\n",
    "    predicted_words=[]\n",
    "    original_words=[]\n",
    "    \n",
    "    for i,sentence_token in enumerate(token_obfused):\n",
    "        if i%10==0:\n",
    "            print(\"Proceeding {}/{}\".format(i+1,len(token_obfused)))\n",
    "        \n",
    "        input_sentence = [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in sentence_token]\n",
    "        obfused_words.extend(token_obfused[i])\n",
    "        \n",
    "        batch_shell = np.zeros((1, len(input_sentence)))\n",
    "        batch_shell[0] = input_sentence\n",
    "        chatbot_logits = sess.run(probs, {input_text: batch_shell,keep_prob:1})[0]\n",
    "        \n",
    "        original_sentence=  [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in token_original[i]]\n",
    "        predicted_sentence= [i for i in np.argmax(chatbot_logits, 1)]\n",
    "        \n",
    "        original_words.extend(token_original[i])\n",
    "        predicted_words.extend([int_to_vocab[i] for i in predicted_sentence])\n",
    "        \n",
    "        mixathe,totalathe=compare_a_the(original_sentence,predicted_sentence,vocab_to_int)\n",
    "        totalnum+=totalathe\n",
    "        mixnum+=mixathe\n",
    "    print(1-mixnum/totalnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123096\n",
      "123096\n",
      "123096\n"
     ]
    }
   ],
   "source": [
    "print(len(original_words))\n",
    "print(len(obfused_words))\n",
    "print(len(predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.tsv', 'w', newline='') as csvfile:\n",
    "    mywriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    mywriter.writerow(['Original', 'Obfused', 'Corrected'])\n",
    "    for i in range(len(original_words)):\n",
    "        mywriter.writerow([original_words[i], obfused_words[i], predicted_words[i]])\n",
    "        \n",
    "with open('a_tale_of_two_city_original.txt','w') as f:\n",
    "    for word in original_words:\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        \n",
    "with open('a_tale_of_two_city_disambiguated.txt','w') as f:\n",
    "    for word in predicted_words:\n",
    "        f.write(word)\n",
    "        f.write(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
