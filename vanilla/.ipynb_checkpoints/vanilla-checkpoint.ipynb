{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    print(\"original vocab_len: \"+str(len(vocab_list)))\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    #max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        data_as_tokens.extend(token_ids)\n",
    "    return data_as_tokens\n",
    "\n",
    "def process_data(datafile, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens= data_to_token_ids(tokenized, vocab_dict, normalize_digits=True)\n",
    "    \n",
    "    data_mixed=[]\n",
    "    for word in data_as_tokens:\n",
    "        if word==vocab_dict[\"the\"] or word==vocab_dict[\"a\"]:\n",
    "            data_mixed.append(vocab_dict[\"the\"] if random.random()<0.5 else vocab_dict[\"a\"])\n",
    "        else:\n",
    "            data_mixed.append(word)\n",
    "            \n",
    "    pickle.dump((data_as_tokens, vocab_dict,  rev_vocab_dict), open('preprocess.p', 'wb'))\n",
    "    \n",
    "    return data_as_tokens, data_mixed,vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text):\n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab_len: 12251\n"
     ]
    }
   ],
   "source": [
    "int_label,int_text, vocab_to_int, int_to_vocab = \\\n",
    "        process_data('original.p', max_vocab_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035501048894626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix,total=compare_a_the(int_label,int_text)\n",
    "mix/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs=tf.placeholder(tf.int32,[None,None],name='input')\n",
    "    targets=tf.placeholder(tf.int32,[None,None],name='targets')\n",
    "    learning_rate=tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, learning_rate,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    def single_cell(keepprob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keepprob)\n",
    "        return drop\n",
    "                                         \n",
    "    cell_fw = tf.contrib.rnn.MultiRNNCell([single_cell(keep_prob) for _ in range(1)])\n",
    "    cell_bw = tf.contrib.rnn.MultiRNNCell([single_cell(keep_prob) for _ in range(1)])\n",
    "    return cell_fw,cell_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_data, name='embed')\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    (cell_fw,cell_bw)=cell\n",
    "    outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,cell_bw=cell_bw, inputs=inputs, dtype=tf.float32)\n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    final_state=tf.concat(final_state, 2)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return (outputs, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs,vocab_size,activation_fn=None)\n",
    "    return (logits, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text,label_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    num_batches = len(int_text) // (batch_size * seq_length)\n",
    "    xtext = int_text[:num_batches * batch_size * seq_length]\n",
    "    ytext = label_text[:num_batches * batch_size * seq_length]\n",
    "    \n",
    "        \n",
    "    xreshape=np.reshape(xtext,[batch_size,-1])\n",
    "    yreshape=np.reshape(ytext,[batch_size,-1])\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0,xreshape.shape[1],seq_length):\n",
    "        xx=xreshape[:,i:i+seq_length]\n",
    "        yy=yreshape[:,i:i+seq_length]\n",
    "        batches.append([xx,yy])\n",
    "        \n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 200\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 200\n",
    "# Sequence Length\n",
    "seq_length = 30\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'\n",
    "\n",
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr,keep_prob = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell= get_init_cell(input_data_shape[0], rnn_size,keep_prob)\n",
    "    logits,final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40156\n",
      "Epoch   0 Batch    0/41   train_loss = 9.393  valid_loss = 9.383\n",
      "Epoch   0 Batch   10/41   train_loss = 8.311  valid_loss = 7.977\n",
      "Epoch   0 Batch   20/41   train_loss = 6.534  valid_loss = 6.605\n",
      "Epoch   0 Batch   30/41   train_loss = 6.442  valid_loss = 6.587\n",
      "Epoch   0 Batch   40/41   train_loss = 6.387  valid_loss = 6.438\n",
      "Epoch   1 Batch    9/41   train_loss = 6.171  valid_loss = 6.279\n",
      "Epoch   1 Batch   19/41   train_loss = 6.069  valid_loss = 6.133\n",
      "Epoch   1 Batch   29/41   train_loss = 5.838  valid_loss = 5.959\n",
      "Epoch   1 Batch   39/41   train_loss = 5.625  valid_loss = 5.764\n",
      "Epoch   2 Batch    8/41   train_loss = 5.455  valid_loss = 5.567\n",
      "Epoch   2 Batch   18/41   train_loss = 5.278  valid_loss = 5.398\n",
      "Epoch   2 Batch   28/41   train_loss = 5.127  valid_loss = 5.250\n",
      "Epoch   2 Batch   38/41   train_loss = 4.915  valid_loss = 5.099\n",
      "Epoch   3 Batch    7/41   train_loss = 4.853  valid_loss = 4.946\n",
      "Epoch   3 Batch   17/41   train_loss = 4.764  valid_loss = 4.795\n",
      "Epoch   3 Batch   27/41   train_loss = 4.482  valid_loss = 4.655\n",
      "Epoch   3 Batch   37/41   train_loss = 4.309  valid_loss = 4.502\n",
      "Epoch   4 Batch    6/41   train_loss = 4.157  valid_loss = 4.350\n",
      "Epoch   4 Batch   16/41   train_loss = 4.072  valid_loss = 4.202\n",
      "Epoch   4 Batch   26/41   train_loss = 3.875  valid_loss = 4.068\n",
      "Epoch   4 Batch   36/41   train_loss = 3.836  valid_loss = 3.932\n",
      "Epoch   5 Batch    5/41   train_loss = 3.629  valid_loss = 3.799\n",
      "Epoch   5 Batch   15/41   train_loss = 3.426  valid_loss = 3.672\n",
      "Epoch   5 Batch   25/41   train_loss = 3.338  valid_loss = 3.559\n",
      "Epoch   5 Batch   35/41   train_loss = 3.178  valid_loss = 3.442\n",
      "Epoch   6 Batch    4/41   train_loss = 3.025  valid_loss = 3.331\n",
      "Epoch   6 Batch   14/41   train_loss = 2.952  valid_loss = 3.223\n",
      "Epoch   6 Batch   24/41   train_loss = 2.940  valid_loss = 3.129\n",
      "Epoch   6 Batch   34/41   train_loss = 2.705  valid_loss = 3.034\n",
      "Epoch   7 Batch    3/41   train_loss = 2.697  valid_loss = 2.943\n",
      "Epoch   7 Batch   13/41   train_loss = 2.720  valid_loss = 2.856\n",
      "Epoch   7 Batch   23/41   train_loss = 2.648  valid_loss = 2.783\n",
      "Epoch   7 Batch   33/41   train_loss = 2.381  valid_loss = 2.707\n",
      "Epoch   8 Batch    2/41   train_loss = 2.293  valid_loss = 2.635\n",
      "Epoch   8 Batch   12/41   train_loss = 2.302  valid_loss = 2.565\n",
      "Epoch   8 Batch   22/41   train_loss = 2.290  valid_loss = 2.508\n",
      "Epoch   8 Batch   32/41   train_loss = 2.023  valid_loss = 2.450\n",
      "Epoch   9 Batch    1/41   train_loss = 2.205  valid_loss = 2.392\n",
      "Epoch   9 Batch   11/41   train_loss = 2.005  valid_loss = 2.334\n",
      "Epoch   9 Batch   21/41   train_loss = 2.015  valid_loss = 2.290\n",
      "Epoch   9 Batch   31/41   train_loss = 1.845  valid_loss = 2.244\n",
      "Epoch  10 Batch    0/41   train_loss = 1.924  valid_loss = 2.197\n",
      "Epoch  10 Batch   10/41   train_loss = 1.786  valid_loss = 2.148\n",
      "Epoch  10 Batch   20/41   train_loss = 1.833  valid_loss = 2.114\n",
      "Epoch  10 Batch   30/41   train_loss = 1.692  valid_loss = 2.077\n",
      "Epoch  10 Batch   40/41   train_loss = 1.673  valid_loss = 2.037\n",
      "Epoch  11 Batch    9/41   train_loss = 1.666  valid_loss = 1.996\n",
      "Epoch  11 Batch   19/41   train_loss = 1.707  valid_loss = 1.966\n",
      "Epoch  11 Batch   29/41   train_loss = 1.546  valid_loss = 1.937\n",
      "Epoch  11 Batch   39/41   train_loss = 1.506  valid_loss = 1.903\n",
      "Epoch  12 Batch    8/41   train_loss = 1.436  valid_loss = 1.866\n",
      "Epoch  12 Batch   18/41   train_loss = 1.495  valid_loss = 1.839\n",
      "Epoch  12 Batch   28/41   train_loss = 1.473  valid_loss = 1.818\n",
      "Epoch  12 Batch   38/41   train_loss = 1.363  valid_loss = 1.788\n",
      "Epoch  13 Batch    7/41   train_loss = 1.402  valid_loss = 1.755\n",
      "Epoch  13 Batch   17/41   train_loss = 1.386  valid_loss = 1.730\n",
      "Epoch  13 Batch   27/41   train_loss = 1.275  valid_loss = 1.717\n",
      "Epoch  13 Batch   37/41   train_loss = 1.180  valid_loss = 1.687\n",
      "Epoch  14 Batch    6/41   train_loss = 1.217  valid_loss = 1.661\n",
      "Epoch  14 Batch   16/41   train_loss = 1.190  valid_loss = 1.635\n",
      "Epoch  14 Batch   26/41   train_loss = 1.153  valid_loss = 1.629\n",
      "Epoch  14 Batch   36/41   train_loss = 1.198  valid_loss = 1.604\n",
      "Epoch  15 Batch    5/41   train_loss = 1.120  valid_loss = 1.580\n",
      "Epoch  15 Batch   15/41   train_loss = 1.069  valid_loss = 1.555\n",
      "Epoch  15 Batch   25/41   train_loss = 1.015  valid_loss = 1.552\n",
      "Epoch  15 Batch   35/41   train_loss = 1.030  valid_loss = 1.534\n",
      "Epoch  16 Batch    4/41   train_loss = 0.975  valid_loss = 1.511\n",
      "Epoch  16 Batch   14/41   train_loss = 0.934  valid_loss = 1.488\n",
      "Epoch  16 Batch   24/41   train_loss = 0.974  valid_loss = 1.483\n",
      "Epoch  16 Batch   34/41   train_loss = 0.889  valid_loss = 1.475\n",
      "Epoch  17 Batch    3/41   train_loss = 0.909  valid_loss = 1.451\n",
      "Epoch  17 Batch   13/41   train_loss = 0.937  valid_loss = 1.431\n",
      "Epoch  17 Batch   23/41   train_loss = 0.901  valid_loss = 1.424\n",
      "Epoch  17 Batch   33/41   train_loss = 0.790  valid_loss = 1.424\n",
      "Epoch  18 Batch    2/41   train_loss = 0.746  valid_loss = 1.401\n",
      "Epoch  18 Batch   12/41   train_loss = 0.799  valid_loss = 1.381\n",
      "Epoch  18 Batch   22/41   train_loss = 0.779  valid_loss = 1.375\n",
      "Epoch  18 Batch   32/41   train_loss = 0.682  valid_loss = 1.376\n",
      "Epoch  19 Batch    1/41   train_loss = 0.855  valid_loss = 1.360\n",
      "Epoch  19 Batch   11/41   train_loss = 0.670  valid_loss = 1.337\n",
      "Epoch  19 Batch   21/41   train_loss = 0.676  valid_loss = 1.332\n",
      "Epoch  19 Batch   31/41   train_loss = 0.640  valid_loss = 1.335\n",
      "Epoch  20 Batch    0/41   train_loss = 0.721  valid_loss = 1.325\n",
      "Epoch  20 Batch   10/41   train_loss = 0.624  valid_loss = 1.301\n",
      "Epoch  20 Batch   20/41   train_loss = 0.642  valid_loss = 1.291\n",
      "Epoch  20 Batch   30/41   train_loss = 0.618  valid_loss = 1.298\n",
      "Epoch  20 Batch   40/41   train_loss = 0.599  valid_loss = 1.294\n",
      "Epoch  21 Batch    9/41   train_loss = 0.589  valid_loss = 1.270\n",
      "Epoch  21 Batch   19/41   train_loss = 0.625  valid_loss = 1.256\n",
      "Epoch  21 Batch   29/41   train_loss = 0.550  valid_loss = 1.263\n",
      "Epoch  21 Batch   39/41   train_loss = 0.551  valid_loss = 1.264\n",
      "Epoch  22 Batch    8/41   train_loss = 0.496  valid_loss = 1.245\n",
      "Epoch  22 Batch   18/41   train_loss = 0.532  valid_loss = 1.227\n",
      "Epoch  22 Batch   28/41   train_loss = 0.566  valid_loss = 1.229\n",
      "Epoch  22 Batch   38/41   train_loss = 0.501  valid_loss = 1.234\n",
      "Epoch  23 Batch    7/41   train_loss = 0.533  valid_loss = 1.224\n",
      "Epoch  23 Batch   17/41   train_loss = 0.482  valid_loss = 1.202\n",
      "Epoch  23 Batch   27/41   train_loss = 0.491  valid_loss = 1.203\n",
      "Epoch  23 Batch   37/41   train_loss = 0.423  valid_loss = 1.206\n",
      "Epoch  24 Batch    6/41   train_loss = 0.460  valid_loss = 1.205\n",
      "Epoch  24 Batch   16/41   train_loss = 0.436  valid_loss = 1.183\n",
      "Epoch  24 Batch   26/41   train_loss = 0.422  valid_loss = 1.176\n",
      "Epoch  24 Batch   36/41   train_loss = 0.417  valid_loss = 1.184\n",
      "Epoch  25 Batch    5/41   train_loss = 0.416  valid_loss = 1.181\n",
      "Epoch  25 Batch   15/41   train_loss = 0.375  valid_loss = 1.165\n",
      "Epoch  25 Batch   25/41   train_loss = 0.373  valid_loss = 1.151\n",
      "Epoch  25 Batch   35/41   train_loss = 0.391  valid_loss = 1.160\n",
      "Epoch  26 Batch    4/41   train_loss = 0.353  valid_loss = 1.160\n",
      "Epoch  26 Batch   14/41   train_loss = 0.324  valid_loss = 1.148\n",
      "Epoch  26 Batch   24/41   train_loss = 0.361  valid_loss = 1.131\n",
      "Epoch  26 Batch   34/41   train_loss = 0.316  valid_loss = 1.135\n",
      "Epoch  27 Batch    3/41   train_loss = 0.326  valid_loss = 1.139\n",
      "Epoch  27 Batch   13/41   train_loss = 0.320  valid_loss = 1.132\n",
      "Epoch  27 Batch   23/41   train_loss = 0.318  valid_loss = 1.117\n",
      "Epoch  27 Batch   33/41   train_loss = 0.285  valid_loss = 1.111\n",
      "Epoch  28 Batch    2/41   train_loss = 0.254  valid_loss = 1.118\n",
      "Epoch  28 Batch   12/41   train_loss = 0.285  valid_loss = 1.114\n",
      "Epoch  28 Batch   22/41   train_loss = 0.257  valid_loss = 1.104\n",
      "Epoch  28 Batch   32/41   train_loss = 0.236  valid_loss = 1.094\n",
      "Epoch  29 Batch    1/41   train_loss = 0.298  valid_loss = 1.096\n",
      "Epoch  29 Batch   11/41   train_loss = 0.218  valid_loss = 1.098\n",
      "Epoch  29 Batch   21/41   train_loss = 0.228  valid_loss = 1.087\n",
      "Epoch  29 Batch   31/41   train_loss = 0.218  valid_loss = 1.082\n",
      "Epoch  30 Batch    0/41   train_loss = 0.247  valid_loss = 1.079\n",
      "Epoch  30 Batch   10/41   train_loss = 0.203  valid_loss = 1.080\n",
      "Epoch  30 Batch   20/41   train_loss = 0.208  valid_loss = 1.073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30 Batch   30/41   train_loss = 0.213  valid_loss = 1.069\n",
      "Epoch  30 Batch   40/41   train_loss = 0.185  valid_loss = 1.066\n",
      "Epoch  31 Batch    9/41   train_loss = 0.179  valid_loss = 1.064\n",
      "Epoch  31 Batch   19/41   train_loss = 0.189  valid_loss = 1.058\n",
      "Epoch  31 Batch   29/41   train_loss = 0.174  valid_loss = 1.055\n",
      "Epoch  31 Batch   39/41   train_loss = 0.174  valid_loss = 1.053\n",
      "Epoch  32 Batch    8/41   train_loss = 0.145  valid_loss = 1.051\n",
      "Epoch  32 Batch   18/41   train_loss = 0.169  valid_loss = 1.045\n",
      "Epoch  32 Batch   28/41   train_loss = 0.176  valid_loss = 1.044\n",
      "Epoch  32 Batch   38/41   train_loss = 0.153  valid_loss = 1.041\n",
      "Epoch  33 Batch    7/41   train_loss = 0.153  valid_loss = 1.038\n",
      "Epoch  33 Batch   17/41   train_loss = 0.138  valid_loss = 1.033\n",
      "Epoch  33 Batch   27/41   train_loss = 0.157  valid_loss = 1.030\n",
      "Epoch  33 Batch   37/41   train_loss = 0.127  valid_loss = 1.031\n",
      "Epoch  34 Batch    6/41   train_loss = 0.128  valid_loss = 1.028\n",
      "Epoch  34 Batch   16/41   train_loss = 0.124  valid_loss = 1.024\n",
      "Epoch  34 Batch   26/41   train_loss = 0.125  valid_loss = 1.018\n",
      "Epoch  34 Batch   36/41   train_loss = 0.120  valid_loss = 1.021\n",
      "Epoch  35 Batch    5/41   train_loss = 0.117  valid_loss = 1.018\n",
      "Epoch  35 Batch   15/41   train_loss = 0.101  valid_loss = 1.015\n",
      "Epoch  35 Batch   25/41   train_loss = 0.110  valid_loss = 1.007\n",
      "Epoch  35 Batch   35/41   train_loss = 0.113  valid_loss = 1.008\n",
      "Epoch  36 Batch    4/41   train_loss = 0.097  valid_loss = 1.009\n",
      "Epoch  36 Batch   14/41   train_loss = 0.091  valid_loss = 1.006\n",
      "Epoch  36 Batch   24/41   train_loss = 0.099  valid_loss = 0.998\n",
      "Epoch  36 Batch   34/41   train_loss = 0.079  valid_loss = 0.996\n",
      "Epoch  37 Batch    3/41   train_loss = 0.084  valid_loss = 0.998\n",
      "Epoch  37 Batch   13/41   train_loss = 0.078  valid_loss = 0.999\n",
      "Epoch  37 Batch   23/41   train_loss = 0.079  valid_loss = 0.992\n",
      "Epoch  37 Batch   33/41   train_loss = 0.074  valid_loss = 0.990\n",
      "Epoch  38 Batch    2/41   train_loss = 0.067  valid_loss = 0.990\n",
      "Epoch  38 Batch   12/41   train_loss = 0.069  valid_loss = 0.990\n",
      "Epoch  38 Batch   22/41   train_loss = 0.069  valid_loss = 0.986\n",
      "Epoch  38 Batch   32/41   train_loss = 0.058  valid_loss = 0.983\n",
      "Epoch  39 Batch    1/41   train_loss = 0.076  valid_loss = 0.983\n",
      "Epoch  39 Batch   11/41   train_loss = 0.057  valid_loss = 0.982\n",
      "Epoch  39 Batch   21/41   train_loss = 0.057  valid_loss = 0.978\n",
      "Epoch  39 Batch   31/41   train_loss = 0.055  valid_loss = 0.976\n",
      "Epoch  40 Batch    0/41   train_loss = 0.062  valid_loss = 0.975\n",
      "Epoch  40 Batch   10/41   train_loss = 0.051  valid_loss = 0.975\n",
      "Epoch  40 Batch   20/41   train_loss = 0.053  valid_loss = 0.972\n",
      "Epoch  40 Batch   30/41   train_loss = 0.053  valid_loss = 0.970\n",
      "Epoch  40 Batch   40/41   train_loss = 0.045  valid_loss = 0.970\n",
      "Epoch  41 Batch    9/41   train_loss = 0.045  valid_loss = 0.968\n",
      "Epoch  41 Batch   19/41   train_loss = 0.050  valid_loss = 0.966\n",
      "Epoch  41 Batch   29/41   train_loss = 0.040  valid_loss = 0.965\n",
      "Epoch  41 Batch   39/41   train_loss = 0.043  valid_loss = 0.965\n",
      "Epoch  42 Batch    8/41   train_loss = 0.041  valid_loss = 0.964\n",
      "Epoch  42 Batch   18/41   train_loss = 0.042  valid_loss = 0.961\n",
      "Epoch  42 Batch   28/41   train_loss = 0.046  valid_loss = 0.960\n",
      "Epoch  42 Batch   38/41   train_loss = 0.044  valid_loss = 0.959\n",
      "Epoch  43 Batch    7/41   train_loss = 0.042  valid_loss = 0.958\n",
      "Epoch  43 Batch   17/41   train_loss = 0.037  valid_loss = 0.957\n",
      "Epoch  43 Batch   27/41   train_loss = 0.040  valid_loss = 0.954\n",
      "Epoch  43 Batch   37/41   train_loss = 0.038  valid_loss = 0.956\n",
      "Epoch  44 Batch    6/41   train_loss = 0.035  valid_loss = 0.957\n",
      "Epoch  44 Batch   16/41   train_loss = 0.035  valid_loss = 0.954\n",
      "Epoch  44 Batch   26/41   train_loss = 0.035  valid_loss = 0.951\n",
      "Epoch  44 Batch   36/41   train_loss = 0.035  valid_loss = 0.952\n",
      "Epoch  45 Batch    5/41   train_loss = 0.030  valid_loss = 0.953\n",
      "Epoch  45 Batch   15/41   train_loss = 0.030  valid_loss = 0.950\n",
      "Epoch  45 Batch   25/41   train_loss = 0.032  valid_loss = 0.947\n",
      "Epoch  45 Batch   35/41   train_loss = 0.035  valid_loss = 0.947\n",
      "Epoch  46 Batch    4/41   train_loss = 0.032  valid_loss = 0.949\n",
      "Epoch  46 Batch   14/41   train_loss = 0.027  valid_loss = 0.945\n",
      "Epoch  46 Batch   24/41   train_loss = 0.028  valid_loss = 0.944\n",
      "Epoch  46 Batch   34/41   train_loss = 0.025  valid_loss = 0.943\n",
      "Epoch  47 Batch    3/41   train_loss = 0.025  valid_loss = 0.945\n",
      "Epoch  47 Batch   13/41   train_loss = 0.028  valid_loss = 0.943\n",
      "Epoch  47 Batch   23/41   train_loss = 0.027  valid_loss = 0.941\n",
      "Epoch  47 Batch   33/41   train_loss = 0.023  valid_loss = 0.941\n",
      "Epoch  48 Batch    2/41   train_loss = 0.024  valid_loss = 0.941\n",
      "Epoch  48 Batch   12/41   train_loss = 0.025  valid_loss = 0.941\n",
      "Epoch  48 Batch   22/41   train_loss = 0.029  valid_loss = 0.940\n",
      "Epoch  48 Batch   32/41   train_loss = 0.022  valid_loss = 0.939\n",
      "Epoch  49 Batch    1/41   train_loss = 0.029  valid_loss = 0.939\n",
      "Epoch  49 Batch   11/41   train_loss = 0.022  valid_loss = 0.939\n",
      "Epoch  49 Batch   21/41   train_loss = 0.025  valid_loss = 0.938\n",
      "Epoch  49 Batch   31/41   train_loss = 0.021  valid_loss = 0.935\n",
      "Epoch  50 Batch    0/41   train_loss = 0.025  valid_loss = 0.936\n",
      "Epoch  50 Batch   10/41   train_loss = 0.022  valid_loss = 0.940\n",
      "Epoch  50 Batch   20/41   train_loss = 0.022  valid_loss = 0.936\n",
      "Epoch  50 Batch   30/41   train_loss = 0.022  valid_loss = 0.940\n",
      "Epoch  50 Batch   40/41   train_loss = 0.019  valid_loss = 0.936\n",
      "Epoch  51 Batch    9/41   train_loss = 0.021  valid_loss = 0.937\n",
      "Epoch  51 Batch   19/41   train_loss = 0.022  valid_loss = 0.933\n",
      "Epoch  51 Batch   29/41   train_loss = 0.017  valid_loss = 0.932\n",
      "Epoch  51 Batch   39/41   train_loss = 0.017  valid_loss = 0.935\n",
      "Epoch  52 Batch    8/41   train_loss = 0.019  valid_loss = 0.932\n",
      "Epoch  52 Batch   18/41   train_loss = 0.018  valid_loss = 0.931\n",
      "Epoch  52 Batch   28/41   train_loss = 0.021  valid_loss = 0.931\n",
      "Epoch  52 Batch   38/41   train_loss = 0.021  valid_loss = 0.931\n",
      "Epoch  53 Batch    7/41   train_loss = 0.021  valid_loss = 0.931\n",
      "Epoch  53 Batch   17/41   train_loss = 0.018  valid_loss = 0.930\n",
      "Epoch  53 Batch   27/41   train_loss = 0.020  valid_loss = 0.930\n",
      "Epoch  53 Batch   37/41   train_loss = 0.022  valid_loss = 0.929\n",
      "Epoch  54 Batch    6/41   train_loss = 0.017  valid_loss = 0.928\n",
      "Epoch  54 Batch   16/41   train_loss = 0.018  valid_loss = 0.932\n",
      "Epoch  54 Batch   26/41   train_loss = 0.016  valid_loss = 0.927\n",
      "Epoch  54 Batch   36/41   train_loss = 0.018  valid_loss = 0.928\n",
      "Epoch  55 Batch    5/41   train_loss = 0.014  valid_loss = 0.929\n",
      "Epoch  55 Batch   15/41   train_loss = 0.017  valid_loss = 0.927\n",
      "Epoch  55 Batch   25/41   train_loss = 0.017  valid_loss = 0.926\n",
      "Epoch  55 Batch   35/41   train_loss = 0.017  valid_loss = 0.925\n",
      "Epoch  56 Batch    4/41   train_loss = 0.015  valid_loss = 0.928\n",
      "Epoch  56 Batch   14/41   train_loss = 0.015  valid_loss = 0.925\n",
      "Epoch  56 Batch   24/41   train_loss = 0.015  valid_loss = 0.925\n",
      "Epoch  56 Batch   34/41   train_loss = 0.014  valid_loss = 0.925\n",
      "Epoch  57 Batch    3/41   train_loss = 0.013  valid_loss = 0.925\n",
      "Epoch  57 Batch   13/41   train_loss = 0.015  valid_loss = 0.925\n",
      "Epoch  57 Batch   23/41   train_loss = 0.014  valid_loss = 0.924\n",
      "Epoch  57 Batch   33/41   train_loss = 0.012  valid_loss = 0.925\n",
      "Epoch  58 Batch    2/41   train_loss = 0.011  valid_loss = 0.925\n",
      "Epoch  58 Batch   12/41   train_loss = 0.013  valid_loss = 0.925\n",
      "Epoch  58 Batch   22/41   train_loss = 0.016  valid_loss = 0.923\n",
      "Epoch  58 Batch   32/41   train_loss = 0.012  valid_loss = 0.923\n",
      "Epoch  59 Batch    1/41   train_loss = 0.015  valid_loss = 0.923\n",
      "Epoch  59 Batch   11/41   train_loss = 0.012  valid_loss = 0.925\n",
      "Epoch  59 Batch   21/41   train_loss = 0.014  valid_loss = 0.923\n",
      "Epoch  59 Batch   31/41   train_loss = 0.011  valid_loss = 0.923\n",
      "Epoch  60 Batch    0/41   train_loss = 0.013  valid_loss = 0.924\n",
      "Epoch  60 Batch   10/41   train_loss = 0.012  valid_loss = 0.923\n",
      "Epoch  60 Batch   20/41   train_loss = 0.012  valid_loss = 0.922\n",
      "Epoch  60 Batch   30/41   train_loss = 0.011  valid_loss = 0.922\n",
      "Epoch  60 Batch   40/41   train_loss = 0.011  valid_loss = 0.922\n",
      "Epoch  61 Batch    9/41   train_loss = 0.011  valid_loss = 0.922\n",
      "Epoch  61 Batch   19/41   train_loss = 0.013  valid_loss = 0.921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  61 Batch   29/41   train_loss = 0.009  valid_loss = 0.921\n",
      "Epoch  61 Batch   39/41   train_loss = 0.011  valid_loss = 0.921\n",
      "Epoch  62 Batch    8/41   train_loss = 0.010  valid_loss = 0.921\n",
      "Epoch  62 Batch   18/41   train_loss = 0.011  valid_loss = 0.921\n",
      "Epoch  62 Batch   28/41   train_loss = 0.011  valid_loss = 0.921\n",
      "Epoch  62 Batch   38/41   train_loss = 0.012  valid_loss = 0.921\n",
      "Epoch  63 Batch    7/41   train_loss = 0.013  valid_loss = 0.922\n",
      "Epoch  63 Batch   17/41   train_loss = 0.010  valid_loss = 0.922\n",
      "Epoch  63 Batch   27/41   train_loss = 0.012  valid_loss = 0.921\n",
      "Epoch  63 Batch   37/41   train_loss = 0.010  valid_loss = 0.921\n",
      "Epoch  64 Batch    6/41   train_loss = 0.011  valid_loss = 0.921\n",
      "Epoch  64 Batch   16/41   train_loss = 0.010  valid_loss = 0.921\n",
      "Epoch  64 Batch   26/41   train_loss = 0.010  valid_loss = 0.921\n",
      "Epoch  64 Batch   36/41   train_loss = 0.009  valid_loss = 0.921\n",
      "Epoch  65 Batch    5/41   train_loss = 0.008  valid_loss = 0.923\n",
      "Epoch  65 Batch   15/41   train_loss = 0.010  valid_loss = 0.921\n",
      "Epoch  65 Batch   25/41   train_loss = 0.009  valid_loss = 0.920\n",
      "Epoch  65 Batch   35/41   train_loss = 0.010  valid_loss = 0.918\n",
      "Epoch  66 Batch    4/41   train_loss = 0.011  valid_loss = 0.918\n",
      "Epoch  66 Batch   14/41   train_loss = 0.010  valid_loss = 0.919\n",
      "Epoch  66 Batch   24/41   train_loss = 0.008  valid_loss = 0.918\n",
      "Epoch  66 Batch   34/41   train_loss = 0.009  valid_loss = 0.918\n",
      "Epoch  67 Batch    3/41   train_loss = 0.007  valid_loss = 0.918\n",
      "Epoch  67 Batch   13/41   train_loss = 0.009  valid_loss = 0.919\n",
      "Epoch  67 Batch   23/41   train_loss = 0.009  valid_loss = 0.917\n",
      "Epoch  67 Batch   33/41   train_loss = 0.007  valid_loss = 0.917\n",
      "Epoch  68 Batch    2/41   train_loss = 0.008  valid_loss = 0.917\n",
      "Epoch  68 Batch   12/41   train_loss = 0.008  valid_loss = 0.918\n",
      "Epoch  68 Batch   22/41   train_loss = 0.010  valid_loss = 0.919\n",
      "Epoch  68 Batch   32/41   train_loss = 0.007  valid_loss = 0.919\n",
      "Epoch  69 Batch    1/41   train_loss = 0.011  valid_loss = 0.919\n",
      "Epoch  69 Batch   11/41   train_loss = 0.008  valid_loss = 0.919\n",
      "Epoch  69 Batch   21/41   train_loss = 0.010  valid_loss = 0.918\n",
      "Epoch  69 Batch   31/41   train_loss = 0.007  valid_loss = 0.917\n",
      "Epoch  70 Batch    0/41   train_loss = 0.009  valid_loss = 0.917\n",
      "Epoch  70 Batch   10/41   train_loss = 0.007  valid_loss = 0.919\n",
      "Epoch  70 Batch   20/41   train_loss = 0.008  valid_loss = 0.918\n",
      "Epoch  70 Batch   30/41   train_loss = 0.007  valid_loss = 0.918\n",
      "Epoch  70 Batch   40/41   train_loss = 0.007  valid_loss = 0.918\n",
      "Epoch  71 Batch    9/41   train_loss = 0.008  valid_loss = 0.917\n",
      "Epoch  71 Batch   19/41   train_loss = 0.007  valid_loss = 0.917\n",
      "Epoch  71 Batch   29/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  71 Batch   39/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  72 Batch    8/41   train_loss = 0.007  valid_loss = 0.916\n",
      "Epoch  72 Batch   18/41   train_loss = 0.007  valid_loss = 0.918\n",
      "Epoch  72 Batch   28/41   train_loss = 0.007  valid_loss = 0.917\n",
      "Epoch  72 Batch   38/41   train_loss = 0.007  valid_loss = 0.916\n",
      "Epoch  73 Batch    7/41   train_loss = 0.008  valid_loss = 0.916\n",
      "Epoch  73 Batch   17/41   train_loss = 0.007  valid_loss = 0.920\n",
      "Epoch  73 Batch   27/41   train_loss = 0.008  valid_loss = 0.916\n",
      "Epoch  73 Batch   37/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  74 Batch    6/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  74 Batch   16/41   train_loss = 0.007  valid_loss = 0.916\n",
      "Epoch  74 Batch   26/41   train_loss = 0.007  valid_loss = 0.916\n",
      "Epoch  74 Batch   36/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  75 Batch    5/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  75 Batch   15/41   train_loss = 0.007  valid_loss = 0.918\n",
      "Epoch  75 Batch   25/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  75 Batch   35/41   train_loss = 0.007  valid_loss = 0.916\n",
      "Epoch  76 Batch    4/41   train_loss = 0.007  valid_loss = 0.917\n",
      "Epoch  76 Batch   14/41   train_loss = 0.008  valid_loss = 0.918\n",
      "Epoch  76 Batch   24/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  76 Batch   34/41   train_loss = 0.005  valid_loss = 0.918\n",
      "Epoch  77 Batch    3/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  77 Batch   13/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  77 Batch   23/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  77 Batch   33/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  78 Batch    2/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  78 Batch   12/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  78 Batch   22/41   train_loss = 0.009  valid_loss = 0.917\n",
      "Epoch  78 Batch   32/41   train_loss = 0.007  valid_loss = 0.917\n",
      "Epoch  79 Batch    1/41   train_loss = 0.008  valid_loss = 0.917\n",
      "Epoch  79 Batch   11/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  79 Batch   21/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  79 Batch   31/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  80 Batch    0/41   train_loss = 0.006  valid_loss = 0.915\n",
      "Epoch  80 Batch   10/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  80 Batch   20/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  80 Batch   30/41   train_loss = 0.005  valid_loss = 0.919\n",
      "Epoch  80 Batch   40/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  81 Batch    9/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  81 Batch   19/41   train_loss = 0.006  valid_loss = 0.915\n",
      "Epoch  81 Batch   29/41   train_loss = 0.004  valid_loss = 0.915\n",
      "Epoch  81 Batch   39/41   train_loss = 0.005  valid_loss = 0.914\n",
      "Epoch  82 Batch    8/41   train_loss = 0.004  valid_loss = 0.915\n",
      "Epoch  82 Batch   18/41   train_loss = 0.005  valid_loss = 0.917\n",
      "Epoch  82 Batch   28/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  82 Batch   38/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  83 Batch    7/41   train_loss = 0.006  valid_loss = 0.915\n",
      "Epoch  83 Batch   17/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch  83 Batch   27/41   train_loss = 0.006  valid_loss = 0.914\n",
      "Epoch  83 Batch   37/41   train_loss = 0.005  valid_loss = 0.914\n",
      "Epoch  84 Batch    6/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  84 Batch   16/41   train_loss = 0.006  valid_loss = 0.913\n",
      "Epoch  84 Batch   26/41   train_loss = 0.005  valid_loss = 0.914\n",
      "Epoch  84 Batch   36/41   train_loss = 0.005  valid_loss = 0.913\n",
      "Epoch  85 Batch    5/41   train_loss = 0.004  valid_loss = 0.914\n",
      "Epoch  85 Batch   15/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  85 Batch   25/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  85 Batch   35/41   train_loss = 0.006  valid_loss = 0.917\n",
      "Epoch  86 Batch    4/41   train_loss = 0.006  valid_loss = 0.914\n",
      "Epoch  86 Batch   14/41   train_loss = 0.006  valid_loss = 0.914\n",
      "Epoch  86 Batch   24/41   train_loss = 0.004  valid_loss = 0.912\n",
      "Epoch  86 Batch   34/41   train_loss = 0.004  valid_loss = 0.916\n",
      "Epoch  87 Batch    3/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  87 Batch   13/41   train_loss = 0.004  valid_loss = 0.911\n",
      "Epoch  87 Batch   23/41   train_loss = 0.004  valid_loss = 0.912\n",
      "Epoch  87 Batch   33/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  88 Batch    2/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  88 Batch   12/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  88 Batch   22/41   train_loss = 0.006  valid_loss = 0.915\n",
      "Epoch  88 Batch   32/41   train_loss = 0.004  valid_loss = 0.916\n",
      "Epoch  89 Batch    1/41   train_loss = 0.006  valid_loss = 0.915\n",
      "Epoch  89 Batch   11/41   train_loss = 0.004  valid_loss = 0.914\n",
      "Epoch  89 Batch   21/41   train_loss = 0.006  valid_loss = 0.916\n",
      "Epoch  89 Batch   31/41   train_loss = 0.004  valid_loss = 0.915\n",
      "Epoch  90 Batch    0/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  90 Batch   10/41   train_loss = 0.004  valid_loss = 0.914\n",
      "Epoch  90 Batch   20/41   train_loss = 0.005  valid_loss = 0.915\n",
      "Epoch  90 Batch   30/41   train_loss = 0.003  valid_loss = 0.918\n",
      "Epoch  90 Batch   40/41   train_loss = 0.004  valid_loss = 0.914\n",
      "Epoch  91 Batch    9/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  91 Batch   19/41   train_loss = 0.004  valid_loss = 0.914\n",
      "Epoch  91 Batch   29/41   train_loss = 0.003  valid_loss = 0.913\n",
      "Epoch  91 Batch   39/41   train_loss = 0.003  valid_loss = 0.914\n",
      "Epoch  92 Batch    8/41   train_loss = 0.003  valid_loss = 0.913\n",
      "Epoch  92 Batch   18/41   train_loss = 0.004  valid_loss = 0.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  92 Batch   28/41   train_loss = 0.004  valid_loss = 0.912\n",
      "Epoch  92 Batch   38/41   train_loss = 0.003  valid_loss = 0.916\n",
      "Epoch  93 Batch    7/41   train_loss = 0.004  valid_loss = 0.917\n",
      "Epoch  93 Batch   17/41   train_loss = 0.005  valid_loss = 0.914\n",
      "Epoch  93 Batch   27/41   train_loss = 0.006  valid_loss = 0.913\n",
      "Epoch  93 Batch   37/41   train_loss = 0.004  valid_loss = 0.914\n",
      "Epoch  94 Batch    6/41   train_loss = 0.003  valid_loss = 0.917\n",
      "Epoch  94 Batch   16/41   train_loss = 0.004  valid_loss = 0.917\n",
      "Epoch  94 Batch   26/41   train_loss = 0.004  valid_loss = 0.919\n",
      "Epoch  94 Batch   36/41   train_loss = 0.004  valid_loss = 0.921\n",
      "Epoch  95 Batch    5/41   train_loss = 0.003  valid_loss = 0.919\n",
      "Epoch  95 Batch   15/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  95 Batch   25/41   train_loss = 0.004  valid_loss = 0.915\n",
      "Epoch  95 Batch   35/41   train_loss = 0.004  valid_loss = 0.925\n",
      "Epoch  96 Batch    4/41   train_loss = 0.005  valid_loss = 0.920\n",
      "Epoch  96 Batch   14/41   train_loss = 0.005  valid_loss = 0.912\n",
      "Epoch  96 Batch   24/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch  96 Batch   34/41   train_loss = 0.003  valid_loss = 0.914\n",
      "Epoch  97 Batch    3/41   train_loss = 0.003  valid_loss = 0.913\n",
      "Epoch  97 Batch   13/41   train_loss = 0.004  valid_loss = 0.911\n",
      "Epoch  97 Batch   23/41   train_loss = 0.003  valid_loss = 0.918\n",
      "Epoch  97 Batch   33/41   train_loss = 0.003  valid_loss = 0.916\n",
      "Epoch  98 Batch    2/41   train_loss = 0.003  valid_loss = 0.911\n",
      "Epoch  98 Batch   12/41   train_loss = 0.003  valid_loss = 0.910\n",
      "Epoch  98 Batch   22/41   train_loss = 0.004  valid_loss = 0.911\n",
      "Epoch  98 Batch   32/41   train_loss = 0.003  valid_loss = 0.912\n",
      "Epoch  99 Batch    1/41   train_loss = 0.006  valid_loss = 0.912\n",
      "Epoch  99 Batch   11/41   train_loss = 0.006  valid_loss = 0.912\n",
      "Epoch  99 Batch   21/41   train_loss = 0.004  valid_loss = 0.907\n",
      "Epoch  99 Batch   31/41   train_loss = 0.003  valid_loss = 0.906\n",
      "Epoch 100 Batch    0/41   train_loss = 0.005  valid_loss = 0.916\n",
      "Epoch 100 Batch   10/41   train_loss = 0.004  valid_loss = 0.916\n",
      "Epoch 100 Batch   20/41   train_loss = 0.004  valid_loss = 0.909\n",
      "Epoch 100 Batch   30/41   train_loss = 0.002  valid_loss = 0.913\n",
      "Epoch 100 Batch   40/41   train_loss = 0.003  valid_loss = 0.910\n",
      "Epoch 101 Batch    9/41   train_loss = 0.003  valid_loss = 0.911\n",
      "Epoch 101 Batch   19/41   train_loss = 0.005  valid_loss = 0.922\n",
      "Epoch 101 Batch   29/41   train_loss = 0.003  valid_loss = 0.908\n",
      "Epoch 101 Batch   39/41   train_loss = 0.003  valid_loss = 0.913\n",
      "Epoch 102 Batch    8/41   train_loss = 0.003  valid_loss = 0.906\n",
      "Epoch 102 Batch   18/41   train_loss = 0.004  valid_loss = 0.905\n",
      "Epoch 102 Batch   28/41   train_loss = 0.003  valid_loss = 0.911\n",
      "Epoch 102 Batch   38/41   train_loss = 0.004  valid_loss = 0.925\n",
      "Epoch 103 Batch    7/41   train_loss = 0.003  valid_loss = 0.912\n",
      "Epoch 103 Batch   17/41   train_loss = 0.002  valid_loss = 0.907\n",
      "Epoch 103 Batch   27/41   train_loss = 0.005  valid_loss = 0.910\n",
      "Epoch 103 Batch   37/41   train_loss = 0.003  valid_loss = 0.927\n",
      "Epoch 104 Batch    6/41   train_loss = 0.002  valid_loss = 0.918\n",
      "Epoch 104 Batch   16/41   train_loss = 0.003  valid_loss = 0.912\n",
      "Epoch 104 Batch   26/41   train_loss = 0.003  valid_loss = 0.914\n",
      "Epoch 104 Batch   36/41   train_loss = 0.003  valid_loss = 0.916\n",
      "Epoch 105 Batch    5/41   train_loss = 0.002  valid_loss = 0.910\n",
      "Epoch 105 Batch   15/41   train_loss = 0.004  valid_loss = 0.908\n",
      "Epoch 105 Batch   25/41   train_loss = 0.003  valid_loss = 0.910\n",
      "Epoch 105 Batch   35/41   train_loss = 0.003  valid_loss = 0.908\n",
      "Epoch 106 Batch    4/41   train_loss = 0.005  valid_loss = 0.907\n",
      "Epoch 106 Batch   14/41   train_loss = 0.004  valid_loss = 0.911\n",
      "Epoch 106 Batch   24/41   train_loss = 0.004  valid_loss = 0.904\n",
      "Epoch 106 Batch   34/41   train_loss = 0.002  valid_loss = 0.907\n",
      "Epoch 107 Batch    3/41   train_loss = 0.002  valid_loss = 0.910\n",
      "Epoch 107 Batch   13/41   train_loss = 0.004  valid_loss = 0.920\n",
      "Epoch 107 Batch   23/41   train_loss = 0.004  valid_loss = 0.912\n",
      "Epoch 107 Batch   33/41   train_loss = 0.004  valid_loss = 0.913\n",
      "Epoch 108 Batch    2/41   train_loss = 0.004  valid_loss = 0.919\n",
      "Epoch 108 Batch   12/41   train_loss = 0.005  valid_loss = 0.918\n",
      "Epoch 108 Batch   22/41   train_loss = 0.003  valid_loss = 0.920\n",
      "Epoch 108 Batch   32/41   train_loss = 0.003  valid_loss = 0.917\n",
      "Epoch 109 Batch    1/41   train_loss = 0.005  valid_loss = 0.936\n",
      "Epoch 109 Batch   11/41   train_loss = 0.003  valid_loss = 0.921\n",
      "Epoch 109 Batch   21/41   train_loss = 0.004  valid_loss = 0.908\n",
      "Epoch 109 Batch   31/41   train_loss = 0.004  valid_loss = 0.915\n",
      "Epoch 110 Batch    0/41   train_loss = 0.004  valid_loss = 0.953\n",
      "Epoch 110 Batch   10/41   train_loss = 0.007  valid_loss = 0.928\n",
      "Epoch 110 Batch   20/41   train_loss = 0.004  valid_loss = 0.904\n",
      "Epoch 110 Batch   30/41   train_loss = 0.002  valid_loss = 0.903\n",
      "Epoch 110 Batch   40/41   train_loss = 0.003  valid_loss = 0.902\n",
      "Epoch 111 Batch    9/41   train_loss = 0.004  valid_loss = 0.928\n",
      "Epoch 111 Batch   19/41   train_loss = 0.003  valid_loss = 0.911\n",
      "Epoch 111 Batch   29/41   train_loss = 0.002  valid_loss = 0.906\n",
      "Epoch 111 Batch   39/41   train_loss = 0.004  valid_loss = 0.899\n",
      "Epoch 112 Batch    8/41   train_loss = 0.004  valid_loss = 0.899\n",
      "Epoch 112 Batch   18/41   train_loss = 0.003  valid_loss = 0.898\n",
      "Epoch 112 Batch   28/41   train_loss = 0.003  valid_loss = 0.911\n",
      "Epoch 112 Batch   38/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 113 Batch    7/41   train_loss = 0.003  valid_loss = 0.894\n",
      "Epoch 113 Batch   17/41   train_loss = 0.007  valid_loss = 0.912\n",
      "Epoch 113 Batch   27/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch 113 Batch   37/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch 114 Batch    6/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch 114 Batch   16/41   train_loss = 0.007  valid_loss = 0.911\n",
      "Epoch 114 Batch   26/41   train_loss = 0.005  valid_loss = 0.936\n",
      "Epoch 114 Batch   36/41   train_loss = 0.019  valid_loss = 0.911\n",
      "Epoch 115 Batch    5/41   train_loss = 0.006  valid_loss = 0.896\n",
      "Epoch 115 Batch   15/41   train_loss = 0.008  valid_loss = 0.914\n",
      "Epoch 115 Batch   25/41   train_loss = 0.003  valid_loss = 0.898\n",
      "Epoch 115 Batch   35/41   train_loss = 0.003  valid_loss = 0.899\n",
      "Epoch 116 Batch    4/41   train_loss = 0.005  valid_loss = 0.932\n",
      "Epoch 116 Batch   14/41   train_loss = 0.009  valid_loss = 0.926\n",
      "Epoch 116 Batch   24/41   train_loss = 0.003  valid_loss = 0.897\n",
      "Epoch 116 Batch   34/41   train_loss = 0.004  valid_loss = 0.875\n",
      "Epoch 117 Batch    3/41   train_loss = 0.008  valid_loss = 0.876\n",
      "Epoch 117 Batch   13/41   train_loss = 0.008  valid_loss = 0.872\n",
      "Epoch 117 Batch   23/41   train_loss = 0.013  valid_loss = 0.873\n",
      "Epoch 117 Batch   33/41   train_loss = 0.005  valid_loss = 0.877\n",
      "Epoch 118 Batch    2/41   train_loss = 0.009  valid_loss = 0.892\n",
      "Epoch 118 Batch   12/41   train_loss = 0.008  valid_loss = 0.880\n",
      "Epoch 118 Batch   22/41   train_loss = 0.006  valid_loss = 0.892\n",
      "Epoch 118 Batch   32/41   train_loss = 0.005  valid_loss = 0.895\n",
      "Epoch 119 Batch    1/41   train_loss = 0.005  valid_loss = 0.881\n",
      "Epoch 119 Batch   11/41   train_loss = 0.002  valid_loss = 0.876\n",
      "Epoch 119 Batch   21/41   train_loss = 0.004  valid_loss = 0.877\n",
      "Epoch 119 Batch   31/41   train_loss = 0.002  valid_loss = 0.880\n",
      "Epoch 120 Batch    0/41   train_loss = 0.002  valid_loss = 0.880\n",
      "Epoch 120 Batch   10/41   train_loss = 0.002  valid_loss = 0.882\n",
      "Epoch 120 Batch   20/41   train_loss = 0.003  valid_loss = 0.885\n",
      "Epoch 120 Batch   30/41   train_loss = 0.001  valid_loss = 0.887\n",
      "Epoch 120 Batch   40/41   train_loss = 0.002  valid_loss = 0.888\n",
      "Epoch 121 Batch    9/41   train_loss = 0.002  valid_loss = 0.890\n",
      "Epoch 121 Batch   19/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 121 Batch   29/41   train_loss = 0.001  valid_loss = 0.892\n",
      "Epoch 121 Batch   39/41   train_loss = 0.002  valid_loss = 0.892\n",
      "Epoch 122 Batch    8/41   train_loss = 0.001  valid_loss = 0.893\n",
      "Epoch 122 Batch   18/41   train_loss = 0.002  valid_loss = 0.894\n",
      "Epoch 122 Batch   28/41   train_loss = 0.002  valid_loss = 0.895\n",
      "Epoch 122 Batch   38/41   train_loss = 0.002  valid_loss = 0.895\n",
      "Epoch 123 Batch    7/41   train_loss = 0.001  valid_loss = 0.896\n",
      "Epoch 123 Batch   17/41   train_loss = 0.002  valid_loss = 0.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123 Batch   27/41   train_loss = 0.004  valid_loss = 0.898\n",
      "Epoch 123 Batch   37/41   train_loss = 0.002  valid_loss = 0.898\n",
      "Epoch 124 Batch    6/41   train_loss = 0.002  valid_loss = 0.898\n",
      "Epoch 124 Batch   16/41   train_loss = 0.002  valid_loss = 0.899\n",
      "Epoch 124 Batch   26/41   train_loss = 0.002  valid_loss = 0.900\n",
      "Epoch 124 Batch   36/41   train_loss = 0.002  valid_loss = 0.899\n",
      "Epoch 125 Batch    5/41   train_loss = 0.002  valid_loss = 0.899\n",
      "Epoch 125 Batch   15/41   train_loss = 0.002  valid_loss = 0.900\n",
      "Epoch 125 Batch   25/41   train_loss = 0.001  valid_loss = 0.901\n",
      "Epoch 125 Batch   35/41   train_loss = 0.002  valid_loss = 0.900\n",
      "Epoch 126 Batch    4/41   train_loss = 0.003  valid_loss = 0.900\n",
      "Epoch 126 Batch   14/41   train_loss = 0.002  valid_loss = 0.901\n",
      "Epoch 126 Batch   24/41   train_loss = 0.001  valid_loss = 0.901\n",
      "Epoch 126 Batch   34/41   train_loss = 0.001  valid_loss = 0.901\n",
      "Epoch 127 Batch    3/41   train_loss = 0.002  valid_loss = 0.900\n",
      "Epoch 127 Batch   13/41   train_loss = 0.002  valid_loss = 0.901\n",
      "Epoch 127 Batch   23/41   train_loss = 0.002  valid_loss = 0.902\n",
      "Epoch 127 Batch   33/41   train_loss = 0.001  valid_loss = 0.902\n",
      "Epoch 128 Batch    2/41   train_loss = 0.001  valid_loss = 0.902\n",
      "Epoch 128 Batch   12/41   train_loss = 0.002  valid_loss = 0.902\n",
      "Epoch 128 Batch   22/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 128 Batch   32/41   train_loss = 0.002  valid_loss = 0.904\n",
      "Epoch 129 Batch    1/41   train_loss = 0.002  valid_loss = 0.903\n",
      "Epoch 129 Batch   11/41   train_loss = 0.002  valid_loss = 0.902\n",
      "Epoch 129 Batch   21/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 129 Batch   31/41   train_loss = 0.002  valid_loss = 0.903\n",
      "Epoch 130 Batch    0/41   train_loss = 0.002  valid_loss = 0.902\n",
      "Epoch 130 Batch   10/41   train_loss = 0.001  valid_loss = 0.902\n",
      "Epoch 130 Batch   20/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 130 Batch   30/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 130 Batch   40/41   train_loss = 0.001  valid_loss = 0.902\n",
      "Epoch 131 Batch    9/41   train_loss = 0.001  valid_loss = 0.902\n",
      "Epoch 131 Batch   19/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 131 Batch   29/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 131 Batch   39/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 132 Batch    8/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 132 Batch   18/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 132 Batch   28/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 132 Batch   38/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 133 Batch    7/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 133 Batch   17/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 133 Batch   27/41   train_loss = 0.004  valid_loss = 0.905\n",
      "Epoch 133 Batch   37/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 134 Batch    6/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 134 Batch   16/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 134 Batch   26/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 134 Batch   36/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 135 Batch    5/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 135 Batch   15/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 135 Batch   25/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 135 Batch   35/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 136 Batch    4/41   train_loss = 0.002  valid_loss = 0.903\n",
      "Epoch 136 Batch   14/41   train_loss = 0.002  valid_loss = 0.904\n",
      "Epoch 136 Batch   24/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 136 Batch   34/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 137 Batch    3/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 137 Batch   13/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 137 Batch   23/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 137 Batch   33/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 138 Batch    2/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 138 Batch   12/41   train_loss = 0.002  valid_loss = 0.902\n",
      "Epoch 138 Batch   22/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 138 Batch   32/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 139 Batch    1/41   train_loss = 0.002  valid_loss = 0.903\n",
      "Epoch 139 Batch   11/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 139 Batch   21/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 139 Batch   31/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 140 Batch    0/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 140 Batch   10/41   train_loss = 0.001  valid_loss = 0.903\n",
      "Epoch 140 Batch   20/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 140 Batch   30/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 140 Batch   40/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 141 Batch    9/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 141 Batch   19/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 141 Batch   29/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 141 Batch   39/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 142 Batch    8/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 142 Batch   18/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 142 Batch   28/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 142 Batch   38/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 143 Batch    7/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 143 Batch   17/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 143 Batch   27/41   train_loss = 0.003  valid_loss = 0.905\n",
      "Epoch 143 Batch   37/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 144 Batch    6/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 144 Batch   16/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 144 Batch   26/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 144 Batch   36/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 145 Batch    5/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 145 Batch   15/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 145 Batch   25/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 145 Batch   35/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 146 Batch    4/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 146 Batch   14/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 146 Batch   24/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 146 Batch   34/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 147 Batch    3/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 147 Batch   13/41   train_loss = 0.002  valid_loss = 0.904\n",
      "Epoch 147 Batch   23/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 147 Batch   33/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 148 Batch    2/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 148 Batch   12/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 148 Batch   22/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 148 Batch   32/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 149 Batch    1/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 149 Batch   11/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 149 Batch   21/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 149 Batch   31/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 150 Batch    0/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 150 Batch   10/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 150 Batch   20/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 150 Batch   30/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 150 Batch   40/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 151 Batch    9/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 151 Batch   19/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 151 Batch   29/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 151 Batch   39/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 152 Batch    8/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 152 Batch   18/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 152 Batch   28/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 152 Batch   38/41   train_loss = 0.001  valid_loss = 0.904\n",
      "Epoch 153 Batch    7/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 153 Batch   17/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 153 Batch   27/41   train_loss = 0.002  valid_loss = 0.907\n",
      "Epoch 153 Batch   37/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 154 Batch    6/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 154 Batch   16/41   train_loss = 0.001  valid_loss = 0.905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154 Batch   26/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 154 Batch   36/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 155 Batch    5/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 155 Batch   15/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 155 Batch   25/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 155 Batch   35/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 156 Batch    4/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 156 Batch   14/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 156 Batch   24/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 156 Batch   34/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 157 Batch    3/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 157 Batch   13/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 157 Batch   23/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 157 Batch   33/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 158 Batch    2/41   train_loss = 0.000  valid_loss = 0.906\n",
      "Epoch 158 Batch   12/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 158 Batch   22/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 158 Batch   32/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 159 Batch    1/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 159 Batch   11/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 159 Batch   21/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 159 Batch   31/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 160 Batch    0/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 160 Batch   10/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 160 Batch   20/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 160 Batch   30/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 160 Batch   40/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 161 Batch    9/41   train_loss = 0.001  valid_loss = 0.905\n",
      "Epoch 161 Batch   19/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 161 Batch   29/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 161 Batch   39/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 162 Batch    8/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 162 Batch   18/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 162 Batch   28/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 162 Batch   38/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 163 Batch    7/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 163 Batch   17/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 163 Batch   27/41   train_loss = 0.002  valid_loss = 0.908\n",
      "Epoch 163 Batch   37/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 164 Batch    6/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 164 Batch   16/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 164 Batch   26/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 164 Batch   36/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 165 Batch    5/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 165 Batch   15/41   train_loss = 0.001  valid_loss = 0.906\n",
      "Epoch 165 Batch   25/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 165 Batch   35/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 166 Batch    4/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 166 Batch   14/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 166 Batch   24/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 166 Batch   34/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 167 Batch    3/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 167 Batch   13/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 167 Batch   23/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 167 Batch   33/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 168 Batch    2/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 168 Batch   12/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 168 Batch   22/41   train_loss = 0.001  valid_loss = 0.909\n",
      "Epoch 168 Batch   32/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 169 Batch    1/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 169 Batch   11/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 169 Batch   21/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 169 Batch   31/41   train_loss = 0.001  valid_loss = 0.909\n",
      "Epoch 170 Batch    0/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 170 Batch   10/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 170 Batch   20/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 170 Batch   30/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 170 Batch   40/41   train_loss = 0.001  valid_loss = 0.907\n",
      "Epoch 171 Batch    9/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 171 Batch   19/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 171 Batch   29/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 171 Batch   39/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 172 Batch    8/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 172 Batch   18/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 172 Batch   28/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 172 Batch   38/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 173 Batch    7/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 173 Batch   17/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 173 Batch   27/41   train_loss = 0.001  valid_loss = 0.909\n",
      "Epoch 173 Batch   37/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 174 Batch    6/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 174 Batch   16/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 174 Batch   26/41   train_loss = 0.001  valid_loss = 0.909\n",
      "Epoch 174 Batch   36/41   train_loss = 0.001  valid_loss = 0.909\n",
      "Epoch 175 Batch    5/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 175 Batch   15/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 175 Batch   25/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 175 Batch   35/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 176 Batch    4/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 176 Batch   14/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 176 Batch   24/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 176 Batch   34/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 177 Batch    3/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 177 Batch   13/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 177 Batch   23/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 177 Batch   33/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 178 Batch    2/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 178 Batch   12/41   train_loss = 0.001  valid_loss = 0.911\n",
      "Epoch 178 Batch   22/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 178 Batch   32/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 179 Batch    1/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 179 Batch   11/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 179 Batch   21/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 179 Batch   31/41   train_loss = 0.001  valid_loss = 0.911\n",
      "Epoch 180 Batch    0/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 180 Batch   10/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 180 Batch   20/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 180 Batch   30/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 180 Batch   40/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 181 Batch    9/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 181 Batch   19/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 181 Batch   29/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 181 Batch   39/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 182 Batch    8/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 182 Batch   18/41   train_loss = 0.001  valid_loss = 0.909\n",
      "Epoch 182 Batch   28/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 182 Batch   38/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 183 Batch    7/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 183 Batch   17/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 183 Batch   27/41   train_loss = 0.002  valid_loss = 0.911\n",
      "Epoch 183 Batch   37/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 184 Batch    6/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 184 Batch   16/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 184 Batch   26/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 184 Batch   36/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 185 Batch    5/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 185 Batch   15/41   train_loss = 0.000  valid_loss = 0.909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185 Batch   25/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 185 Batch   35/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 186 Batch    4/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 186 Batch   14/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 186 Batch   24/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 186 Batch   34/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 187 Batch    3/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 187 Batch   13/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 187 Batch   23/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 187 Batch   33/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 188 Batch    2/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 188 Batch   12/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 188 Batch   22/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 188 Batch   32/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 189 Batch    1/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 189 Batch   11/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 189 Batch   21/41   train_loss = 0.000  valid_loss = 0.909\n",
      "Epoch 189 Batch   31/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 190 Batch    0/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 190 Batch   10/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 190 Batch   20/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 190 Batch   30/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 190 Batch   40/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 191 Batch    9/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 191 Batch   19/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 191 Batch   29/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 191 Batch   39/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 192 Batch    8/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 192 Batch   18/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 192 Batch   28/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 192 Batch   38/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 193 Batch    7/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 193 Batch   17/41   train_loss = 0.000  valid_loss = 0.911\n",
      "Epoch 193 Batch   27/41   train_loss = 0.001  valid_loss = 0.910\n",
      "Epoch 193 Batch   37/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 194 Batch    6/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 194 Batch   16/41   train_loss = 0.000  valid_loss = 0.913\n",
      "Epoch 194 Batch   26/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 194 Batch   36/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 195 Batch    5/41   train_loss = 0.000  valid_loss = 0.914\n",
      "Epoch 195 Batch   15/41   train_loss = 0.000  valid_loss = 0.915\n",
      "Epoch 195 Batch   25/41   train_loss = 0.000  valid_loss = 0.913\n",
      "Epoch 195 Batch   35/41   train_loss = 0.001  valid_loss = 0.912\n",
      "Epoch 196 Batch    4/41   train_loss = 0.000  valid_loss = 0.914\n",
      "Epoch 196 Batch   14/41   train_loss = 0.000  valid_loss = 0.913\n",
      "Epoch 196 Batch   24/41   train_loss = 0.000  valid_loss = 0.913\n",
      "Epoch 196 Batch   34/41   train_loss = 0.000  valid_loss = 0.912\n",
      "Epoch 197 Batch    3/41   train_loss = 0.000  valid_loss = 0.914\n",
      "Epoch 197 Batch   13/41   train_loss = 0.000  valid_loss = 0.914\n",
      "Epoch 197 Batch   23/41   train_loss = 0.000  valid_loss = 0.914\n",
      "Epoch 197 Batch   33/41   train_loss = 0.000  valid_loss = 0.914\n",
      "Epoch 198 Batch    2/41   train_loss = 0.000  valid_loss = 0.908\n",
      "Epoch 198 Batch   12/41   train_loss = 0.000  valid_loss = 0.915\n",
      "Epoch 198 Batch   22/41   train_loss = 0.000  valid_loss = 0.910\n",
      "Epoch 198 Batch   32/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 199 Batch    1/41   train_loss = 0.001  valid_loss = 0.908\n",
      "Epoch 199 Batch   11/41   train_loss = 0.000  valid_loss = 0.907\n",
      "Epoch 199 Batch   21/41   train_loss = 0.000  valid_loss = 0.904\n",
      "Epoch 199 Batch   31/41   train_loss = 0.000  valid_loss = 0.904\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "split_index=int(0.8*len(int_text))\n",
    "train_source = int_text[:split_index]\n",
    "valid_source = int_text[split_index:]\n",
    "\n",
    "train_label= int_label[:split_index]\n",
    "valid_label= int_label[split_index:]\n",
    "\n",
    "print(len(valid_label))\n",
    "train_batches = get_batches(train_source,train_label, batch_size, seq_length)\n",
    "valid_batches=get_batches(valid_source,valid_label, 800, seq_length)\n",
    "valid_x=valid_batches[0][0]\n",
    "valid_y=valid_batches[0][1]\n",
    "#print(valid_x)\n",
    "#print(valid_y)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(train_batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                lr: learning_rate,\n",
    "                keep_prob:0.5}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(train_batches) + batch_i) % show_every_n_batches == 0:\n",
    "                feed_valid={\n",
    "                input_text: valid_x,\n",
    "                keep_prob:1,\n",
    "                targets: valid_y}\n",
    "                valid_loss,= sess.run([cost], feed_valid)\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}  valid_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(train_batches),\n",
    "                    train_loss,\n",
    "                    valid_loss))\n",
    "                \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab= pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))\n",
    "token_original,len_original,token_obfused,len_obfused=pickle.load(open('twotaledumped.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return loaded_graph.get_tensor_by_name(\"input:0\"), \\\n",
    "           loaded_graph.get_tensor_by_name(\"probs:0\"),\\\n",
    "           loaded_graph.get_tensor_by_name(\"keep_prob:0\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text,vocab_to_int):\n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Proceeding 1/5809\n",
      "Proceeding 11/5809\n",
      "Proceeding 21/5809\n",
      "Proceeding 31/5809\n",
      "Proceeding 41/5809\n",
      "Proceeding 51/5809\n",
      "Proceeding 61/5809\n",
      "Proceeding 71/5809\n",
      "Proceeding 81/5809\n",
      "Proceeding 91/5809\n",
      "Proceeding 101/5809\n",
      "Proceeding 111/5809\n",
      "Proceeding 121/5809\n",
      "Proceeding 131/5809\n",
      "Proceeding 141/5809\n",
      "Proceeding 151/5809\n",
      "Proceeding 161/5809\n",
      "Proceeding 171/5809\n",
      "Proceeding 181/5809\n",
      "Proceeding 191/5809\n",
      "Proceeding 201/5809\n",
      "Proceeding 211/5809\n",
      "Proceeding 221/5809\n",
      "Proceeding 231/5809\n",
      "Proceeding 241/5809\n",
      "Proceeding 251/5809\n",
      "Proceeding 261/5809\n",
      "Proceeding 271/5809\n",
      "Proceeding 281/5809\n",
      "Proceeding 291/5809\n",
      "Proceeding 301/5809\n",
      "Proceeding 311/5809\n",
      "Proceeding 321/5809\n",
      "Proceeding 331/5809\n",
      "Proceeding 341/5809\n",
      "Proceeding 351/5809\n",
      "Proceeding 361/5809\n",
      "Proceeding 371/5809\n",
      "Proceeding 381/5809\n",
      "Proceeding 391/5809\n",
      "Proceeding 401/5809\n",
      "Proceeding 411/5809\n",
      "Proceeding 421/5809\n",
      "Proceeding 431/5809\n",
      "Proceeding 441/5809\n",
      "Proceeding 451/5809\n",
      "Proceeding 461/5809\n",
      "Proceeding 471/5809\n",
      "Proceeding 481/5809\n",
      "Proceeding 491/5809\n",
      "Proceeding 501/5809\n",
      "Proceeding 511/5809\n",
      "Proceeding 521/5809\n",
      "Proceeding 531/5809\n",
      "Proceeding 541/5809\n",
      "Proceeding 551/5809\n",
      "Proceeding 561/5809\n",
      "Proceeding 571/5809\n",
      "Proceeding 581/5809\n",
      "Proceeding 591/5809\n",
      "Proceeding 601/5809\n",
      "Proceeding 611/5809\n",
      "Proceeding 621/5809\n",
      "Proceeding 631/5809\n",
      "Proceeding 641/5809\n",
      "Proceeding 651/5809\n",
      "Proceeding 661/5809\n",
      "Proceeding 671/5809\n",
      "Proceeding 681/5809\n",
      "Proceeding 691/5809\n",
      "Proceeding 701/5809\n",
      "Proceeding 711/5809\n",
      "Proceeding 721/5809\n",
      "Proceeding 731/5809\n",
      "Proceeding 741/5809\n",
      "Proceeding 751/5809\n",
      "Proceeding 761/5809\n",
      "Proceeding 771/5809\n",
      "Proceeding 781/5809\n",
      "Proceeding 791/5809\n",
      "Proceeding 801/5809\n",
      "Proceeding 811/5809\n",
      "Proceeding 821/5809\n",
      "Proceeding 831/5809\n",
      "Proceeding 841/5809\n",
      "Proceeding 851/5809\n",
      "Proceeding 861/5809\n",
      "Proceeding 871/5809\n",
      "Proceeding 881/5809\n",
      "Proceeding 891/5809\n",
      "Proceeding 901/5809\n",
      "Proceeding 911/5809\n",
      "Proceeding 921/5809\n",
      "Proceeding 931/5809\n",
      "Proceeding 941/5809\n",
      "Proceeding 951/5809\n",
      "Proceeding 961/5809\n",
      "Proceeding 971/5809\n",
      "Proceeding 981/5809\n",
      "Proceeding 991/5809\n",
      "Proceeding 1001/5809\n",
      "Proceeding 1011/5809\n",
      "Proceeding 1021/5809\n",
      "Proceeding 1031/5809\n",
      "Proceeding 1041/5809\n",
      "Proceeding 1051/5809\n",
      "Proceeding 1061/5809\n",
      "Proceeding 1071/5809\n",
      "Proceeding 1081/5809\n",
      "Proceeding 1091/5809\n",
      "Proceeding 1101/5809\n",
      "Proceeding 1111/5809\n",
      "Proceeding 1121/5809\n",
      "Proceeding 1131/5809\n",
      "Proceeding 1141/5809\n",
      "Proceeding 1151/5809\n",
      "Proceeding 1161/5809\n",
      "Proceeding 1171/5809\n",
      "Proceeding 1181/5809\n",
      "Proceeding 1191/5809\n",
      "Proceeding 1201/5809\n",
      "Proceeding 1211/5809\n",
      "Proceeding 1221/5809\n",
      "Proceeding 1231/5809\n",
      "Proceeding 1241/5809\n",
      "Proceeding 1251/5809\n",
      "Proceeding 1261/5809\n",
      "Proceeding 1271/5809\n",
      "Proceeding 1281/5809\n",
      "Proceeding 1291/5809\n",
      "Proceeding 1301/5809\n",
      "Proceeding 1311/5809\n",
      "Proceeding 1321/5809\n",
      "Proceeding 1331/5809\n",
      "Proceeding 1341/5809\n",
      "Proceeding 1351/5809\n",
      "Proceeding 1361/5809\n",
      "Proceeding 1371/5809\n",
      "Proceeding 1381/5809\n",
      "Proceeding 1391/5809\n",
      "Proceeding 1401/5809\n",
      "Proceeding 1411/5809\n",
      "Proceeding 1421/5809\n",
      "Proceeding 1431/5809\n",
      "Proceeding 1441/5809\n",
      "Proceeding 1451/5809\n",
      "Proceeding 1461/5809\n",
      "Proceeding 1471/5809\n",
      "Proceeding 1481/5809\n",
      "Proceeding 1491/5809\n",
      "Proceeding 1501/5809\n",
      "Proceeding 1511/5809\n",
      "Proceeding 1521/5809\n",
      "Proceeding 1531/5809\n",
      "Proceeding 1541/5809\n",
      "Proceeding 1551/5809\n",
      "Proceeding 1561/5809\n",
      "Proceeding 1571/5809\n",
      "Proceeding 1581/5809\n",
      "Proceeding 1591/5809\n",
      "Proceeding 1601/5809\n",
      "Proceeding 1611/5809\n",
      "Proceeding 1621/5809\n",
      "Proceeding 1631/5809\n",
      "Proceeding 1641/5809\n",
      "Proceeding 1651/5809\n",
      "Proceeding 1661/5809\n",
      "Proceeding 1671/5809\n",
      "Proceeding 1681/5809\n",
      "Proceeding 1691/5809\n",
      "Proceeding 1701/5809\n",
      "Proceeding 1711/5809\n",
      "Proceeding 1721/5809\n",
      "Proceeding 1731/5809\n",
      "Proceeding 1741/5809\n",
      "Proceeding 1751/5809\n",
      "Proceeding 1761/5809\n",
      "Proceeding 1771/5809\n",
      "Proceeding 1781/5809\n",
      "Proceeding 1791/5809\n",
      "Proceeding 1801/5809\n",
      "Proceeding 1811/5809\n",
      "Proceeding 1821/5809\n",
      "Proceeding 1831/5809\n",
      "Proceeding 1841/5809\n",
      "Proceeding 1851/5809\n",
      "Proceeding 1861/5809\n",
      "Proceeding 1871/5809\n",
      "Proceeding 1881/5809\n",
      "Proceeding 1891/5809\n",
      "Proceeding 1901/5809\n",
      "Proceeding 1911/5809\n",
      "Proceeding 1921/5809\n",
      "Proceeding 1931/5809\n",
      "Proceeding 1941/5809\n",
      "Proceeding 1951/5809\n",
      "Proceeding 1961/5809\n",
      "Proceeding 1971/5809\n",
      "Proceeding 1981/5809\n",
      "Proceeding 1991/5809\n",
      "Proceeding 2001/5809\n",
      "Proceeding 2011/5809\n",
      "Proceeding 2021/5809\n",
      "Proceeding 2031/5809\n",
      "Proceeding 2041/5809\n",
      "Proceeding 2051/5809\n",
      "Proceeding 2061/5809\n",
      "Proceeding 2071/5809\n",
      "Proceeding 2081/5809\n",
      "Proceeding 2091/5809\n",
      "Proceeding 2101/5809\n",
      "Proceeding 2111/5809\n",
      "Proceeding 2121/5809\n",
      "Proceeding 2131/5809\n",
      "Proceeding 2141/5809\n",
      "Proceeding 2151/5809\n",
      "Proceeding 2161/5809\n",
      "Proceeding 2171/5809\n",
      "Proceeding 2181/5809\n",
      "Proceeding 2191/5809\n",
      "Proceeding 2201/5809\n",
      "Proceeding 2211/5809\n",
      "Proceeding 2221/5809\n",
      "Proceeding 2231/5809\n",
      "Proceeding 2241/5809\n",
      "Proceeding 2251/5809\n",
      "Proceeding 2261/5809\n",
      "Proceeding 2271/5809\n",
      "Proceeding 2281/5809\n",
      "Proceeding 2291/5809\n",
      "Proceeding 2301/5809\n",
      "Proceeding 2311/5809\n",
      "Proceeding 2321/5809\n",
      "Proceeding 2331/5809\n",
      "Proceeding 2341/5809\n",
      "Proceeding 2351/5809\n",
      "Proceeding 2361/5809\n",
      "Proceeding 2371/5809\n",
      "Proceeding 2381/5809\n",
      "Proceeding 2391/5809\n",
      "Proceeding 2401/5809\n",
      "Proceeding 2411/5809\n",
      "Proceeding 2421/5809\n",
      "Proceeding 2431/5809\n",
      "Proceeding 2441/5809\n",
      "Proceeding 2451/5809\n",
      "Proceeding 2461/5809\n",
      "Proceeding 2471/5809\n",
      "Proceeding 2481/5809\n",
      "Proceeding 2491/5809\n",
      "Proceeding 2501/5809\n",
      "Proceeding 2511/5809\n",
      "Proceeding 2521/5809\n",
      "Proceeding 2531/5809\n",
      "Proceeding 2541/5809\n",
      "Proceeding 2551/5809\n",
      "Proceeding 2561/5809\n",
      "Proceeding 2571/5809\n",
      "Proceeding 2581/5809\n",
      "Proceeding 2591/5809\n",
      "Proceeding 2601/5809\n",
      "Proceeding 2611/5809\n",
      "Proceeding 2621/5809\n",
      "Proceeding 2631/5809\n",
      "Proceeding 2641/5809\n",
      "Proceeding 2651/5809\n",
      "Proceeding 2661/5809\n",
      "Proceeding 2671/5809\n",
      "Proceeding 2681/5809\n",
      "Proceeding 2691/5809\n",
      "Proceeding 2701/5809\n",
      "Proceeding 2711/5809\n",
      "Proceeding 2721/5809\n",
      "Proceeding 2731/5809\n",
      "Proceeding 2741/5809\n",
      "Proceeding 2751/5809\n",
      "Proceeding 2761/5809\n",
      "Proceeding 2771/5809\n",
      "Proceeding 2781/5809\n",
      "Proceeding 2791/5809\n",
      "Proceeding 2801/5809\n",
      "Proceeding 2811/5809\n",
      "Proceeding 2821/5809\n",
      "Proceeding 2831/5809\n",
      "Proceeding 2841/5809\n",
      "Proceeding 2851/5809\n",
      "Proceeding 2861/5809\n",
      "Proceeding 2871/5809\n",
      "Proceeding 2881/5809\n",
      "Proceeding 2891/5809\n",
      "Proceeding 2901/5809\n",
      "Proceeding 2911/5809\n",
      "Proceeding 2921/5809\n",
      "Proceeding 2931/5809\n",
      "Proceeding 2941/5809\n",
      "Proceeding 2951/5809\n",
      "Proceeding 2961/5809\n",
      "Proceeding 2971/5809\n",
      "Proceeding 2981/5809\n",
      "Proceeding 2991/5809\n",
      "Proceeding 3001/5809\n",
      "Proceeding 3011/5809\n",
      "Proceeding 3021/5809\n",
      "Proceeding 3031/5809\n",
      "Proceeding 3041/5809\n",
      "Proceeding 3051/5809\n",
      "Proceeding 3061/5809\n",
      "Proceeding 3071/5809\n",
      "Proceeding 3081/5809\n",
      "Proceeding 3091/5809\n",
      "Proceeding 3101/5809\n",
      "Proceeding 3111/5809\n",
      "Proceeding 3121/5809\n",
      "Proceeding 3131/5809\n",
      "Proceeding 3141/5809\n",
      "Proceeding 3151/5809\n",
      "Proceeding 3161/5809\n",
      "Proceeding 3171/5809\n",
      "Proceeding 3181/5809\n",
      "Proceeding 3191/5809\n",
      "Proceeding 3201/5809\n",
      "Proceeding 3211/5809\n",
      "Proceeding 3221/5809\n",
      "Proceeding 3231/5809\n",
      "Proceeding 3241/5809\n",
      "Proceeding 3251/5809\n",
      "Proceeding 3261/5809\n",
      "Proceeding 3271/5809\n",
      "Proceeding 3281/5809\n",
      "Proceeding 3291/5809\n",
      "Proceeding 3301/5809\n",
      "Proceeding 3311/5809\n",
      "Proceeding 3321/5809\n",
      "Proceeding 3331/5809\n",
      "Proceeding 3341/5809\n",
      "Proceeding 3351/5809\n",
      "Proceeding 3361/5809\n",
      "Proceeding 3371/5809\n",
      "Proceeding 3381/5809\n",
      "Proceeding 3391/5809\n",
      "Proceeding 3401/5809\n",
      "Proceeding 3411/5809\n",
      "Proceeding 3421/5809\n",
      "Proceeding 3431/5809\n",
      "Proceeding 3441/5809\n",
      "Proceeding 3451/5809\n",
      "Proceeding 3461/5809\n",
      "Proceeding 3471/5809\n",
      "Proceeding 3481/5809\n",
      "Proceeding 3491/5809\n",
      "Proceeding 3501/5809\n",
      "Proceeding 3511/5809\n",
      "Proceeding 3521/5809\n",
      "Proceeding 3531/5809\n",
      "Proceeding 3541/5809\n",
      "Proceeding 3551/5809\n",
      "Proceeding 3561/5809\n",
      "Proceeding 3571/5809\n",
      "Proceeding 3581/5809\n",
      "Proceeding 3591/5809\n",
      "Proceeding 3601/5809\n",
      "Proceeding 3611/5809\n",
      "Proceeding 3621/5809\n",
      "Proceeding 3631/5809\n",
      "Proceeding 3641/5809\n",
      "Proceeding 3651/5809\n",
      "Proceeding 3661/5809\n",
      "Proceeding 3671/5809\n",
      "Proceeding 3681/5809\n",
      "Proceeding 3691/5809\n",
      "Proceeding 3701/5809\n",
      "Proceeding 3711/5809\n",
      "Proceeding 3721/5809\n",
      "Proceeding 3731/5809\n",
      "Proceeding 3741/5809\n",
      "Proceeding 3751/5809\n",
      "Proceeding 3761/5809\n",
      "Proceeding 3771/5809\n",
      "Proceeding 3781/5809\n",
      "Proceeding 3791/5809\n",
      "Proceeding 3801/5809\n",
      "Proceeding 3811/5809\n",
      "Proceeding 3821/5809\n",
      "Proceeding 3831/5809\n",
      "Proceeding 3841/5809\n",
      "Proceeding 3851/5809\n",
      "Proceeding 3861/5809\n",
      "Proceeding 3871/5809\n",
      "Proceeding 3881/5809\n",
      "Proceeding 3891/5809\n",
      "Proceeding 3901/5809\n",
      "Proceeding 3911/5809\n",
      "Proceeding 3921/5809\n",
      "Proceeding 3931/5809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding 3941/5809\n",
      "Proceeding 3951/5809\n",
      "Proceeding 3961/5809\n",
      "Proceeding 3971/5809\n",
      "Proceeding 3981/5809\n",
      "Proceeding 3991/5809\n",
      "Proceeding 4001/5809\n",
      "Proceeding 4011/5809\n",
      "Proceeding 4021/5809\n",
      "Proceeding 4031/5809\n",
      "Proceeding 4041/5809\n",
      "Proceeding 4051/5809\n",
      "Proceeding 4061/5809\n",
      "Proceeding 4071/5809\n",
      "Proceeding 4081/5809\n",
      "Proceeding 4091/5809\n",
      "Proceeding 4101/5809\n",
      "Proceeding 4111/5809\n",
      "Proceeding 4121/5809\n",
      "Proceeding 4131/5809\n",
      "Proceeding 4141/5809\n",
      "Proceeding 4151/5809\n",
      "Proceeding 4161/5809\n",
      "Proceeding 4171/5809\n",
      "Proceeding 4181/5809\n",
      "Proceeding 4191/5809\n",
      "Proceeding 4201/5809\n",
      "Proceeding 4211/5809\n",
      "Proceeding 4221/5809\n",
      "Proceeding 4231/5809\n",
      "Proceeding 4241/5809\n",
      "Proceeding 4251/5809\n",
      "Proceeding 4261/5809\n",
      "Proceeding 4271/5809\n",
      "Proceeding 4281/5809\n",
      "Proceeding 4291/5809\n",
      "Proceeding 4301/5809\n",
      "Proceeding 4311/5809\n",
      "Proceeding 4321/5809\n",
      "Proceeding 4331/5809\n",
      "Proceeding 4341/5809\n",
      "Proceeding 4351/5809\n",
      "Proceeding 4361/5809\n",
      "Proceeding 4371/5809\n",
      "Proceeding 4381/5809\n",
      "Proceeding 4391/5809\n",
      "Proceeding 4401/5809\n",
      "Proceeding 4411/5809\n",
      "Proceeding 4421/5809\n",
      "Proceeding 4431/5809\n",
      "Proceeding 4441/5809\n",
      "Proceeding 4451/5809\n",
      "Proceeding 4461/5809\n",
      "Proceeding 4471/5809\n",
      "Proceeding 4481/5809\n",
      "Proceeding 4491/5809\n",
      "Proceeding 4501/5809\n",
      "Proceeding 4511/5809\n",
      "Proceeding 4521/5809\n",
      "Proceeding 4531/5809\n",
      "Proceeding 4541/5809\n",
      "Proceeding 4551/5809\n",
      "Proceeding 4561/5809\n",
      "Proceeding 4571/5809\n",
      "Proceeding 4581/5809\n",
      "Proceeding 4591/5809\n",
      "Proceeding 4601/5809\n",
      "Proceeding 4611/5809\n",
      "Proceeding 4621/5809\n",
      "Proceeding 4631/5809\n",
      "Proceeding 4641/5809\n",
      "Proceeding 4651/5809\n",
      "Proceeding 4661/5809\n",
      "Proceeding 4671/5809\n",
      "Proceeding 4681/5809\n",
      "Proceeding 4691/5809\n",
      "Proceeding 4701/5809\n",
      "Proceeding 4711/5809\n",
      "Proceeding 4721/5809\n",
      "Proceeding 4731/5809\n",
      "Proceeding 4741/5809\n",
      "Proceeding 4751/5809\n",
      "Proceeding 4761/5809\n",
      "Proceeding 4771/5809\n",
      "Proceeding 4781/5809\n",
      "Proceeding 4791/5809\n",
      "Proceeding 4801/5809\n",
      "Proceeding 4811/5809\n",
      "Proceeding 4821/5809\n",
      "Proceeding 4831/5809\n",
      "Proceeding 4841/5809\n",
      "Proceeding 4851/5809\n",
      "Proceeding 4861/5809\n",
      "Proceeding 4871/5809\n",
      "Proceeding 4881/5809\n",
      "Proceeding 4891/5809\n",
      "Proceeding 4901/5809\n",
      "Proceeding 4911/5809\n",
      "Proceeding 4921/5809\n",
      "Proceeding 4931/5809\n",
      "Proceeding 4941/5809\n",
      "Proceeding 4951/5809\n",
      "Proceeding 4961/5809\n",
      "Proceeding 4971/5809\n",
      "Proceeding 4981/5809\n",
      "Proceeding 4991/5809\n",
      "Proceeding 5001/5809\n",
      "Proceeding 5011/5809\n",
      "Proceeding 5021/5809\n",
      "Proceeding 5031/5809\n",
      "Proceeding 5041/5809\n",
      "Proceeding 5051/5809\n",
      "Proceeding 5061/5809\n",
      "Proceeding 5071/5809\n",
      "Proceeding 5081/5809\n",
      "Proceeding 5091/5809\n",
      "Proceeding 5101/5809\n",
      "Proceeding 5111/5809\n",
      "Proceeding 5121/5809\n",
      "Proceeding 5131/5809\n",
      "Proceeding 5141/5809\n",
      "Proceeding 5151/5809\n",
      "Proceeding 5161/5809\n",
      "Proceeding 5171/5809\n",
      "Proceeding 5181/5809\n",
      "Proceeding 5191/5809\n",
      "Proceeding 5201/5809\n",
      "Proceeding 5211/5809\n",
      "Proceeding 5221/5809\n",
      "Proceeding 5231/5809\n",
      "Proceeding 5241/5809\n",
      "Proceeding 5251/5809\n",
      "Proceeding 5261/5809\n",
      "Proceeding 5271/5809\n",
      "Proceeding 5281/5809\n",
      "Proceeding 5291/5809\n",
      "Proceeding 5301/5809\n",
      "Proceeding 5311/5809\n",
      "Proceeding 5321/5809\n",
      "Proceeding 5331/5809\n",
      "Proceeding 5341/5809\n",
      "Proceeding 5351/5809\n",
      "Proceeding 5361/5809\n",
      "Proceeding 5371/5809\n",
      "Proceeding 5381/5809\n",
      "Proceeding 5391/5809\n",
      "Proceeding 5401/5809\n",
      "Proceeding 5411/5809\n",
      "Proceeding 5421/5809\n",
      "Proceeding 5431/5809\n",
      "Proceeding 5441/5809\n",
      "Proceeding 5451/5809\n",
      "Proceeding 5461/5809\n",
      "Proceeding 5471/5809\n",
      "Proceeding 5481/5809\n",
      "Proceeding 5491/5809\n",
      "Proceeding 5501/5809\n",
      "Proceeding 5511/5809\n",
      "Proceeding 5521/5809\n",
      "Proceeding 5531/5809\n",
      "Proceeding 5541/5809\n",
      "Proceeding 5551/5809\n",
      "Proceeding 5561/5809\n",
      "Proceeding 5571/5809\n",
      "Proceeding 5581/5809\n",
      "Proceeding 5591/5809\n",
      "Proceeding 5601/5809\n",
      "Proceeding 5611/5809\n",
      "Proceeding 5621/5809\n",
      "Proceeding 5631/5809\n",
      "Proceeding 5641/5809\n",
      "Proceeding 5651/5809\n",
      "Proceeding 5661/5809\n",
      "Proceeding 5671/5809\n",
      "Proceeding 5681/5809\n",
      "Proceeding 5691/5809\n",
      "Proceeding 5701/5809\n",
      "Proceeding 5711/5809\n",
      "Proceeding 5721/5809\n",
      "Proceeding 5731/5809\n",
      "Proceeding 5741/5809\n",
      "Proceeding 5751/5809\n",
      "Proceeding 5761/5809\n",
      "Proceeding 5771/5809\n",
      "Proceeding 5781/5809\n",
      "Proceeding 5791/5809\n",
      "Proceeding 5801/5809\n",
      "0.737466307277628\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, probs,keep_prob = get_tensors(loaded_graph)\n",
    "        \n",
    "    mixnum=0\n",
    "    totalnum=0\n",
    "    for i,sentence_token in enumerate(token_obfused):\n",
    "        if i%10==0:\n",
    "            print(\"Proceeding {}/{}\".format(i+1,len(token_obfused)))\n",
    "        \n",
    "        input_sentence = [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in sentence_token]\n",
    "        \n",
    "        batch_shell = np.zeros((1, len(input_sentence)))\n",
    "        batch_shell[0] = input_sentence\n",
    "        chatbot_logits = sess.run(probs, {input_text: batch_shell,keep_prob:1})[0]\n",
    "        \n",
    "        original_sentence=  [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in token_original[i]]\n",
    "        predicted_sentence= [i for i in np.argmax(chatbot_logits, 1)]\n",
    "        \n",
    "        mixathe,totalathe=compare_a_the(original_sentence,predicted_sentence,vocab_to_int)\n",
    "        totalnum+=totalathe\n",
    "        mixnum+=mixathe\n",
    "    print(1-mixnum/totalnum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_sentence = [\"I\",\"saw\",\"the\",\"man\",\",\",\"who\",\"is\",\"a\",\"husband\",\"of\",\"me\"]\n",
    "    print(seq_length)\n",
    "    input_sentence = [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in input_sentence]  \n",
    "    batch_shell = np.zeros((7, len(input_sentence)))\n",
    "    batch_shell[0] = input_sentence\n",
    "    chatbot_logits = sess.run(probs, {input_text: batch_shell})[0]\n",
    "    print(chatbot_logits.shape)\n",
    "    print('Input')\n",
    "    print('  Word Ids:      {}'.format([i for i in input_sentence]))\n",
    "    print('  Input Words: {}'.format([int_to_vocab[i] for i in input_sentence]))\n",
    "    print('\\nPrediction')\n",
    "    print('  Word Ids:      {}'.format([i for i in np.argmax(chatbot_logits, 1)]))\n",
    "    print('  Chatbot Answer Words: {}'.format([int_to_vocab[i] for i in np.argmax(chatbot_logits, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
