{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Special token definition, define the regular expression to split the sentences and punctuations.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    print(\"original vocab_len: \"+str(len(vocab_list)))\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    #max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        data_as_tokens.extend(token_ids)\n",
    "    return data_as_tokens\n",
    "\n",
    "def process_data(datafile, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens= data_to_token_ids(tokenized, vocab_dict, normalize_digits=True)\n",
    "    \n",
    "    data_mixed=[]\n",
    "    for word in data_as_tokens:\n",
    "        if word==vocab_dict[\"the\"] or word==vocab_dict[\"a\"]:\n",
    "            data_mixed.append(vocab_dict[\"the\"] if random.random()<0.5 else vocab_dict[\"a\"])\n",
    "        else:\n",
    "            data_mixed.append(word)\n",
    "            \n",
    "    pickle.dump((data_as_tokens, vocab_dict,  rev_vocab_dict), open('preprocess.p', 'wb'))\n",
    "    \n",
    "    return data_as_tokens, data_mixed,vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text):\n",
    "    # Compare the total a/the pair and obfuscated a/the pair\n",
    "    \n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vocab_len: 12251\n"
     ]
    }
   ],
   "source": [
    "## Process the original data for training, which is the novel Oliver Twist\n",
    "int_label,int_text, vocab_to_int, int_to_vocab = \\\n",
    "        process_data('original.p', max_vocab_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086332096175569"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial accuracy\n",
    "mix,total=compare_a_the(int_label,int_text)\n",
    "mix/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data size\n",
    "len(int_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Tensorflow version and GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "\n",
    "    inputs=tf.placeholder(tf.int32,[None,None],name='input')\n",
    "    targets=tf.placeholder(tf.int32,[None,None],name='targets')\n",
    "    learning_rate=tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, learning_rate,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    def single_cell(keepprob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keepprob)\n",
    "        return drop\n",
    "                                         \n",
    "    cell_fw = tf.contrib.rnn.MultiRNNCell([single_cell(keep_prob) for _ in range(1)])\n",
    "    cell_bw = tf.contrib.rnn.MultiRNNCell([single_cell(keep_prob) for _ in range(1)])\n",
    "    return cell_fw,cell_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, input_data, name='embed')\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    (cell_fw,cell_bw)=cell\n",
    "    outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,cell_bw=cell_bw, inputs=inputs, dtype=tf.float32)\n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    final_state=tf.concat(final_state, 2)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return (outputs, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs,vocab_size,activation_fn=None)\n",
    "    return (logits, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text,label_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    num_batches = len(int_text) // (batch_size * seq_length)\n",
    "    xtext = int_text[:num_batches * batch_size * seq_length]\n",
    "    ytext = label_text[:num_batches * batch_size * seq_length]\n",
    "    \n",
    "        \n",
    "    xreshape=np.reshape(xtext,[batch_size,-1])\n",
    "    yreshape=np.reshape(ytext,[batch_size,-1])\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0,xreshape.shape[1],seq_length):\n",
    "        xx=xreshape[:,i:i+seq_length]\n",
    "        yy=yreshape[:,i:i+seq_length]\n",
    "        batches.append([xx,yy])\n",
    "        \n",
    "    return np.asarray(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 120\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 200\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 200\n",
    "# Sequence Length\n",
    "seq_length = 30\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'\n",
    "\n",
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr,keep_prob = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell= get_init_cell(input_data_shape[0], rnn_size,keep_prob)\n",
    "    logits,final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/41   train_loss = 9.391  valid_loss = 9.381\n",
      "Epoch   0 Batch   10/41   train_loss = 8.200  valid_loss = 7.868\n",
      "Epoch   0 Batch   20/41   train_loss = 6.551  valid_loss = 6.636\n",
      "Epoch   0 Batch   30/41   train_loss = 6.435  valid_loss = 6.588\n",
      "Epoch   0 Batch   40/41   train_loss = 6.389  valid_loss = 6.438\n",
      "Epoch   1 Batch    9/41   train_loss = 6.163  valid_loss = 6.283\n",
      "Epoch   1 Batch   19/41   train_loss = 6.073  valid_loss = 6.135\n",
      "Epoch   1 Batch   29/41   train_loss = 5.823  valid_loss = 5.956\n",
      "Epoch   1 Batch   39/41   train_loss = 5.614  valid_loss = 5.765\n",
      "Epoch   2 Batch    8/41   train_loss = 5.443  valid_loss = 5.572\n",
      "Epoch   2 Batch   18/41   train_loss = 5.267  valid_loss = 5.410\n",
      "Epoch   2 Batch   28/41   train_loss = 5.124  valid_loss = 5.264\n",
      "Epoch   2 Batch   38/41   train_loss = 4.922  valid_loss = 5.121\n",
      "Epoch   3 Batch    7/41   train_loss = 4.848  valid_loss = 4.965\n",
      "Epoch   3 Batch   17/41   train_loss = 4.768  valid_loss = 4.812\n",
      "Epoch   3 Batch   27/41   train_loss = 4.479  valid_loss = 4.660\n",
      "Epoch   3 Batch   37/41   train_loss = 4.317  valid_loss = 4.502\n",
      "Epoch   4 Batch    6/41   train_loss = 4.142  valid_loss = 4.340\n",
      "Epoch   4 Batch   16/41   train_loss = 4.049  valid_loss = 4.181\n",
      "Epoch   4 Batch   26/41   train_loss = 3.832  valid_loss = 4.034\n",
      "Epoch   4 Batch   36/41   train_loss = 3.789  valid_loss = 3.887\n",
      "Epoch   5 Batch    5/41   train_loss = 3.591  valid_loss = 3.745\n",
      "Epoch   5 Batch   15/41   train_loss = 3.366  valid_loss = 3.611\n",
      "Epoch   5 Batch   25/41   train_loss = 3.282  valid_loss = 3.492\n",
      "Epoch   5 Batch   35/41   train_loss = 3.132  valid_loss = 3.376\n",
      "Epoch   6 Batch    4/41   train_loss = 2.979  valid_loss = 3.268\n",
      "Epoch   6 Batch   14/41   train_loss = 2.891  valid_loss = 3.166\n",
      "Epoch   6 Batch   24/41   train_loss = 2.904  valid_loss = 3.077\n",
      "Epoch   6 Batch   34/41   train_loss = 2.669  valid_loss = 2.987\n",
      "Epoch   7 Batch    3/41   train_loss = 2.658  valid_loss = 2.904\n",
      "Epoch   7 Batch   13/41   train_loss = 2.688  valid_loss = 2.825\n",
      "Epoch   7 Batch   23/41   train_loss = 2.617  valid_loss = 2.755\n",
      "Epoch   7 Batch   33/41   train_loss = 2.353  valid_loss = 2.681\n",
      "Epoch   8 Batch    2/41   train_loss = 2.267  valid_loss = 2.615\n",
      "Epoch   8 Batch   12/41   train_loss = 2.292  valid_loss = 2.548\n",
      "Epoch   8 Batch   22/41   train_loss = 2.287  valid_loss = 2.491\n",
      "Epoch   8 Batch   32/41   train_loss = 2.008  valid_loss = 2.430\n",
      "Epoch   9 Batch    1/41   train_loss = 2.193  valid_loss = 2.376\n",
      "Epoch   9 Batch   11/41   train_loss = 1.995  valid_loss = 2.319\n",
      "Epoch   9 Batch   21/41   train_loss = 2.011  valid_loss = 2.276\n",
      "Epoch   9 Batch   31/41   train_loss = 1.851  valid_loss = 2.225\n",
      "Epoch  10 Batch    0/41   train_loss = 1.942  valid_loss = 2.181\n",
      "Epoch  10 Batch   10/41   train_loss = 1.779  valid_loss = 2.134\n",
      "Epoch  10 Batch   20/41   train_loss = 1.830  valid_loss = 2.100\n",
      "Epoch  10 Batch   30/41   train_loss = 1.685  valid_loss = 2.059\n",
      "Epoch  10 Batch   40/41   train_loss = 1.684  valid_loss = 2.023\n",
      "Epoch  11 Batch    9/41   train_loss = 1.667  valid_loss = 1.981\n",
      "Epoch  11 Batch   19/41   train_loss = 1.727  valid_loss = 1.956\n",
      "Epoch  11 Batch   29/41   train_loss = 1.554  valid_loss = 1.922\n",
      "Epoch  11 Batch   39/41   train_loss = 1.515  valid_loss = 1.890\n",
      "Epoch  12 Batch    8/41   train_loss = 1.455  valid_loss = 1.854\n",
      "Epoch  12 Batch   18/41   train_loss = 1.499  valid_loss = 1.834\n",
      "Epoch  12 Batch   28/41   train_loss = 1.486  valid_loss = 1.807\n",
      "Epoch  12 Batch   38/41   train_loss = 1.371  valid_loss = 1.776\n",
      "Epoch  13 Batch    7/41   train_loss = 1.398  valid_loss = 1.748\n",
      "Epoch  13 Batch   17/41   train_loss = 1.397  valid_loss = 1.725\n",
      "Epoch  13 Batch   27/41   train_loss = 1.301  valid_loss = 1.710\n",
      "Epoch  13 Batch   37/41   train_loss = 1.206  valid_loss = 1.679\n",
      "Epoch  14 Batch    6/41   train_loss = 1.227  valid_loss = 1.657\n",
      "Epoch  14 Batch   16/41   train_loss = 1.208  valid_loss = 1.630\n",
      "Epoch  14 Batch   26/41   train_loss = 1.166  valid_loss = 1.626\n",
      "Epoch  14 Batch   36/41   train_loss = 1.220  valid_loss = 1.594\n",
      "Epoch  15 Batch    5/41   train_loss = 1.151  valid_loss = 1.577\n",
      "Epoch  15 Batch   15/41   train_loss = 1.102  valid_loss = 1.550\n",
      "Epoch  15 Batch   25/41   train_loss = 1.042  valid_loss = 1.549\n",
      "Epoch  15 Batch   35/41   train_loss = 1.053  valid_loss = 1.525\n",
      "Epoch  16 Batch    4/41   train_loss = 0.987  valid_loss = 1.507\n",
      "Epoch  16 Batch   14/41   train_loss = 0.950  valid_loss = 1.484\n",
      "Epoch  16 Batch   24/41   train_loss = 1.003  valid_loss = 1.481\n",
      "Epoch  16 Batch   34/41   train_loss = 0.915  valid_loss = 1.467\n",
      "Epoch  17 Batch    3/41   train_loss = 0.935  valid_loss = 1.444\n",
      "Epoch  17 Batch   13/41   train_loss = 0.970  valid_loss = 1.427\n",
      "Epoch  17 Batch   23/41   train_loss = 0.924  valid_loss = 1.419\n",
      "Epoch  17 Batch   33/41   train_loss = 0.803  valid_loss = 1.415\n",
      "Epoch  18 Batch    2/41   train_loss = 0.762  valid_loss = 1.393\n",
      "Epoch  18 Batch   12/41   train_loss = 0.827  valid_loss = 1.376\n",
      "Epoch  18 Batch   22/41   train_loss = 0.811  valid_loss = 1.366\n",
      "Epoch  18 Batch   32/41   train_loss = 0.702  valid_loss = 1.369\n",
      "Epoch  19 Batch    1/41   train_loss = 0.869  valid_loss = 1.347\n",
      "Epoch  19 Batch   11/41   train_loss = 0.701  valid_loss = 1.332\n",
      "Epoch  19 Batch   21/41   train_loss = 0.701  valid_loss = 1.320\n",
      "Epoch  19 Batch   31/41   train_loss = 0.665  valid_loss = 1.327\n",
      "Epoch  20 Batch    0/41   train_loss = 0.752  valid_loss = 1.310\n",
      "Epoch  20 Batch   10/41   train_loss = 0.638  valid_loss = 1.291\n",
      "Epoch  20 Batch   20/41   train_loss = 0.660  valid_loss = 1.284\n",
      "Epoch  20 Batch   30/41   train_loss = 0.629  valid_loss = 1.285\n",
      "Epoch  20 Batch   40/41   train_loss = 0.622  valid_loss = 1.280\n",
      "Epoch  21 Batch    9/41   train_loss = 0.610  valid_loss = 1.257\n",
      "Epoch  21 Batch   19/41   train_loss = 0.653  valid_loss = 1.250\n",
      "Epoch  21 Batch   29/41   train_loss = 0.565  valid_loss = 1.250\n",
      "Epoch  21 Batch   39/41   train_loss = 0.583  valid_loss = 1.249\n",
      "Epoch  22 Batch    8/41   train_loss = 0.517  valid_loss = 1.230\n",
      "Epoch  22 Batch   18/41   train_loss = 0.552  valid_loss = 1.216\n",
      "Epoch  22 Batch   28/41   train_loss = 0.592  valid_loss = 1.221\n",
      "Epoch  22 Batch   38/41   train_loss = 0.522  valid_loss = 1.218\n",
      "Epoch  23 Batch    7/41   train_loss = 0.539  valid_loss = 1.208\n",
      "Epoch  23 Batch   17/41   train_loss = 0.500  valid_loss = 1.187\n",
      "Epoch  23 Batch   27/41   train_loss = 0.505  valid_loss = 1.193\n",
      "Epoch  23 Batch   37/41   train_loss = 0.451  valid_loss = 1.194\n",
      "Epoch  24 Batch    6/41   train_loss = 0.474  valid_loss = 1.184\n",
      "Epoch  24 Batch   16/41   train_loss = 0.459  valid_loss = 1.165\n",
      "Epoch  24 Batch   26/41   train_loss = 0.437  valid_loss = 1.162\n",
      "Epoch  24 Batch   36/41   train_loss = 0.438  valid_loss = 1.170\n",
      "Epoch  25 Batch    5/41   train_loss = 0.451  valid_loss = 1.164\n",
      "Epoch  25 Batch   15/41   train_loss = 0.409  valid_loss = 1.147\n",
      "Epoch  25 Batch   25/41   train_loss = 0.393  valid_loss = 1.135\n",
      "Epoch  25 Batch   35/41   train_loss = 0.408  valid_loss = 1.144\n",
      "Epoch  26 Batch    4/41   train_loss = 0.373  valid_loss = 1.144\n",
      "Epoch  26 Batch   14/41   train_loss = 0.345  valid_loss = 1.130\n",
      "Epoch  26 Batch   24/41   train_loss = 0.386  valid_loss = 1.114\n",
      "Epoch  26 Batch   34/41   train_loss = 0.348  valid_loss = 1.118\n",
      "Epoch  27 Batch    3/41   train_loss = 0.363  valid_loss = 1.123\n",
      "Epoch  27 Batch   13/41   train_loss = 0.345  valid_loss = 1.115\n",
      "Epoch  27 Batch   23/41   train_loss = 0.333  valid_loss = 1.099\n",
      "Epoch  27 Batch   33/41   train_loss = 0.300  valid_loss = 1.094\n",
      "Epoch  28 Batch    2/41   train_loss = 0.271  valid_loss = 1.101\n",
      "Epoch  28 Batch   12/41   train_loss = 0.310  valid_loss = 1.097\n",
      "Epoch  28 Batch   22/41   train_loss = 0.291  valid_loss = 1.085\n",
      "Epoch  28 Batch   32/41   train_loss = 0.256  valid_loss = 1.078\n",
      "Epoch  29 Batch    1/41   train_loss = 0.321  valid_loss = 1.082\n",
      "Epoch  29 Batch   11/41   train_loss = 0.234  valid_loss = 1.080\n",
      "Epoch  29 Batch   21/41   train_loss = 0.252  valid_loss = 1.071\n",
      "Epoch  29 Batch   31/41   train_loss = 0.230  valid_loss = 1.066\n",
      "Epoch  30 Batch    0/41   train_loss = 0.276  valid_loss = 1.063\n",
      "Epoch  30 Batch   10/41   train_loss = 0.220  valid_loss = 1.063\n",
      "Epoch  30 Batch   20/41   train_loss = 0.224  valid_loss = 1.054\n",
      "Epoch  30 Batch   30/41   train_loss = 0.222  valid_loss = 1.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30 Batch   40/41   train_loss = 0.204  valid_loss = 1.050\n",
      "Epoch  31 Batch    9/41   train_loss = 0.203  valid_loss = 1.050\n",
      "Epoch  31 Batch   19/41   train_loss = 0.219  valid_loss = 1.041\n",
      "Epoch  31 Batch   29/41   train_loss = 0.195  valid_loss = 1.039\n",
      "Epoch  31 Batch   39/41   train_loss = 0.193  valid_loss = 1.038\n",
      "Epoch  32 Batch    8/41   train_loss = 0.159  valid_loss = 1.037\n",
      "Epoch  32 Batch   18/41   train_loss = 0.182  valid_loss = 1.030\n",
      "Epoch  32 Batch   28/41   train_loss = 0.200  valid_loss = 1.024\n",
      "Epoch  32 Batch   38/41   train_loss = 0.162  valid_loss = 1.024\n",
      "Epoch  33 Batch    7/41   train_loss = 0.173  valid_loss = 1.024\n",
      "Epoch  33 Batch   17/41   train_loss = 0.152  valid_loss = 1.020\n",
      "Epoch  33 Batch   27/41   train_loss = 0.160  valid_loss = 1.011\n",
      "Epoch  33 Batch   37/41   train_loss = 0.144  valid_loss = 1.011\n",
      "Epoch  34 Batch    6/41   train_loss = 0.144  valid_loss = 1.012\n",
      "Epoch  34 Batch   16/41   train_loss = 0.141  valid_loss = 1.008\n",
      "Epoch  34 Batch   26/41   train_loss = 0.136  valid_loss = 1.001\n",
      "Epoch  34 Batch   36/41   train_loss = 0.135  valid_loss = 1.000\n",
      "Epoch  35 Batch    5/41   train_loss = 0.138  valid_loss = 1.000\n",
      "Epoch  35 Batch   15/41   train_loss = 0.124  valid_loss = 0.997\n",
      "Epoch  35 Batch   25/41   train_loss = 0.117  valid_loss = 0.992\n",
      "Epoch  35 Batch   35/41   train_loss = 0.120  valid_loss = 0.990\n",
      "Epoch  36 Batch    4/41   train_loss = 0.108  valid_loss = 0.990\n",
      "Epoch  36 Batch   14/41   train_loss = 0.097  valid_loss = 0.988\n",
      "Epoch  36 Batch   24/41   train_loss = 0.110  valid_loss = 0.983\n",
      "Epoch  36 Batch   34/41   train_loss = 0.093  valid_loss = 0.981\n",
      "Epoch  37 Batch    3/41   train_loss = 0.092  valid_loss = 0.981\n",
      "Epoch  37 Batch   13/41   train_loss = 0.091  valid_loss = 0.979\n",
      "Epoch  37 Batch   23/41   train_loss = 0.093  valid_loss = 0.975\n",
      "Epoch  37 Batch   33/41   train_loss = 0.085  valid_loss = 0.971\n",
      "Epoch  38 Batch    2/41   train_loss = 0.071  valid_loss = 0.970\n",
      "Epoch  38 Batch   12/41   train_loss = 0.079  valid_loss = 0.971\n",
      "Epoch  38 Batch   22/41   train_loss = 0.083  valid_loss = 0.966\n",
      "Epoch  38 Batch   32/41   train_loss = 0.073  valid_loss = 0.963\n",
      "Epoch  39 Batch    1/41   train_loss = 0.084  valid_loss = 0.961\n",
      "Epoch  39 Batch   11/41   train_loss = 0.062  valid_loss = 0.962\n",
      "Epoch  39 Batch   21/41   train_loss = 0.065  valid_loss = 0.959\n",
      "Epoch  39 Batch   31/41   train_loss = 0.060  valid_loss = 0.957\n",
      "Epoch  40 Batch    0/41   train_loss = 0.070  valid_loss = 0.955\n",
      "Epoch  40 Batch   10/41   train_loss = 0.057  valid_loss = 0.955\n",
      "Epoch  40 Batch   20/41   train_loss = 0.060  valid_loss = 0.951\n",
      "Epoch  40 Batch   30/41   train_loss = 0.056  valid_loss = 0.950\n",
      "Epoch  40 Batch   40/41   train_loss = 0.050  valid_loss = 0.949\n",
      "Epoch  41 Batch    9/41   train_loss = 0.052  valid_loss = 0.948\n",
      "Epoch  41 Batch   19/41   train_loss = 0.056  valid_loss = 0.946\n",
      "Epoch  41 Batch   29/41   train_loss = 0.045  valid_loss = 0.943\n",
      "Epoch  41 Batch   39/41   train_loss = 0.048  valid_loss = 0.941\n",
      "Epoch  42 Batch    8/41   train_loss = 0.042  valid_loss = 0.940\n",
      "Epoch  42 Batch   18/41   train_loss = 0.049  valid_loss = 0.939\n",
      "Epoch  42 Batch   28/41   train_loss = 0.051  valid_loss = 0.938\n",
      "Epoch  42 Batch   38/41   train_loss = 0.043  valid_loss = 0.937\n",
      "Epoch  43 Batch    7/41   train_loss = 0.047  valid_loss = 0.935\n",
      "Epoch  43 Batch   17/41   train_loss = 0.043  valid_loss = 0.934\n",
      "Epoch  43 Batch   27/41   train_loss = 0.045  valid_loss = 0.932\n",
      "Epoch  43 Batch   37/41   train_loss = 0.044  valid_loss = 0.933\n",
      "Epoch  44 Batch    6/41   train_loss = 0.036  valid_loss = 0.932\n",
      "Epoch  44 Batch   16/41   train_loss = 0.041  valid_loss = 0.931\n",
      "Epoch  44 Batch   26/41   train_loss = 0.041  valid_loss = 0.929\n",
      "Epoch  44 Batch   36/41   train_loss = 0.038  valid_loss = 0.928\n",
      "Epoch  45 Batch    5/41   train_loss = 0.036  valid_loss = 0.927\n",
      "Epoch  45 Batch   15/41   train_loss = 0.039  valid_loss = 0.926\n",
      "Epoch  45 Batch   25/41   train_loss = 0.036  valid_loss = 0.925\n",
      "Epoch  45 Batch   35/41   train_loss = 0.039  valid_loss = 0.924\n",
      "Epoch  46 Batch    4/41   train_loss = 0.034  valid_loss = 0.923\n",
      "Epoch  46 Batch   14/41   train_loss = 0.031  valid_loss = 0.923\n",
      "Epoch  46 Batch   24/41   train_loss = 0.036  valid_loss = 0.921\n",
      "Epoch  46 Batch   34/41   train_loss = 0.029  valid_loss = 0.921\n",
      "Epoch  47 Batch    3/41   train_loss = 0.030  valid_loss = 0.920\n",
      "Epoch  47 Batch   13/41   train_loss = 0.032  valid_loss = 0.919\n",
      "Epoch  47 Batch   23/41   train_loss = 0.030  valid_loss = 0.917\n",
      "Epoch  47 Batch   33/41   train_loss = 0.027  valid_loss = 0.918\n",
      "Epoch  48 Batch    2/41   train_loss = 0.026  valid_loss = 0.918\n",
      "Epoch  48 Batch   12/41   train_loss = 0.029  valid_loss = 0.916\n",
      "Epoch  48 Batch   22/41   train_loss = 0.031  valid_loss = 0.914\n",
      "Epoch  48 Batch   32/41   train_loss = 0.027  valid_loss = 0.915\n",
      "Epoch  49 Batch    1/41   train_loss = 0.034  valid_loss = 0.916\n",
      "Epoch  49 Batch   11/41   train_loss = 0.026  valid_loss = 0.915\n",
      "Epoch  49 Batch   21/41   train_loss = 0.024  valid_loss = 0.913\n",
      "Epoch  49 Batch   31/41   train_loss = 0.025  valid_loss = 0.913\n",
      "Epoch  50 Batch    0/41   train_loss = 0.027  valid_loss = 0.913\n",
      "Epoch  50 Batch   10/41   train_loss = 0.023  valid_loss = 0.911\n",
      "Epoch  50 Batch   20/41   train_loss = 0.025  valid_loss = 0.910\n",
      "Epoch  50 Batch   30/41   train_loss = 0.025  valid_loss = 0.909\n",
      "Epoch  50 Batch   40/41   train_loss = 0.022  valid_loss = 0.911\n",
      "Epoch  51 Batch    9/41   train_loss = 0.022  valid_loss = 0.909\n",
      "Epoch  51 Batch   19/41   train_loss = 0.024  valid_loss = 0.907\n",
      "Epoch  51 Batch   29/41   train_loss = 0.019  valid_loss = 0.907\n",
      "Epoch  51 Batch   39/41   train_loss = 0.021  valid_loss = 0.908\n",
      "Epoch  52 Batch    8/41   train_loss = 0.021  valid_loss = 0.908\n",
      "Epoch  52 Batch   18/41   train_loss = 0.025  valid_loss = 0.906\n",
      "Epoch  52 Batch   28/41   train_loss = 0.023  valid_loss = 0.905\n",
      "Epoch  52 Batch   38/41   train_loss = 0.023  valid_loss = 0.905\n",
      "Epoch  53 Batch    7/41   train_loss = 0.024  valid_loss = 0.904\n",
      "Epoch  53 Batch   17/41   train_loss = 0.019  valid_loss = 0.903\n",
      "Epoch  53 Batch   27/41   train_loss = 0.022  valid_loss = 0.903\n",
      "Epoch  53 Batch   37/41   train_loss = 0.021  valid_loss = 0.904\n",
      "Epoch  54 Batch    6/41   train_loss = 0.020  valid_loss = 0.904\n",
      "Epoch  54 Batch   16/41   train_loss = 0.021  valid_loss = 0.903\n",
      "Epoch  54 Batch   26/41   train_loss = 0.020  valid_loss = 0.902\n",
      "Epoch  54 Batch   36/41   train_loss = 0.019  valid_loss = 0.901\n",
      "Epoch  55 Batch    5/41   train_loss = 0.017  valid_loss = 0.902\n",
      "Epoch  55 Batch   15/41   train_loss = 0.020  valid_loss = 0.903\n",
      "Epoch  55 Batch   25/41   train_loss = 0.021  valid_loss = 0.901\n",
      "Epoch  55 Batch   35/41   train_loss = 0.019  valid_loss = 0.902\n",
      "Epoch  56 Batch    4/41   train_loss = 0.020  valid_loss = 0.902\n",
      "Epoch  56 Batch   14/41   train_loss = 0.018  valid_loss = 0.901\n",
      "Epoch  56 Batch   24/41   train_loss = 0.016  valid_loss = 0.898\n",
      "Epoch  56 Batch   34/41   train_loss = 0.016  valid_loss = 0.899\n",
      "Epoch  57 Batch    3/41   train_loss = 0.016  valid_loss = 0.900\n",
      "Epoch  57 Batch   13/41   train_loss = 0.018  valid_loss = 0.900\n",
      "Epoch  57 Batch   23/41   train_loss = 0.016  valid_loss = 0.898\n",
      "Epoch  57 Batch   33/41   train_loss = 0.016  valid_loss = 0.898\n",
      "Epoch  58 Batch    2/41   train_loss = 0.015  valid_loss = 0.900\n",
      "Epoch  58 Batch   12/41   train_loss = 0.015  valid_loss = 0.898\n",
      "Epoch  58 Batch   22/41   train_loss = 0.017  valid_loss = 0.896\n",
      "Epoch  58 Batch   32/41   train_loss = 0.014  valid_loss = 0.897\n",
      "Epoch  59 Batch    1/41   train_loss = 0.018  valid_loss = 0.897\n",
      "Epoch  59 Batch   11/41   train_loss = 0.016  valid_loss = 0.896\n",
      "Epoch  59 Batch   21/41   train_loss = 0.017  valid_loss = 0.896\n",
      "Epoch  59 Batch   31/41   train_loss = 0.015  valid_loss = 0.896\n",
      "Epoch  60 Batch    0/41   train_loss = 0.014  valid_loss = 0.896\n",
      "Epoch  60 Batch   10/41   train_loss = 0.014  valid_loss = 0.895\n",
      "Epoch  60 Batch   20/41   train_loss = 0.013  valid_loss = 0.894\n",
      "Epoch  60 Batch   30/41   train_loss = 0.015  valid_loss = 0.894\n",
      "Epoch  60 Batch   40/41   train_loss = 0.013  valid_loss = 0.895\n",
      "Epoch  61 Batch    9/41   train_loss = 0.012  valid_loss = 0.895\n",
      "Epoch  61 Batch   19/41   train_loss = 0.015  valid_loss = 0.894\n",
      "Epoch  61 Batch   29/41   train_loss = 0.010  valid_loss = 0.894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  61 Batch   39/41   train_loss = 0.012  valid_loss = 0.894\n",
      "Epoch  62 Batch    8/41   train_loss = 0.011  valid_loss = 0.895\n",
      "Epoch  62 Batch   18/41   train_loss = 0.014  valid_loss = 0.894\n",
      "Epoch  62 Batch   28/41   train_loss = 0.013  valid_loss = 0.893\n",
      "Epoch  62 Batch   38/41   train_loss = 0.012  valid_loss = 0.894\n",
      "Epoch  63 Batch    7/41   train_loss = 0.014  valid_loss = 0.896\n",
      "Epoch  63 Batch   17/41   train_loss = 0.012  valid_loss = 0.893\n",
      "Epoch  63 Batch   27/41   train_loss = 0.016  valid_loss = 0.892\n",
      "Epoch  63 Batch   37/41   train_loss = 0.012  valid_loss = 0.893\n",
      "Epoch  64 Batch    6/41   train_loss = 0.011  valid_loss = 0.895\n",
      "Epoch  64 Batch   16/41   train_loss = 0.013  valid_loss = 0.891\n",
      "Epoch  64 Batch   26/41   train_loss = 0.012  valid_loss = 0.890\n",
      "Epoch  64 Batch   36/41   train_loss = 0.013  valid_loss = 0.892\n",
      "Epoch  65 Batch    5/41   train_loss = 0.010  valid_loss = 0.893\n",
      "Epoch  65 Batch   15/41   train_loss = 0.011  valid_loss = 0.891\n",
      "Epoch  65 Batch   25/41   train_loss = 0.011  valid_loss = 0.891\n",
      "Epoch  65 Batch   35/41   train_loss = 0.011  valid_loss = 0.892\n",
      "Epoch  66 Batch    4/41   train_loss = 0.011  valid_loss = 0.891\n",
      "Epoch  66 Batch   14/41   train_loss = 0.010  valid_loss = 0.891\n",
      "Epoch  66 Batch   24/41   train_loss = 0.011  valid_loss = 0.891\n",
      "Epoch  66 Batch   34/41   train_loss = 0.010  valid_loss = 0.890\n",
      "Epoch  67 Batch    3/41   train_loss = 0.009  valid_loss = 0.890\n",
      "Epoch  67 Batch   13/41   train_loss = 0.010  valid_loss = 0.892\n",
      "Epoch  67 Batch   23/41   train_loss = 0.011  valid_loss = 0.891\n",
      "Epoch  67 Batch   33/41   train_loss = 0.009  valid_loss = 0.889\n",
      "Epoch  68 Batch    2/41   train_loss = 0.009  valid_loss = 0.890\n",
      "Epoch  68 Batch   12/41   train_loss = 0.010  valid_loss = 0.894\n",
      "Epoch  68 Batch   22/41   train_loss = 0.011  valid_loss = 0.890\n",
      "Epoch  68 Batch   32/41   train_loss = 0.010  valid_loss = 0.890\n",
      "Epoch  69 Batch    1/41   train_loss = 0.014  valid_loss = 0.889\n",
      "Epoch  69 Batch   11/41   train_loss = 0.009  valid_loss = 0.893\n",
      "Epoch  69 Batch   21/41   train_loss = 0.010  valid_loss = 0.889\n",
      "Epoch  69 Batch   31/41   train_loss = 0.010  valid_loss = 0.889\n",
      "Epoch  70 Batch    0/41   train_loss = 0.009  valid_loss = 0.889\n",
      "Epoch  70 Batch   10/41   train_loss = 0.008  valid_loss = 0.892\n",
      "Epoch  70 Batch   20/41   train_loss = 0.010  valid_loss = 0.888\n",
      "Epoch  70 Batch   30/41   train_loss = 0.009  valid_loss = 0.889\n",
      "Epoch  70 Batch   40/41   train_loss = 0.009  valid_loss = 0.889\n",
      "Epoch  71 Batch    9/41   train_loss = 0.010  valid_loss = 0.890\n",
      "Epoch  71 Batch   19/41   train_loss = 0.011  valid_loss = 0.887\n",
      "Epoch  71 Batch   29/41   train_loss = 0.007  valid_loss = 0.889\n",
      "Epoch  71 Batch   39/41   train_loss = 0.008  valid_loss = 0.886\n",
      "Epoch  72 Batch    8/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  72 Batch   18/41   train_loss = 0.008  valid_loss = 0.888\n",
      "Epoch  72 Batch   28/41   train_loss = 0.009  valid_loss = 0.889\n",
      "Epoch  72 Batch   38/41   train_loss = 0.007  valid_loss = 0.888\n",
      "Epoch  73 Batch    7/41   train_loss = 0.009  valid_loss = 0.888\n",
      "Epoch  73 Batch   17/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  73 Batch   27/41   train_loss = 0.010  valid_loss = 0.888\n",
      "Epoch  73 Batch   37/41   train_loss = 0.010  valid_loss = 0.888\n",
      "Epoch  74 Batch    6/41   train_loss = 0.009  valid_loss = 0.888\n",
      "Epoch  74 Batch   16/41   train_loss = 0.010  valid_loss = 0.888\n",
      "Epoch  74 Batch   26/41   train_loss = 0.007  valid_loss = 0.886\n",
      "Epoch  74 Batch   36/41   train_loss = 0.009  valid_loss = 0.887\n",
      "Epoch  75 Batch    5/41   train_loss = 0.007  valid_loss = 0.890\n",
      "Epoch  75 Batch   15/41   train_loss = 0.008  valid_loss = 0.887\n",
      "Epoch  75 Batch   25/41   train_loss = 0.008  valid_loss = 0.886\n",
      "Epoch  75 Batch   35/41   train_loss = 0.008  valid_loss = 0.886\n",
      "Epoch  76 Batch    4/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  76 Batch   14/41   train_loss = 0.008  valid_loss = 0.887\n",
      "Epoch  76 Batch   24/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  76 Batch   34/41   train_loss = 0.007  valid_loss = 0.888\n",
      "Epoch  77 Batch    3/41   train_loss = 0.006  valid_loss = 0.888\n",
      "Epoch  77 Batch   13/41   train_loss = 0.006  valid_loss = 0.888\n",
      "Epoch  77 Batch   23/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  77 Batch   33/41   train_loss = 0.006  valid_loss = 0.887\n",
      "Epoch  78 Batch    2/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  78 Batch   12/41   train_loss = 0.007  valid_loss = 0.889\n",
      "Epoch  78 Batch   22/41   train_loss = 0.008  valid_loss = 0.885\n",
      "Epoch  78 Batch   32/41   train_loss = 0.006  valid_loss = 0.885\n",
      "Epoch  79 Batch    1/41   train_loss = 0.009  valid_loss = 0.886\n",
      "Epoch  79 Batch   11/41   train_loss = 0.007  valid_loss = 0.891\n",
      "Epoch  79 Batch   21/41   train_loss = 0.008  valid_loss = 0.886\n",
      "Epoch  79 Batch   31/41   train_loss = 0.006  valid_loss = 0.887\n",
      "Epoch  80 Batch    0/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  80 Batch   10/41   train_loss = 0.005  valid_loss = 0.890\n",
      "Epoch  80 Batch   20/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  80 Batch   30/41   train_loss = 0.007  valid_loss = 0.889\n",
      "Epoch  80 Batch   40/41   train_loss = 0.006  valid_loss = 0.885\n",
      "Epoch  81 Batch    9/41   train_loss = 0.007  valid_loss = 0.887\n",
      "Epoch  81 Batch   19/41   train_loss = 0.007  valid_loss = 0.886\n",
      "Epoch  81 Batch   29/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  81 Batch   39/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  82 Batch    8/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  82 Batch   18/41   train_loss = 0.006  valid_loss = 0.888\n",
      "Epoch  82 Batch   28/41   train_loss = 0.006  valid_loss = 0.887\n",
      "Epoch  82 Batch   38/41   train_loss = 0.006  valid_loss = 0.886\n",
      "Epoch  83 Batch    7/41   train_loss = 0.006  valid_loss = 0.887\n",
      "Epoch  83 Batch   17/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  83 Batch   27/41   train_loss = 0.006  valid_loss = 0.887\n",
      "Epoch  83 Batch   37/41   train_loss = 0.005  valid_loss = 0.888\n",
      "Epoch  84 Batch    6/41   train_loss = 0.005  valid_loss = 0.888\n",
      "Epoch  84 Batch   16/41   train_loss = 0.006  valid_loss = 0.889\n",
      "Epoch  84 Batch   26/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  84 Batch   36/41   train_loss = 0.006  valid_loss = 0.887\n",
      "Epoch  85 Batch    5/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  85 Batch   15/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  85 Batch   25/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  85 Batch   35/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  86 Batch    4/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  86 Batch   14/41   train_loss = 0.005  valid_loss = 0.892\n",
      "Epoch  86 Batch   24/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  86 Batch   34/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  87 Batch    3/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  87 Batch   13/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  87 Batch   23/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch  87 Batch   33/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  88 Batch    2/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch  88 Batch   12/41   train_loss = 0.005  valid_loss = 0.889\n",
      "Epoch  88 Batch   22/41   train_loss = 0.006  valid_loss = 0.886\n",
      "Epoch  88 Batch   32/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  89 Batch    1/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  89 Batch   11/41   train_loss = 0.007  valid_loss = 0.892\n",
      "Epoch  89 Batch   21/41   train_loss = 0.005  valid_loss = 0.886\n",
      "Epoch  89 Batch   31/41   train_loss = 0.004  valid_loss = 0.885\n",
      "Epoch  90 Batch    0/41   train_loss = 0.004  valid_loss = 0.885\n",
      "Epoch  90 Batch   10/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  90 Batch   20/41   train_loss = 0.005  valid_loss = 0.886\n",
      "Epoch  90 Batch   30/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  90 Batch   40/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  91 Batch    9/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  91 Batch   19/41   train_loss = 0.005  valid_loss = 0.886\n",
      "Epoch  91 Batch   29/41   train_loss = 0.003  valid_loss = 0.885\n",
      "Epoch  91 Batch   39/41   train_loss = 0.004  valid_loss = 0.885\n",
      "Epoch  92 Batch    8/41   train_loss = 0.004  valid_loss = 0.885\n",
      "Epoch  92 Batch   18/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  92 Batch   28/41   train_loss = 0.004  valid_loss = 0.887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  92 Batch   38/41   train_loss = 0.005  valid_loss = 0.885\n",
      "Epoch  93 Batch    7/41   train_loss = 0.004  valid_loss = 0.885\n",
      "Epoch  93 Batch   17/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  93 Batch   27/41   train_loss = 0.008  valid_loss = 0.887\n",
      "Epoch  93 Batch   37/41   train_loss = 0.006  valid_loss = 0.889\n",
      "Epoch  94 Batch    6/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch  94 Batch   16/41   train_loss = 0.004  valid_loss = 0.892\n",
      "Epoch  94 Batch   26/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch  94 Batch   36/41   train_loss = 0.006  valid_loss = 0.891\n",
      "Epoch  95 Batch    5/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch  95 Batch   15/41   train_loss = 0.004  valid_loss = 0.892\n",
      "Epoch  95 Batch   25/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  95 Batch   35/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch  96 Batch    4/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  96 Batch   14/41   train_loss = 0.003  valid_loss = 0.897\n",
      "Epoch  96 Batch   24/41   train_loss = 0.005  valid_loss = 0.888\n",
      "Epoch  96 Batch   34/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch  97 Batch    3/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch  97 Batch   13/41   train_loss = 0.003  valid_loss = 0.893\n",
      "Epoch  97 Batch   23/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch  97 Batch   33/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch  98 Batch    2/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch  98 Batch   12/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch  98 Batch   22/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch  98 Batch   32/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch  99 Batch    1/41   train_loss = 0.006  valid_loss = 0.886\n",
      "Epoch  99 Batch   11/41   train_loss = 0.005  valid_loss = 0.887\n",
      "Epoch  99 Batch   21/41   train_loss = 0.004  valid_loss = 0.887\n",
      "Epoch  99 Batch   31/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 100 Batch    0/41   train_loss = 0.003  valid_loss = 0.885\n",
      "Epoch 100 Batch   10/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch 100 Batch   20/41   train_loss = 0.003  valid_loss = 0.889\n",
      "Epoch 100 Batch   30/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch 100 Batch   40/41   train_loss = 0.003  valid_loss = 0.888\n",
      "Epoch 101 Batch    9/41   train_loss = 0.004  valid_loss = 0.885\n",
      "Epoch 101 Batch   19/41   train_loss = 0.005  valid_loss = 0.893\n",
      "Epoch 101 Batch   29/41   train_loss = 0.004  valid_loss = 0.888\n",
      "Epoch 101 Batch   39/41   train_loss = 0.003  valid_loss = 0.891\n",
      "Epoch 102 Batch    8/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch 102 Batch   18/41   train_loss = 0.003  valid_loss = 0.896\n",
      "Epoch 102 Batch   28/41   train_loss = 0.005  valid_loss = 0.891\n",
      "Epoch 102 Batch   38/41   train_loss = 0.004  valid_loss = 0.893\n",
      "Epoch 103 Batch    7/41   train_loss = 0.003  valid_loss = 0.886\n",
      "Epoch 103 Batch   17/41   train_loss = 0.004  valid_loss = 0.894\n",
      "Epoch 103 Batch   27/41   train_loss = 0.004  valid_loss = 0.890\n",
      "Epoch 103 Batch   37/41   train_loss = 0.003  valid_loss = 0.893\n",
      "Epoch 104 Batch    6/41   train_loss = 0.004  valid_loss = 0.889\n",
      "Epoch 104 Batch   16/41   train_loss = 0.003  valid_loss = 0.894\n",
      "Epoch 104 Batch   26/41   train_loss = 0.003  valid_loss = 0.885\n",
      "Epoch 104 Batch   36/41   train_loss = 0.003  valid_loss = 0.890\n",
      "Epoch 105 Batch    5/41   train_loss = 0.002  valid_loss = 0.884\n",
      "Epoch 105 Batch   15/41   train_loss = 0.003  valid_loss = 0.894\n",
      "Epoch 105 Batch   25/41   train_loss = 0.002  valid_loss = 0.885\n",
      "Epoch 105 Batch   35/41   train_loss = 0.003  valid_loss = 0.885\n",
      "Epoch 106 Batch    4/41   train_loss = 0.003  valid_loss = 0.884\n",
      "Epoch 106 Batch   14/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 106 Batch   24/41   train_loss = 0.003  valid_loss = 0.884\n",
      "Epoch 106 Batch   34/41   train_loss = 0.003  valid_loss = 0.884\n",
      "Epoch 107 Batch    3/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 107 Batch   13/41   train_loss = 0.004  valid_loss = 0.886\n",
      "Epoch 107 Batch   23/41   train_loss = 0.003  valid_loss = 0.884\n",
      "Epoch 107 Batch   33/41   train_loss = 0.002  valid_loss = 0.881\n",
      "Epoch 108 Batch    2/41   train_loss = 0.002  valid_loss = 0.887\n",
      "Epoch 108 Batch   12/41   train_loss = 0.004  valid_loss = 0.884\n",
      "Epoch 108 Batch   22/41   train_loss = 0.007  valid_loss = 0.892\n",
      "Epoch 108 Batch   32/41   train_loss = 0.005  valid_loss = 0.892\n",
      "Epoch 109 Batch    1/41   train_loss = 0.007  valid_loss = 0.894\n",
      "Epoch 109 Batch   11/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 109 Batch   21/41   train_loss = 0.003  valid_loss = 0.898\n",
      "Epoch 109 Batch   31/41   train_loss = 0.005  valid_loss = 0.904\n",
      "Epoch 110 Batch    0/41   train_loss = 0.004  valid_loss = 0.898\n",
      "Epoch 110 Batch   10/41   train_loss = 0.003  valid_loss = 0.887\n",
      "Epoch 110 Batch   20/41   train_loss = 0.003  valid_loss = 0.894\n",
      "Epoch 110 Batch   30/41   train_loss = 0.002  valid_loss = 0.891\n",
      "Epoch 110 Batch   40/41   train_loss = 0.002  valid_loss = 0.899\n",
      "Epoch 111 Batch    9/41   train_loss = 0.005  valid_loss = 0.902\n",
      "Epoch 111 Batch   19/41   train_loss = 0.006  valid_loss = 0.894\n",
      "Epoch 111 Batch   29/41   train_loss = 0.002  valid_loss = 0.883\n",
      "Epoch 111 Batch   39/41   train_loss = 0.002  valid_loss = 0.885\n",
      "Epoch 112 Batch    8/41   train_loss = 0.003  valid_loss = 0.880\n",
      "Epoch 112 Batch   18/41   train_loss = 0.003  valid_loss = 0.901\n",
      "Epoch 112 Batch   28/41   train_loss = 0.005  valid_loss = 0.888\n",
      "Epoch 112 Batch   38/41   train_loss = 0.004  valid_loss = 0.881\n",
      "Epoch 113 Batch    7/41   train_loss = 0.003  valid_loss = 0.880\n",
      "Epoch 113 Batch   17/41   train_loss = 0.002  valid_loss = 0.880\n",
      "Epoch 113 Batch   27/41   train_loss = 0.004  valid_loss = 0.879\n",
      "Epoch 113 Batch   37/41   train_loss = 0.002  valid_loss = 0.880\n",
      "Epoch 114 Batch    6/41   train_loss = 0.002  valid_loss = 0.880\n",
      "Epoch 114 Batch   16/41   train_loss = 0.003  valid_loss = 0.880\n",
      "Epoch 114 Batch   26/41   train_loss = 0.006  valid_loss = 0.889\n",
      "Epoch 114 Batch   36/41   train_loss = 0.003  valid_loss = 0.878\n",
      "Epoch 115 Batch    5/41   train_loss = 0.002  valid_loss = 0.880\n",
      "Epoch 115 Batch   15/41   train_loss = 0.002  valid_loss = 0.882\n",
      "Epoch 115 Batch   25/41   train_loss = 0.003  valid_loss = 0.896\n",
      "Epoch 115 Batch   35/41   train_loss = 0.006  valid_loss = 0.909\n",
      "Epoch 116 Batch    4/41   train_loss = 0.012  valid_loss = 0.902\n",
      "Epoch 116 Batch   14/41   train_loss = 0.005  valid_loss = 0.893\n",
      "Epoch 116 Batch   24/41   train_loss = 0.003  valid_loss = 0.895\n",
      "Epoch 116 Batch   34/41   train_loss = 0.002  valid_loss = 0.902\n",
      "Epoch 117 Batch    3/41   train_loss = 0.004  valid_loss = 0.908\n",
      "Epoch 117 Batch   13/41   train_loss = 0.004  valid_loss = 0.906\n",
      "Epoch 117 Batch   23/41   train_loss = 0.003  valid_loss = 0.898\n",
      "Epoch 117 Batch   33/41   train_loss = 0.003  valid_loss = 0.875\n",
      "Epoch 118 Batch    2/41   train_loss = 0.002  valid_loss = 0.879\n",
      "Epoch 118 Batch   12/41   train_loss = 0.003  valid_loss = 0.872\n",
      "Epoch 118 Batch   22/41   train_loss = 0.003  valid_loss = 0.882\n",
      "Epoch 118 Batch   32/41   train_loss = 0.002  valid_loss = 0.870\n",
      "Epoch 119 Batch    1/41   train_loss = 0.006  valid_loss = 0.867\n",
      "Epoch 119 Batch   11/41   train_loss = 0.010  valid_loss = 0.878\n",
      "Epoch 119 Batch   21/41   train_loss = 0.009  valid_loss = 0.866\n",
      "Epoch 119 Batch   31/41   train_loss = 0.015  valid_loss = 0.873\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb6e20ac1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_index=int(0.8*len(int_text))\n",
    "train_source = int_text[:split_index]\n",
    "valid_source = int_text[split_index:]\n",
    "\n",
    "train_label= int_label[:split_index]\n",
    "valid_label= int_label[split_index:]\n",
    "\n",
    "#print(len(valid_label))\n",
    "train_batches = get_batches(train_source,train_label, batch_size, seq_length)\n",
    "valid_batches=get_batches(valid_source,valid_label, len(valid_label)//seq_length, seq_length)\n",
    "valid_x=valid_batches[0][0]\n",
    "valid_y=valid_batches[0][1]\n",
    "#print(valid_x)\n",
    "#print(valid_y)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_train_loss=[]\n",
    "    batch_valid_loss=[]\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(train_batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                lr: learning_rate,\n",
    "                keep_prob:0.5}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            batch_train_loss.append(train_loss)\n",
    "                                    \n",
    "           \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(train_batches) + batch_i) % show_every_n_batches == 0:\n",
    "                feed_valid={\n",
    "                    input_text: valid_x,\\\n",
    "                    keep_prob:1,\\\n",
    "                    targets: valid_y}\n",
    "                    \n",
    "                valid_loss,= sess.run([cost], feed_valid)\n",
    "                [batch_valid_loss.append(valid_loss) for i in range(show_every_n_batches)]\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}  valid_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(train_batches),\n",
    "                    train_loss,\n",
    "                    valid_loss))\n",
    "                \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    \n",
    "    plt.plot(batch_train_loss, label='train loss')\n",
    "    plt.plot(batch_valid_loss, label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restore the data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab= pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))\n",
    "token_original,len_original,token_obfused,len_obfused=pickle.load(open('twotaledumped.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, ProbsTensor, KeepprobTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return loaded_graph.get_tensor_by_name(\"input:0\"), \\\n",
    "           loaded_graph.get_tensor_by_name(\"probs:0\"),\\\n",
    "           loaded_graph.get_tensor_by_name(\"keep_prob:0\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_a_the(x_text,y_text,vocab_to_int):\n",
    "    theid=vocab_to_int[\"the\"]\n",
    "    aid=vocab_to_int[\"a\"]\n",
    "    total=0\n",
    "    mix=0\n",
    "    for i,word in enumerate(x_text):\n",
    "        if word==theid or word==aid:\n",
    "            total+=1\n",
    "            if x_text[i]!=y_text[i]:\n",
    "                mix+=1\n",
    "    return mix,total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the original novel \"A Tale of Two Cities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Proceeding 1/5809\n",
      "Proceeding 11/5809\n",
      "Proceeding 21/5809\n",
      "Proceeding 31/5809\n",
      "Proceeding 41/5809\n",
      "Proceeding 51/5809\n",
      "Proceeding 61/5809\n",
      "Proceeding 71/5809\n",
      "Proceeding 81/5809\n",
      "Proceeding 91/5809\n",
      "Proceeding 101/5809\n",
      "Proceeding 111/5809\n",
      "Proceeding 121/5809\n",
      "Proceeding 131/5809\n",
      "Proceeding 141/5809\n",
      "Proceeding 151/5809\n",
      "Proceeding 161/5809\n",
      "Proceeding 171/5809\n",
      "Proceeding 181/5809\n",
      "Proceeding 191/5809\n",
      "Proceeding 201/5809\n",
      "Proceeding 211/5809\n",
      "Proceeding 221/5809\n",
      "Proceeding 231/5809\n",
      "Proceeding 241/5809\n",
      "Proceeding 251/5809\n",
      "Proceeding 261/5809\n",
      "Proceeding 271/5809\n",
      "Proceeding 281/5809\n",
      "Proceeding 291/5809\n",
      "Proceeding 301/5809\n",
      "Proceeding 311/5809\n",
      "Proceeding 321/5809\n",
      "Proceeding 331/5809\n",
      "Proceeding 341/5809\n",
      "Proceeding 351/5809\n",
      "Proceeding 361/5809\n",
      "Proceeding 371/5809\n",
      "Proceeding 381/5809\n",
      "Proceeding 391/5809\n",
      "Proceeding 401/5809\n",
      "Proceeding 411/5809\n",
      "Proceeding 421/5809\n",
      "Proceeding 431/5809\n",
      "Proceeding 441/5809\n",
      "Proceeding 451/5809\n",
      "Proceeding 461/5809\n",
      "Proceeding 471/5809\n",
      "Proceeding 481/5809\n",
      "Proceeding 491/5809\n",
      "Proceeding 501/5809\n",
      "Proceeding 511/5809\n",
      "Proceeding 521/5809\n",
      "Proceeding 531/5809\n",
      "Proceeding 541/5809\n",
      "Proceeding 551/5809\n",
      "Proceeding 561/5809\n",
      "Proceeding 571/5809\n",
      "Proceeding 581/5809\n",
      "Proceeding 591/5809\n",
      "Proceeding 601/5809\n",
      "Proceeding 611/5809\n",
      "Proceeding 621/5809\n",
      "Proceeding 631/5809\n",
      "Proceeding 641/5809\n",
      "Proceeding 651/5809\n",
      "Proceeding 661/5809\n",
      "Proceeding 671/5809\n",
      "Proceeding 681/5809\n",
      "Proceeding 691/5809\n",
      "Proceeding 701/5809\n",
      "Proceeding 711/5809\n",
      "Proceeding 721/5809\n",
      "Proceeding 731/5809\n",
      "Proceeding 741/5809\n",
      "Proceeding 751/5809\n",
      "Proceeding 761/5809\n",
      "Proceeding 771/5809\n",
      "Proceeding 781/5809\n",
      "Proceeding 791/5809\n",
      "Proceeding 801/5809\n",
      "Proceeding 811/5809\n",
      "Proceeding 821/5809\n",
      "Proceeding 831/5809\n",
      "Proceeding 841/5809\n",
      "Proceeding 851/5809\n",
      "Proceeding 861/5809\n",
      "Proceeding 871/5809\n",
      "Proceeding 881/5809\n",
      "Proceeding 891/5809\n",
      "Proceeding 901/5809\n",
      "Proceeding 911/5809\n",
      "Proceeding 921/5809\n",
      "Proceeding 931/5809\n",
      "Proceeding 941/5809\n",
      "Proceeding 951/5809\n",
      "Proceeding 961/5809\n",
      "Proceeding 971/5809\n",
      "Proceeding 981/5809\n",
      "Proceeding 991/5809\n",
      "Proceeding 1001/5809\n",
      "Proceeding 1011/5809\n",
      "Proceeding 1021/5809\n",
      "Proceeding 1031/5809\n",
      "Proceeding 1041/5809\n",
      "Proceeding 1051/5809\n",
      "Proceeding 1061/5809\n",
      "Proceeding 1071/5809\n",
      "Proceeding 1081/5809\n",
      "Proceeding 1091/5809\n",
      "Proceeding 1101/5809\n",
      "Proceeding 1111/5809\n",
      "Proceeding 1121/5809\n",
      "Proceeding 1131/5809\n",
      "Proceeding 1141/5809\n",
      "Proceeding 1151/5809\n",
      "Proceeding 1161/5809\n",
      "Proceeding 1171/5809\n",
      "Proceeding 1181/5809\n",
      "Proceeding 1191/5809\n",
      "Proceeding 1201/5809\n",
      "Proceeding 1211/5809\n",
      "Proceeding 1221/5809\n",
      "Proceeding 1231/5809\n",
      "Proceeding 1241/5809\n",
      "Proceeding 1251/5809\n",
      "Proceeding 1261/5809\n",
      "Proceeding 1271/5809\n",
      "Proceeding 1281/5809\n",
      "Proceeding 1291/5809\n",
      "Proceeding 1301/5809\n",
      "Proceeding 1311/5809\n",
      "Proceeding 1321/5809\n",
      "Proceeding 1331/5809\n",
      "Proceeding 1341/5809\n",
      "Proceeding 1351/5809\n",
      "Proceeding 1361/5809\n",
      "Proceeding 1371/5809\n",
      "Proceeding 1381/5809\n",
      "Proceeding 1391/5809\n",
      "Proceeding 1401/5809\n",
      "Proceeding 1411/5809\n",
      "Proceeding 1421/5809\n",
      "Proceeding 1431/5809\n",
      "Proceeding 1441/5809\n",
      "Proceeding 1451/5809\n",
      "Proceeding 1461/5809\n",
      "Proceeding 1471/5809\n",
      "Proceeding 1481/5809\n",
      "Proceeding 1491/5809\n",
      "Proceeding 1501/5809\n",
      "Proceeding 1511/5809\n",
      "Proceeding 1521/5809\n",
      "Proceeding 1531/5809\n",
      "Proceeding 1541/5809\n",
      "Proceeding 1551/5809\n",
      "Proceeding 1561/5809\n",
      "Proceeding 1571/5809\n",
      "Proceeding 1581/5809\n",
      "Proceeding 1591/5809\n",
      "Proceeding 1601/5809\n",
      "Proceeding 1611/5809\n",
      "Proceeding 1621/5809\n",
      "Proceeding 1631/5809\n",
      "Proceeding 1641/5809\n",
      "Proceeding 1651/5809\n",
      "Proceeding 1661/5809\n",
      "Proceeding 1671/5809\n",
      "Proceeding 1681/5809\n",
      "Proceeding 1691/5809\n",
      "Proceeding 1701/5809\n",
      "Proceeding 1711/5809\n",
      "Proceeding 1721/5809\n",
      "Proceeding 1731/5809\n",
      "Proceeding 1741/5809\n",
      "Proceeding 1751/5809\n",
      "Proceeding 1761/5809\n",
      "Proceeding 1771/5809\n",
      "Proceeding 1781/5809\n",
      "Proceeding 1791/5809\n",
      "Proceeding 1801/5809\n",
      "Proceeding 1811/5809\n",
      "Proceeding 1821/5809\n",
      "Proceeding 1831/5809\n",
      "Proceeding 1841/5809\n",
      "Proceeding 1851/5809\n",
      "Proceeding 1861/5809\n",
      "Proceeding 1871/5809\n",
      "Proceeding 1881/5809\n",
      "Proceeding 1891/5809\n",
      "Proceeding 1901/5809\n",
      "Proceeding 1911/5809\n",
      "Proceeding 1921/5809\n",
      "Proceeding 1931/5809\n",
      "Proceeding 1941/5809\n",
      "Proceeding 1951/5809\n",
      "Proceeding 1961/5809\n",
      "Proceeding 1971/5809\n",
      "Proceeding 1981/5809\n",
      "Proceeding 1991/5809\n",
      "Proceeding 2001/5809\n",
      "Proceeding 2011/5809\n",
      "Proceeding 2021/5809\n",
      "Proceeding 2031/5809\n",
      "Proceeding 2041/5809\n",
      "Proceeding 2051/5809\n",
      "Proceeding 2061/5809\n",
      "Proceeding 2071/5809\n",
      "Proceeding 2081/5809\n",
      "Proceeding 2091/5809\n",
      "Proceeding 2101/5809\n",
      "Proceeding 2111/5809\n",
      "Proceeding 2121/5809\n",
      "Proceeding 2131/5809\n",
      "Proceeding 2141/5809\n",
      "Proceeding 2151/5809\n",
      "Proceeding 2161/5809\n",
      "Proceeding 2171/5809\n",
      "Proceeding 2181/5809\n",
      "Proceeding 2191/5809\n",
      "Proceeding 2201/5809\n",
      "Proceeding 2211/5809\n",
      "Proceeding 2221/5809\n",
      "Proceeding 2231/5809\n",
      "Proceeding 2241/5809\n",
      "Proceeding 2251/5809\n",
      "Proceeding 2261/5809\n",
      "Proceeding 2271/5809\n",
      "Proceeding 2281/5809\n",
      "Proceeding 2291/5809\n",
      "Proceeding 2301/5809\n",
      "Proceeding 2311/5809\n",
      "Proceeding 2321/5809\n",
      "Proceeding 2331/5809\n",
      "Proceeding 2341/5809\n",
      "Proceeding 2351/5809\n",
      "Proceeding 2361/5809\n",
      "Proceeding 2371/5809\n",
      "Proceeding 2381/5809\n",
      "Proceeding 2391/5809\n",
      "Proceeding 2401/5809\n",
      "Proceeding 2411/5809\n",
      "Proceeding 2421/5809\n",
      "Proceeding 2431/5809\n",
      "Proceeding 2441/5809\n",
      "Proceeding 2451/5809\n",
      "Proceeding 2461/5809\n",
      "Proceeding 2471/5809\n",
      "Proceeding 2481/5809\n",
      "Proceeding 2491/5809\n",
      "Proceeding 2501/5809\n",
      "Proceeding 2511/5809\n",
      "Proceeding 2521/5809\n",
      "Proceeding 2531/5809\n",
      "Proceeding 2541/5809\n",
      "Proceeding 2551/5809\n",
      "Proceeding 2561/5809\n",
      "Proceeding 2571/5809\n",
      "Proceeding 2581/5809\n",
      "Proceeding 2591/5809\n",
      "Proceeding 2601/5809\n",
      "Proceeding 2611/5809\n",
      "Proceeding 2621/5809\n",
      "Proceeding 2631/5809\n",
      "Proceeding 2641/5809\n",
      "Proceeding 2651/5809\n",
      "Proceeding 2661/5809\n",
      "Proceeding 2671/5809\n",
      "Proceeding 2681/5809\n",
      "Proceeding 2691/5809\n",
      "Proceeding 2701/5809\n",
      "Proceeding 2711/5809\n",
      "Proceeding 2721/5809\n",
      "Proceeding 2731/5809\n",
      "Proceeding 2741/5809\n",
      "Proceeding 2751/5809\n",
      "Proceeding 2761/5809\n",
      "Proceeding 2771/5809\n",
      "Proceeding 2781/5809\n",
      "Proceeding 2791/5809\n",
      "Proceeding 2801/5809\n",
      "Proceeding 2811/5809\n",
      "Proceeding 2821/5809\n",
      "Proceeding 2831/5809\n",
      "Proceeding 2841/5809\n",
      "Proceeding 2851/5809\n",
      "Proceeding 2861/5809\n",
      "Proceeding 2871/5809\n",
      "Proceeding 2881/5809\n",
      "Proceeding 2891/5809\n",
      "Proceeding 2901/5809\n",
      "Proceeding 2911/5809\n",
      "Proceeding 2921/5809\n",
      "Proceeding 2931/5809\n",
      "Proceeding 2941/5809\n",
      "Proceeding 2951/5809\n",
      "Proceeding 2961/5809\n",
      "Proceeding 2971/5809\n",
      "Proceeding 2981/5809\n",
      "Proceeding 2991/5809\n",
      "Proceeding 3001/5809\n",
      "Proceeding 3011/5809\n",
      "Proceeding 3021/5809\n",
      "Proceeding 3031/5809\n",
      "Proceeding 3041/5809\n",
      "Proceeding 3051/5809\n",
      "Proceeding 3061/5809\n",
      "Proceeding 3071/5809\n",
      "Proceeding 3081/5809\n",
      "Proceeding 3091/5809\n",
      "Proceeding 3101/5809\n",
      "Proceeding 3111/5809\n",
      "Proceeding 3121/5809\n",
      "Proceeding 3131/5809\n",
      "Proceeding 3141/5809\n",
      "Proceeding 3151/5809\n",
      "Proceeding 3161/5809\n",
      "Proceeding 3171/5809\n",
      "Proceeding 3181/5809\n",
      "Proceeding 3191/5809\n",
      "Proceeding 3201/5809\n",
      "Proceeding 3211/5809\n",
      "Proceeding 3221/5809\n",
      "Proceeding 3231/5809\n",
      "Proceeding 3241/5809\n",
      "Proceeding 3251/5809\n",
      "Proceeding 3261/5809\n",
      "Proceeding 3271/5809\n",
      "Proceeding 3281/5809\n",
      "Proceeding 3291/5809\n",
      "Proceeding 3301/5809\n",
      "Proceeding 3311/5809\n",
      "Proceeding 3321/5809\n",
      "Proceeding 3331/5809\n",
      "Proceeding 3341/5809\n",
      "Proceeding 3351/5809\n",
      "Proceeding 3361/5809\n",
      "Proceeding 3371/5809\n",
      "Proceeding 3381/5809\n",
      "Proceeding 3391/5809\n",
      "Proceeding 3401/5809\n",
      "Proceeding 3411/5809\n",
      "Proceeding 3421/5809\n",
      "Proceeding 3431/5809\n",
      "Proceeding 3441/5809\n",
      "Proceeding 3451/5809\n",
      "Proceeding 3461/5809\n",
      "Proceeding 3471/5809\n",
      "Proceeding 3481/5809\n",
      "Proceeding 3491/5809\n",
      "Proceeding 3501/5809\n",
      "Proceeding 3511/5809\n",
      "Proceeding 3521/5809\n",
      "Proceeding 3531/5809\n",
      "Proceeding 3541/5809\n",
      "Proceeding 3551/5809\n",
      "Proceeding 3561/5809\n",
      "Proceeding 3571/5809\n",
      "Proceeding 3581/5809\n",
      "Proceeding 3591/5809\n",
      "Proceeding 3601/5809\n",
      "Proceeding 3611/5809\n",
      "Proceeding 3621/5809\n",
      "Proceeding 3631/5809\n",
      "Proceeding 3641/5809\n",
      "Proceeding 3651/5809\n",
      "Proceeding 3661/5809\n",
      "Proceeding 3671/5809\n",
      "Proceeding 3681/5809\n",
      "Proceeding 3691/5809\n",
      "Proceeding 3701/5809\n",
      "Proceeding 3711/5809\n",
      "Proceeding 3721/5809\n",
      "Proceeding 3731/5809\n",
      "Proceeding 3741/5809\n",
      "Proceeding 3751/5809\n",
      "Proceeding 3761/5809\n",
      "Proceeding 3771/5809\n",
      "Proceeding 3781/5809\n",
      "Proceeding 3791/5809\n",
      "Proceeding 3801/5809\n",
      "Proceeding 3811/5809\n",
      "Proceeding 3821/5809\n",
      "Proceeding 3831/5809\n",
      "Proceeding 3841/5809\n",
      "Proceeding 3851/5809\n",
      "Proceeding 3861/5809\n",
      "Proceeding 3871/5809\n",
      "Proceeding 3881/5809\n",
      "Proceeding 3891/5809\n",
      "Proceeding 3901/5809\n",
      "Proceeding 3911/5809\n",
      "Proceeding 3921/5809\n",
      "Proceeding 3931/5809\n",
      "Proceeding 3941/5809\n",
      "Proceeding 3951/5809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding 3961/5809\n",
      "Proceeding 3971/5809\n",
      "Proceeding 3981/5809\n",
      "Proceeding 3991/5809\n",
      "Proceeding 4001/5809\n",
      "Proceeding 4011/5809\n",
      "Proceeding 4021/5809\n",
      "Proceeding 4031/5809\n",
      "Proceeding 4041/5809\n",
      "Proceeding 4051/5809\n",
      "Proceeding 4061/5809\n",
      "Proceeding 4071/5809\n",
      "Proceeding 4081/5809\n",
      "Proceeding 4091/5809\n",
      "Proceeding 4101/5809\n",
      "Proceeding 4111/5809\n",
      "Proceeding 4121/5809\n",
      "Proceeding 4131/5809\n",
      "Proceeding 4141/5809\n",
      "Proceeding 4151/5809\n",
      "Proceeding 4161/5809\n",
      "Proceeding 4171/5809\n",
      "Proceeding 4181/5809\n",
      "Proceeding 4191/5809\n",
      "Proceeding 4201/5809\n",
      "Proceeding 4211/5809\n",
      "Proceeding 4221/5809\n",
      "Proceeding 4231/5809\n",
      "Proceeding 4241/5809\n",
      "Proceeding 4251/5809\n",
      "Proceeding 4261/5809\n",
      "Proceeding 4271/5809\n",
      "Proceeding 4281/5809\n",
      "Proceeding 4291/5809\n",
      "Proceeding 4301/5809\n",
      "Proceeding 4311/5809\n",
      "Proceeding 4321/5809\n",
      "Proceeding 4331/5809\n",
      "Proceeding 4341/5809\n",
      "Proceeding 4351/5809\n",
      "Proceeding 4361/5809\n",
      "Proceeding 4371/5809\n",
      "Proceeding 4381/5809\n",
      "Proceeding 4391/5809\n",
      "Proceeding 4401/5809\n",
      "Proceeding 4411/5809\n",
      "Proceeding 4421/5809\n",
      "Proceeding 4431/5809\n",
      "Proceeding 4441/5809\n",
      "Proceeding 4451/5809\n",
      "Proceeding 4461/5809\n",
      "Proceeding 4471/5809\n",
      "Proceeding 4481/5809\n",
      "Proceeding 4491/5809\n",
      "Proceeding 4501/5809\n",
      "Proceeding 4511/5809\n",
      "Proceeding 4521/5809\n",
      "Proceeding 4531/5809\n",
      "Proceeding 4541/5809\n",
      "Proceeding 4551/5809\n",
      "Proceeding 4561/5809\n",
      "Proceeding 4571/5809\n",
      "Proceeding 4581/5809\n",
      "Proceeding 4591/5809\n",
      "Proceeding 4601/5809\n",
      "Proceeding 4611/5809\n",
      "Proceeding 4621/5809\n",
      "Proceeding 4631/5809\n",
      "Proceeding 4641/5809\n",
      "Proceeding 4651/5809\n",
      "Proceeding 4661/5809\n",
      "Proceeding 4671/5809\n",
      "Proceeding 4681/5809\n",
      "Proceeding 4691/5809\n",
      "Proceeding 4701/5809\n",
      "Proceeding 4711/5809\n",
      "Proceeding 4721/5809\n",
      "Proceeding 4731/5809\n",
      "Proceeding 4741/5809\n",
      "Proceeding 4751/5809\n",
      "Proceeding 4761/5809\n",
      "Proceeding 4771/5809\n",
      "Proceeding 4781/5809\n",
      "Proceeding 4791/5809\n",
      "Proceeding 4801/5809\n",
      "Proceeding 4811/5809\n",
      "Proceeding 4821/5809\n",
      "Proceeding 4831/5809\n",
      "Proceeding 4841/5809\n",
      "Proceeding 4851/5809\n",
      "Proceeding 4861/5809\n",
      "Proceeding 4871/5809\n",
      "Proceeding 4881/5809\n",
      "Proceeding 4891/5809\n",
      "Proceeding 4901/5809\n",
      "Proceeding 4911/5809\n",
      "Proceeding 4921/5809\n",
      "Proceeding 4931/5809\n",
      "Proceeding 4941/5809\n",
      "Proceeding 4951/5809\n",
      "Proceeding 4961/5809\n",
      "Proceeding 4971/5809\n",
      "Proceeding 4981/5809\n",
      "Proceeding 4991/5809\n",
      "Proceeding 5001/5809\n",
      "Proceeding 5011/5809\n",
      "Proceeding 5021/5809\n",
      "Proceeding 5031/5809\n",
      "Proceeding 5041/5809\n",
      "Proceeding 5051/5809\n",
      "Proceeding 5061/5809\n",
      "Proceeding 5071/5809\n",
      "Proceeding 5081/5809\n",
      "Proceeding 5091/5809\n",
      "Proceeding 5101/5809\n",
      "Proceeding 5111/5809\n",
      "Proceeding 5121/5809\n",
      "Proceeding 5131/5809\n",
      "Proceeding 5141/5809\n",
      "Proceeding 5151/5809\n",
      "Proceeding 5161/5809\n",
      "Proceeding 5171/5809\n",
      "Proceeding 5181/5809\n",
      "Proceeding 5191/5809\n",
      "Proceeding 5201/5809\n",
      "Proceeding 5211/5809\n",
      "Proceeding 5221/5809\n",
      "Proceeding 5231/5809\n",
      "Proceeding 5241/5809\n",
      "Proceeding 5251/5809\n",
      "Proceeding 5261/5809\n",
      "Proceeding 5271/5809\n",
      "Proceeding 5281/5809\n",
      "Proceeding 5291/5809\n",
      "Proceeding 5301/5809\n",
      "Proceeding 5311/5809\n",
      "Proceeding 5321/5809\n",
      "Proceeding 5331/5809\n",
      "Proceeding 5341/5809\n",
      "Proceeding 5351/5809\n",
      "Proceeding 5361/5809\n",
      "Proceeding 5371/5809\n",
      "Proceeding 5381/5809\n",
      "Proceeding 5391/5809\n",
      "Proceeding 5401/5809\n",
      "Proceeding 5411/5809\n",
      "Proceeding 5421/5809\n",
      "Proceeding 5431/5809\n",
      "Proceeding 5441/5809\n",
      "Proceeding 5451/5809\n",
      "Proceeding 5461/5809\n",
      "Proceeding 5471/5809\n",
      "Proceeding 5481/5809\n",
      "Proceeding 5491/5809\n",
      "Proceeding 5501/5809\n",
      "Proceeding 5511/5809\n",
      "Proceeding 5521/5809\n",
      "Proceeding 5531/5809\n",
      "Proceeding 5541/5809\n",
      "Proceeding 5551/5809\n",
      "Proceeding 5561/5809\n",
      "Proceeding 5571/5809\n",
      "Proceeding 5581/5809\n",
      "Proceeding 5591/5809\n",
      "Proceeding 5601/5809\n",
      "Proceeding 5611/5809\n",
      "Proceeding 5621/5809\n",
      "Proceeding 5631/5809\n",
      "Proceeding 5641/5809\n",
      "Proceeding 5651/5809\n",
      "Proceeding 5661/5809\n",
      "Proceeding 5671/5809\n",
      "Proceeding 5681/5809\n",
      "Proceeding 5691/5809\n",
      "Proceeding 5701/5809\n",
      "Proceeding 5711/5809\n",
      "Proceeding 5721/5809\n",
      "Proceeding 5731/5809\n",
      "Proceeding 5741/5809\n",
      "Proceeding 5751/5809\n",
      "Proceeding 5761/5809\n",
      "Proceeding 5771/5809\n",
      "Proceeding 5781/5809\n",
      "Proceeding 5791/5809\n",
      "Proceeding 5801/5809\n",
      "0.7345013477088949\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, probs,keep_prob = get_tensors(loaded_graph)\n",
    "        \n",
    "    mixnum=0\n",
    "    totalnum=0\n",
    "    \n",
    "    obfused_words=[]\n",
    "    predicted_words=[]\n",
    "    original_words=[]\n",
    "    \n",
    "    for i,sentence_token in enumerate(token_obfused):\n",
    "        if i%10==0:\n",
    "            print(\"Proceeding {}/{}\".format(i+1,len(token_obfused)))\n",
    "        \n",
    "        input_sentence = [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in sentence_token]\n",
    "        obfused_words.extend(token_obfused[i])\n",
    "        \n",
    "        batch_shell = np.zeros((1, len(input_sentence)))\n",
    "        batch_shell[0] = input_sentence\n",
    "        chatbot_logits = sess.run(probs, {input_text: batch_shell,keep_prob:1})[0]\n",
    "        \n",
    "        original_sentence=  [vocab_to_int.get(word, vocab_to_int[_UNK]) for word in token_original[i]]\n",
    "        predicted_sentence= [i for i in np.argmax(chatbot_logits, 1)]\n",
    "        \n",
    "        original_words.extend(token_original[i])\n",
    "        predicted_words.extend([int_to_vocab[i] for i in predicted_sentence])\n",
    "        \n",
    "        mixathe,totalathe=compare_a_the(original_sentence,predicted_sentence,vocab_to_int)\n",
    "        totalnum+=totalathe\n",
    "        mixnum+=mixathe\n",
    "    print(1-mixnum/totalnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123096\n",
      "123096\n",
      "123096\n"
     ]
    }
   ],
   "source": [
    "print(len(original_words))\n",
    "print(len(obfused_words))\n",
    "print(len(predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results.tsv', 'w', newline='') as csvfile:\n",
    "    mywriter = csv.writer(csvfile, delimiter='\\t')\n",
    "    mywriter.writerow(['Original', 'Obfused', 'Corrected'])\n",
    "    for i in range(len(original_words)):\n",
    "        mywriter.writerow([original_words[i], obfused_words[i], predicted_words[i]])\n",
    "        \n",
    "with open('a_tale_of_two_city_original.txt','w') as f:\n",
    "    for word in original_words:\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        \n",
    "with open('a_tale_of_two_city_disambiguated.txt','w') as f:\n",
    "    for word in predicted_words:\n",
    "        f.write(word)\n",
    "        f.write(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
