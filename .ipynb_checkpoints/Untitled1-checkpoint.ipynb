{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, max_seq_len, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    seq_lens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        sentence=sentence[:max_seq_len]\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        # Padding\n",
    "        data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "        # Store original sequence length\n",
    "        seq_lens.append(len(token_ids))\n",
    "\n",
    "    return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "def process_data(datafile, max_vocab_size,max_seq_len):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, max_seq_len,normalize_digits=True)\n",
    "\n",
    "    return data_as_tokens, seq_lens, vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_token_ids, tar_seq_lens, tar_vocab_dict, tar_rev_vocab_dict = \\\n",
    "        process_data('oliver_twist/original.p', max_vocab_size=8000,max_seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_token_ids = np.zeros(tar_token_ids.shape,dtype=np.int)\n",
    "src_seq_lens=tar_seq_lens.copy()\n",
    "\n",
    "for x in range(tar_token_ids.shape[0]):\n",
    "    for y in range(0, tar_token_ids.shape[1]):\n",
    "        if tar_token_ids[x, y]==6 or tar_token_ids[x, y]==11:\n",
    "            src_token_ids[x,y]=6 if random.random()<0.5 else 11\n",
    "        else:\n",
    "            src_token_ids[x,y]=tar_token_ids[x,y]\n",
    "    tar_token_ids[x,tar_seq_lens[x]]=EOS_ID\n",
    "    tar_seq_lens[x]+=1\n",
    "\n",
    "src_vocab_dict, src_rev_vocab_dict=tar_vocab_dict, tar_rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(en_token_ids, sp_token_ids,\n",
    "    en_seq_lens, sp_seq_len, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split the into train and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "    # Add go token to decoder inputs and create targets\n",
    "    for sentence in sp_token_ids:\n",
    "        decoder_inputs.append(np.array([GO_ID] + list(sentence)))\n",
    "        targets.append(np.array(([GO_ID] + list(sentence))[1:] + [0]))\n",
    "\n",
    "    sp_token_ids = np.array(decoder_inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Splitting index\n",
    "    last_train_index = int(0.8*len(en_token_ids))\n",
    "\n",
    "    train_encoder_inputs = en_token_ids[:last_train_index]\n",
    "    train_decoder_inputs = sp_token_ids[:last_train_index]\n",
    "    train_targets = targets[:last_train_index]\n",
    "    train_en_seq_lens = en_seq_lens[:last_train_index]\n",
    "    train_sp_seq_len = sp_seq_len[:last_train_index]\n",
    "\n",
    "    valid_encoder_inputs = en_token_ids[last_train_index:]\n",
    "    valid_decoder_inputs = sp_token_ids[last_train_index:]\n",
    "    valid_targets = targets[last_train_index:]\n",
    "    valid_en_seq_lens = en_seq_lens[last_train_index:]\n",
    "    valid_sp_seq_len = sp_seq_len[last_train_index:]\n",
    "\n",
    "    print(\"%i training samples and %i validations samples\" % (\n",
    "        len(train_encoder_inputs), len(valid_encoder_inputs)))\n",
    "\n",
    "    return train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "        train_en_seq_lens, train_sp_seq_len, \\\n",
    "        valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "        valid_en_seq_lens, valid_sp_seq_len\n",
    "\n",
    "def generate_epoch(encoder_inputs, decoder_inputs, targets, en_seq_lens, sp_seq_lens,\n",
    "    num_epochs, batch_size):\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        yield generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "            en_seq_lens, sp_seq_lens, batch_size)\n",
    "\n",
    "def generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "    en_seq_lens, sp_seq_lens, batch_size):\n",
    "\n",
    "    data_size = len(encoder_inputs)\n",
    "\n",
    "    num_batches = (data_size // batch_size)\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\n",
    "        yield encoder_inputs[start_index:end_index], \\\n",
    "            decoder_inputs[start_index:end_index], \\\n",
    "            targets[start_index:end_index], \\\n",
    "            en_seq_lens[start_index:end_index], \\\n",
    "            sp_seq_lens[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell(FLAGS, dropout, scope):\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        # Get the cell type\n",
    "        if FLAGS.rnn_unit == 'rnn':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.BasicRNNCell\n",
    "        elif FLAGS.rnn_unit == 'gru':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "        elif FLAGS.rnn_unit == 'lstm':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise Exception(\"Choose a valid RNN unit type.\")\n",
    "\n",
    "        # Single cell\n",
    "        single_cell = rnn_cell_type(FLAGS.num_hidden_units)\n",
    "\n",
    "        # Dropout\n",
    "        single_cell = tf.nn.rnn_cell.DropoutWrapper(single_cell,\n",
    "            output_keep_prob=1-dropout)\n",
    "\n",
    "        # Each state as one cell\n",
    "        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [single_cell] * FLAGS.num_layers)\n",
    "\n",
    "    return stacked_cell\n",
    "\n",
    "def rnn_inputs(FLAGS, input_data, vocab_size, scope):\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_input = tf.get_variable(\"W_input\",\n",
    "            [vocab_size, FLAGS.num_hidden_units])\n",
    "\n",
    "    # embeddings will be shape [input_data dimensions, num_hidden units]\n",
    "    embeddings = tf.nn.embedding_lookup(W_input, input_data)\n",
    "    return embeddings\n",
    "\n",
    "def rnn_softmax(FLAGS, outputs, scope):\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_softmax = tf.get_variable(\"W_softmax\",\n",
    "            [FLAGS.num_hidden_units, FLAGS.sp_vocab_size])\n",
    "        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.sp_vocab_size])\n",
    "\n",
    "    logits = tf.matmul(outputs, W_softmax) + b_softmax\n",
    "    return logits\n",
    "\n",
    "class model(object):\n",
    "\n",
    "    def __init__(self, FLAGS):\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoder_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='encoder_inputs')\n",
    "        self.decoder_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='decoder_inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='targets')\n",
    "        self.en_seq_lens = tf.placeholder(tf.int32, shape=[None, ],\n",
    "            name=\"en_seq_lens\")\n",
    "        self.sp_seq_lens = tf.placeholder(tf.int32, shape=[None, ],\n",
    "            name=\"sp_seq_lens\")\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "\n",
    "            # Encoder RNN cell\n",
    "            self.encoder_stacked_cell = rnn_cell(FLAGS, self.dropout,\n",
    "                scope=scope)\n",
    "\n",
    "            # Embed encoder inputs\n",
    "            W_input = tf.get_variable(\"W_input\",\n",
    "                [FLAGS.en_vocab_size, FLAGS.num_hidden_units])\n",
    "            self.embedded_encoder_inputs = rnn_inputs(FLAGS,\n",
    "                self.encoder_inputs, FLAGS.en_vocab_size, scope=scope)\n",
    "            #initial_state = encoder_stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_encoder_outputs, self.encoder_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.encoder_stacked_cell,\n",
    "                inputs=self.embedded_encoder_inputs,\n",
    "                sequence_length=self.en_seq_lens, time_major=False,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        '''\n",
    "        # Convert to list of tensors\n",
    "        self.encoder_outputs = tf.unpack(self.all_outputs, axis=0) # annotations\n",
    "        self.encoder_state = tf.unpack(self.state, axis=0)\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        self.top_states = [tf.reshape(e, [-1, 1,\n",
    "            self.stacked_cell.output_size]) for e in self.encoder_outputs]\n",
    "        self.attention_states = tf.concat(1, self.top_states)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # Decoder (use last relevant state from encoder as initial state)\n",
    "        self.initial_decoder_state = self.encoder_state[0]\n",
    "\n",
    "        '''\n",
    "\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "\n",
    "            # Initial state is last relevant state from encoder\n",
    "            self.decoder_initial_state = self.encoder_state\n",
    "\n",
    "            # Decoder RNN cell\n",
    "            self.decoder_stacked_cell = rnn_cell(FLAGS, self.dropout,\n",
    "                scope=scope)\n",
    "\n",
    "            # Embed decoder RNN inputs\n",
    "            W_input = tf.get_variable(\"W_input\",\n",
    "                [FLAGS.sp_vocab_size, FLAGS.num_hidden_units])\n",
    "            self.embedded_decoder_inputs = rnn_inputs(FLAGS, self.decoder_inputs,\n",
    "                FLAGS.sp_vocab_size, scope=scope)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_decoder_outputs, self.decoder_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.decoder_stacked_cell,\n",
    "                inputs=self.embedded_decoder_inputs,\n",
    "                sequence_length=self.sp_seq_lens, time_major=False,\n",
    "                initial_state=self.decoder_initial_state)\n",
    "\n",
    "            # Softmax on decoder RNN outputs\n",
    "            W_softmax = tf.get_variable(\"W_softmax\",\n",
    "                [FLAGS.num_hidden_units, FLAGS.sp_vocab_size])\n",
    "            b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.sp_vocab_size])\n",
    "\n",
    "            # Logits\n",
    "            self.decoder_outputs_flat = tf.reshape(self.all_decoder_outputs,\n",
    "                [-1, FLAGS.num_hidden_units])\n",
    "            self.logits_flat = rnn_softmax(FLAGS, self.decoder_outputs_flat,\n",
    "                scope=scope)\n",
    "\n",
    "            # Loss with masking\n",
    "            targets_flat = tf.reshape(self.targets, [-1])\n",
    "            losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                self.logits_flat, targets_flat)\n",
    "            mask = tf.sign(tf.to_float(targets_flat))\n",
    "            masked_losses = mask * losses_flat\n",
    "            masked_losses = tf.reshape(masked_losses,  tf.shape(self.targets))\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(masked_losses, reduction_indices=1))\n",
    "\n",
    "        # Optimization\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        # clip the gradient to avoid vanishing or blowing up gradients\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_optimizer = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_vars))\n",
    "\n",
    "\n",
    "    def step(self, sess, FLAGS, batch_encoder_inputs, batch_decoder_inputs,\n",
    "        batch_targets, batch_en_seq_lens, batch_sp_seq_lens, dropout):\n",
    "\n",
    "        input_feed = {self.encoder_inputs: batch_encoder_inputs,\n",
    "            self.decoder_inputs: batch_decoder_inputs,\n",
    "            self.targets: batch_targets,\n",
    "            self.en_seq_lens: batch_en_seq_lens,\n",
    "            self.sp_seq_lens: batch_sp_seq_lens,\n",
    "            self.dropout: dropout}\n",
    "        output_feed = [self.loss, self.train_optimizer]\n",
    "        outputs = sess.run(output_feed, input_feed)\n",
    "\n",
    "        return outputs[0], outputs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
