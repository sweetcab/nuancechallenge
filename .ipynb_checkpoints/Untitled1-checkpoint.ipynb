{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(R\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\"\n",
    "\n",
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, max_seq_len, normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    seq_lens = []\n",
    "    #max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "    max_len=max_seq_len+1\n",
    "    for sentence in tokenized:\n",
    "        sentence=sentence[:max_seq_len]\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, normalize_digits)\n",
    "        # Padding\n",
    "        data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "        # Store original sequence length\n",
    "        seq_lens.append(len(token_ids))\n",
    "\n",
    "    return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "def process_data(datafile, max_vocab_size,max_seq_len):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict, max_seq_len,normalize_digits=True)\n",
    "\n",
    "    return data_as_tokens, seq_lens, vocab_dict, rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tar_token_ids, tar_seq_lens, tar_vocab_dict, tar_rev_vocab_dict = \\\n",
    "        process_data('test.p', max_vocab_size=5000,max_seq_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_token_ids = np.zeros(tar_token_ids.shape,dtype=np.int)\n",
    "src_seq_lens=tar_seq_lens.copy()\n",
    "\n",
    "for x in range(tar_token_ids.shape[0]):\n",
    "    for y in range(0, tar_token_ids.shape[1]):\n",
    "        if tar_token_ids[x, y]==6 or tar_token_ids[x, y]==11:\n",
    "            src_token_ids[x,y]=6 if random.random()<0.5 else 11\n",
    "        else:\n",
    "            src_token_ids[x,y]=tar_token_ids[x,y]\n",
    "    tar_token_ids[x,tar_seq_lens[x]]=EOS_ID\n",
    "    tar_seq_lens[x]+=1\n",
    "\n",
    "src_vocab_dict, src_rev_vocab_dict=tar_vocab_dict, tar_rev_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(en_token_ids, sp_token_ids,\n",
    "    en_seq_lens, sp_seq_len, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split the into train and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "    # Add go token to decoder inputs and create targets\n",
    "    for sentence in sp_token_ids:\n",
    "        decoder_inputs.append(np.array([GO_ID] + list(sentence)))\n",
    "        targets.append(np.array(([GO_ID] + list(sentence))[1:] + [0]))\n",
    "\n",
    "    sp_token_ids = np.array(decoder_inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Splitting index\n",
    "    last_train_index = int(0.8*len(en_token_ids))\n",
    "\n",
    "    train_encoder_inputs = en_token_ids[:last_train_index]\n",
    "    train_decoder_inputs = sp_token_ids[:last_train_index]\n",
    "    train_targets = targets[:last_train_index]\n",
    "    train_en_seq_lens = en_seq_lens[:last_train_index]\n",
    "    train_sp_seq_len = sp_seq_len[:last_train_index]\n",
    "\n",
    "    valid_encoder_inputs = en_token_ids[last_train_index:]\n",
    "    valid_decoder_inputs = sp_token_ids[last_train_index:]\n",
    "    valid_targets = targets[last_train_index:]\n",
    "    valid_en_seq_lens = en_seq_lens[last_train_index:]\n",
    "    valid_sp_seq_len = sp_seq_len[last_train_index:]\n",
    "\n",
    "    print(\"%i training samples and %i validations samples\" % (\n",
    "        len(train_encoder_inputs), len(valid_encoder_inputs)))\n",
    "\n",
    "    return train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "        train_en_seq_lens, train_sp_seq_len, \\\n",
    "        valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "        valid_en_seq_lens, valid_sp_seq_len\n",
    "\n",
    "def generate_epoch(encoder_inputs, decoder_inputs, targets, en_seq_lens, sp_seq_lens,\n",
    "    num_epochs, batch_size):\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        yield generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "            en_seq_lens, sp_seq_lens, batch_size)\n",
    "\n",
    "def generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "    en_seq_lens, sp_seq_lens, batch_size):\n",
    "\n",
    "    data_size = len(encoder_inputs)\n",
    "\n",
    "    num_batches = (data_size // batch_size)\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\n",
    "        yield encoder_inputs[start_index:end_index], \\\n",
    "            decoder_inputs[start_index:end_index], \\\n",
    "            targets[start_index:end_index], \\\n",
    "            en_seq_lens[start_index:end_index], \\\n",
    "            sp_seq_lens[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rnn_cell(FLAGS, dropout, scope):\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        # Get the cell type\n",
    "        if FLAGS.rnn_unit == 'rnn':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.BasicRNNCell\n",
    "        elif FLAGS.rnn_unit == 'gru':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.GRUCell\n",
    "        elif FLAGS.rnn_unit == 'lstm':\n",
    "            rnn_cell_type = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise Exception(\"Choose a valid RNN unit type.\")\n",
    "\n",
    "        # Single cell\n",
    "        single_cell = rnn_cell_type(FLAGS.num_hidden_units)\n",
    "\n",
    "        # Dropout\n",
    "        single_cell = tf.nn.rnn_cell.DropoutWrapper(single_cell,\n",
    "            output_keep_prob=1-dropout)\n",
    "\n",
    "        # Each state as one cell\n",
    "        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [single_cell] * FLAGS.num_layers)\n",
    "\n",
    "    return stacked_cell\n",
    "\n",
    "def rnn_inputs(FLAGS, input_data, vocab_size, scope):\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_input = tf.get_variable(\"W_input\",\n",
    "            [vocab_size, FLAGS.num_hidden_units])\n",
    "\n",
    "    # embeddings will be shape [input_data dimensions, num_hidden units]\n",
    "    embeddings = tf.nn.embedding_lookup(W_input, input_data)\n",
    "    return embeddings\n",
    "\n",
    "def rnn_softmax(FLAGS, outputs, scope):\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_softmax = tf.get_variable(\"W_softmax\",\n",
    "            [FLAGS.num_hidden_units, FLAGS.tar_vocab_size])\n",
    "        b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.tar_vocab_size])\n",
    "\n",
    "    logits = tf.matmul(outputs, W_softmax) + b_softmax\n",
    "    return logits\n",
    "\n",
    "class model(object):\n",
    "\n",
    "    def __init__(self, FLAGS):\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoder_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='encoder_inputs')\n",
    "        self.decoder_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='decoder_inputs')\n",
    "        self.inference_inputs = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='inference_inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, None],\n",
    "            name='targets')\n",
    "        self.src_seq_lens = tf.placeholder(tf.int32, shape=[None, ],\n",
    "            name=\"src_seq_lens\")\n",
    "        self.tar_seq_lens = tf.placeholder(tf.int32, shape=[None, ],\n",
    "            name=\"tar_seq_lens\")\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "\n",
    "            # Encoder RNN cell\n",
    "            self.encoder_stacked_cell = rnn_cell(FLAGS, self.dropout,\n",
    "                scope=scope)\n",
    "\n",
    "            # Embed encoder inputs\n",
    "            W_input = tf.get_variable(\"W_input\",\n",
    "                [FLAGS.src_vocab_size, FLAGS.num_hidden_units])\n",
    "            self.embedded_encoder_inputs = rnn_inputs(FLAGS,\n",
    "                self.encoder_inputs, FLAGS.src_vocab_size, scope=scope)\n",
    "            #initial_state = encoder_stacked_cell.zero_state(FLAGS.batch_size, tf.float32)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_encoder_outputs, self.encoder_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.encoder_stacked_cell,\n",
    "                inputs=self.embedded_encoder_inputs,\n",
    "                sequence_length=self.src_seq_lens, time_major=False,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        '''\n",
    "        # Convert to list of tensors\n",
    "        self.encoder_outputs = tf.unpack(self.all_outputs, axis=0) # annotations\n",
    "        self.encoder_state = tf.unpack(self.state, axis=0)\n",
    "\n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        self.top_states = [tf.reshape(e, [-1, 1,\n",
    "            self.stacked_cell.output_size]) for e in self.encoder_outputs]\n",
    "        self.attention_states = tf.concat(1, self.top_states)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        # Decoder (use last relevant state from encoder as initial state)\n",
    "        self.initial_decoder_state = self.encoder_state[0]\n",
    "\n",
    "        '''\n",
    "\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "\n",
    "            # Initial state is last relevant state from encoder\n",
    "            self.decoder_initial_state = self.encoder_state\n",
    "\n",
    "            # Decoder RNN cell\n",
    "            self.decoder_stacked_cell = rnn_cell(FLAGS, self.dropout,\n",
    "                scope=scope)\n",
    "\n",
    "            # Embed decoder RNN inputs\n",
    "            W_input = tf.get_variable(\"W_input\",\n",
    "                [FLAGS.tar_vocab_size, FLAGS.num_hidden_units])\n",
    "            self.embedded_decoder_inputs = rnn_inputs(FLAGS, self.decoder_inputs,\n",
    "                FLAGS.tar_vocab_size, scope=scope)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_decoder_outputs, self.decoder_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.decoder_stacked_cell,\n",
    "                inputs=self.embedded_decoder_inputs,\n",
    "                sequence_length=self.tar_seq_lens, time_major=False,\n",
    "                initial_state=self.decoder_initial_state)\n",
    "\n",
    "            # Softmax on decoder RNN outputs\n",
    "            W_softmax = tf.get_variable(\"W_softmax\",\n",
    "                [FLAGS.num_hidden_units, FLAGS.tar_vocab_size])\n",
    "            b_softmax = tf.get_variable(\"b_softmax\", [FLAGS.tar_vocab_size])\n",
    "\n",
    "            # Logits\n",
    "            self.decoder_outputs_flat = tf.reshape(self.all_decoder_outputs,\n",
    "                [-1, FLAGS.num_hidden_units])\n",
    "            self.logits_flat = rnn_softmax(FLAGS, self.decoder_outputs_flat,\n",
    "                scope=scope)\n",
    "\n",
    "            # Loss with masking\n",
    "            targets_flat = tf.reshape(self.targets, [-1])\n",
    "            losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits_flat, labels=targets_flat)\n",
    "            mask = tf.sign(tf.to_float(targets_flat))\n",
    "            masked_losses = mask * losses_flat\n",
    "            masked_losses = tf.reshape(masked_losses,  tf.shape(self.targets))\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(masked_losses, reduction_indices=1))\n",
    "            \n",
    "        with tf.variable_scope('decoder',reuse=True) as scope:\n",
    "\n",
    "            # Initial state is last relevant state from encoder\n",
    "            self.inference_initial_state = self.encoder_state\n",
    "\n",
    "            self.embedded_inference_inputs = rnn_inputs(FLAGS, self.inference_inputs,\n",
    "                FLAGS.tar_vocab_size, scope=scope)\n",
    "\n",
    "            # Outputs from encoder RNN\n",
    "            self.all_inference_outputs, self.inference_state = tf.nn.dynamic_rnn(\n",
    "                cell=self.decoder_stacked_cell,\n",
    "                inputs=self.embedded_inference_inputs,\n",
    "                sequence_length=self.tar_seq_lens, time_major=False,\n",
    "                initial_state=self.inference_initial_state)\n",
    "\n",
    "            # Logits\n",
    "            self.inference_outputs_flat = tf.reshape(self.all_inference_outputs,\n",
    "                [-1, FLAGS.num_hidden_units])\n",
    "            self.inference_logits_flat = rnn_softmax(FLAGS, self.inference_outputs_flat,\n",
    "                scope=scope)\n",
    "\n",
    "        # Optimization\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        # clip the gradient to avoid vanishing or blowing up gradients\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_optimizer = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_vars))\n",
    "\n",
    "\n",
    "    def step(self, sess, FLAGS, batch_encoder_inputs, batch_decoder_inputs,\n",
    "        batch_targets, batch_en_seq_lens, batch_sp_seq_lens, dropout):\n",
    "\n",
    "        input_feed = {self.encoder_inputs: batch_encoder_inputs,\n",
    "            self.decoder_inputs: batch_decoder_inputs,\n",
    "            self.targets: batch_targets,\n",
    "            self.src_seq_lens: batch_en_seq_lens,\n",
    "            self.tar_seq_lens: batch_sp_seq_lens,\n",
    "            self.dropout: dropout}\n",
    "        output_feed = [self.loss, self.train_optimizer]\n",
    "        outputs = sess.run(output_feed, input_feed)\n",
    "\n",
    "        return outputs[0], outputs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameters(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Holds all the parameters for NMT.\n",
    "        \"\"\"\n",
    "        self.ckpt_dir = 'checkpoints/'\n",
    "        \n",
    "        self.max_src_vocab_size = 5000\n",
    "        self.max_tar_vocab_size = 5000\n",
    "\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 4\n",
    "\n",
    "        self.rnn_unit = 'gru'\n",
    "        self.num_hidden_units = 500\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 1e-3\n",
    "        self.learning_rate_decay_factor = 0.99\n",
    "        self.max_gradient_norm = 5.0\n",
    "\n",
    "def create_model(sess, FLAGS):\n",
    "\n",
    "    tf_model = model(FLAGS)\n",
    "    print(\"Created a new model\")\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    return tf_model\n",
    "\n",
    "def train(FLAGS):\n",
    "\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "        train_src_seq_lens, train_tar_seq_lens, \\\n",
    "        valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "        valid_src_seq_lens, valid_tar_seq_len = \\\n",
    "        split_data(src_token_ids, tar_token_ids, src_seq_lens, tar_seq_lens,\n",
    "            train_ratio=0.8)\n",
    "\n",
    "    # Update parameters\n",
    "    FLAGS.src_vocab_size = len(src_vocab_dict)\n",
    "    FLAGS.tar_vocab_size = len(tar_vocab_dict)\n",
    "\n",
    "    # Start session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Create new model or load old one\n",
    "        model = create_model(sess, FLAGS)\n",
    "\n",
    "        # Training begins\n",
    "        losses = []\n",
    "        for epoch_num, epoch in enumerate(generate_epoch(train_encoder_inputs,\n",
    "            train_decoder_inputs, train_targets,\n",
    "            train_src_seq_lens, train_tar_seq_lens,\n",
    "            FLAGS.num_epochs, FLAGS.batch_size)):\n",
    "\n",
    "            print(\"EPOCH: %i\" % (epoch_num))\n",
    "            # Decay learning rate\n",
    "            sess.run(tf.assign(model.lr, FLAGS.learning_rate * \\\n",
    "                (FLAGS.learning_rate_decay_factor ** epoch_num)))\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for batch_num, (batch_encoder_inputs, batch_decoder_inputs,\n",
    "                batch_targets, batch_src_seq_lens,\n",
    "                batch_tar_seq_lens) in enumerate(epoch):\n",
    "\n",
    "                loss, _ = model.step(sess, FLAGS,\n",
    "                    batch_encoder_inputs, batch_decoder_inputs, batch_targets,\n",
    "                    batch_src_seq_lens, batch_tar_seq_lens,\n",
    "                    FLAGS.dropout)\n",
    "\n",
    "                batch_loss.append(loss)\n",
    "            \n",
    "            print(np.mean(batch_loss))\n",
    "            losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Save checkpoint.\n",
    "        if not os.path.isdir(FLAGS.ckpt_dir):\n",
    "            os.makedirs(FLAGS.ckpt_dir)\n",
    "        checkpoint_path = os.path.join(FLAGS.ckpt_dir, \"model.ckpt\")\n",
    "        print(\"Saving the model.\")\n",
    "        model.saver.save(sess, checkpoint_path,\n",
    "                         global_step=model.global_step)\n",
    "        \n",
    "        plt.plot(losses, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 training samples and 4 validations samples\n",
      "Created a new model\n",
      "WARNING:tensorflow:From C:\\Users\\ygao\\Anaconda3\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "EPOCH: 0\n",
      "86.1845\n",
      "EPOCH: 1\n",
      "82.9684\n",
      "EPOCH: 2\n",
      "80.1055\n",
      "EPOCH: 3\n",
      "74.1841\n",
      "EPOCH: 4\n",
      "71.2967\n",
      "EPOCH: 5\n",
      "68.2202\n",
      "EPOCH: 6\n",
      "65.6979\n",
      "EPOCH: 7\n",
      "62.6165\n",
      "EPOCH: 8\n",
      "59.015\n",
      "EPOCH: 9\n",
      "56.5337\n",
      "EPOCH: 10\n",
      "53.9495\n",
      "EPOCH: 11\n",
      "50.341\n",
      "EPOCH: 12\n",
      "48.6295\n",
      "EPOCH: 13\n",
      "45.6077\n",
      "EPOCH: 14\n",
      "43.7991\n",
      "EPOCH: 15\n",
      "39.0413\n",
      "EPOCH: 16\n",
      "38.1672\n",
      "EPOCH: 17\n",
      "35.7174\n",
      "EPOCH: 18\n",
      "34.9555\n",
      "EPOCH: 19\n",
      "31.4957\n",
      "EPOCH: 20\n",
      "31.906\n",
      "EPOCH: 21\n",
      "29.4326\n",
      "EPOCH: 22\n",
      "27.6652\n",
      "EPOCH: 23\n",
      "28.7124\n",
      "EPOCH: 24\n",
      "25.8432\n",
      "EPOCH: 25\n",
      "23.9865\n",
      "EPOCH: 26\n",
      "23.8063\n",
      "EPOCH: 27\n",
      "22.7996\n",
      "EPOCH: 28\n",
      "20.6289\n",
      "EPOCH: 29\n",
      "20.1484\n",
      "EPOCH: 30\n",
      "18.3194\n",
      "EPOCH: 31\n",
      "18.0894\n",
      "EPOCH: 32\n",
      "18.2198\n",
      "EPOCH: 33\n",
      "16.9801\n",
      "EPOCH: 34\n",
      "15.9941\n",
      "EPOCH: 35\n",
      "14.9424\n",
      "EPOCH: 36\n",
      "15.3064\n",
      "EPOCH: 37\n",
      "14.6672\n",
      "EPOCH: 38\n",
      "12.8958\n",
      "EPOCH: 39\n",
      "13.1318\n",
      "EPOCH: 40\n",
      "12.3806\n",
      "EPOCH: 41\n",
      "10.8425\n",
      "EPOCH: 42\n",
      "9.69589\n",
      "EPOCH: 43\n",
      "10.0742\n",
      "EPOCH: 44\n",
      "9.06535\n",
      "EPOCH: 45\n",
      "9.3049\n",
      "EPOCH: 46\n",
      "7.94342\n",
      "EPOCH: 47\n",
      "8.11751\n",
      "EPOCH: 48\n",
      "7.2868\n",
      "EPOCH: 49\n",
      "7.36083\n",
      "EPOCH: 50\n",
      "6.75634\n",
      "EPOCH: 51\n",
      "6.46581\n",
      "EPOCH: 52\n",
      "5.86323\n",
      "EPOCH: 53\n",
      "5.03011\n",
      "EPOCH: 54\n",
      "4.95437\n",
      "EPOCH: 55\n",
      "4.40748\n",
      "EPOCH: 56\n",
      "4.69794\n",
      "EPOCH: 57\n",
      "4.30129\n",
      "EPOCH: 58\n",
      "4.11591\n",
      "EPOCH: 59\n",
      "4.02706\n",
      "EPOCH: 60\n",
      "3.53024\n",
      "EPOCH: 61\n",
      "3.3987\n",
      "EPOCH: 62\n",
      "3.09549\n",
      "EPOCH: 63\n",
      "2.74791\n",
      "EPOCH: 64\n",
      "2.40793\n",
      "EPOCH: 65\n",
      "2.48534\n",
      "EPOCH: 66\n",
      "2.51408\n",
      "EPOCH: 67\n",
      "1.99956\n",
      "EPOCH: 68\n",
      "1.93821\n",
      "EPOCH: 69\n",
      "1.81859\n",
      "EPOCH: 70\n",
      "1.69538\n",
      "EPOCH: 71\n",
      "1.38881\n",
      "EPOCH: 72\n",
      "1.57206\n",
      "EPOCH: 73\n",
      "1.37494\n",
      "EPOCH: 74\n",
      "1.36876\n",
      "EPOCH: 75\n",
      "1.21463\n",
      "EPOCH: 76\n",
      "0.981228\n",
      "EPOCH: 77\n",
      "1.02197\n",
      "EPOCH: 78\n",
      "0.965373\n",
      "EPOCH: 79\n",
      "0.843569\n",
      "EPOCH: 80\n",
      "0.786575\n",
      "EPOCH: 81\n",
      "0.796456\n",
      "EPOCH: 82\n",
      "0.782501\n",
      "EPOCH: 83\n",
      "0.683466\n",
      "EPOCH: 84\n",
      "0.647296\n",
      "EPOCH: 85\n",
      "0.634121\n",
      "EPOCH: 86\n",
      "0.56642\n",
      "EPOCH: 87\n",
      "0.527719\n",
      "EPOCH: 88\n",
      "0.494722\n",
      "EPOCH: 89\n",
      "0.498553\n",
      "EPOCH: 90\n",
      "0.396859\n",
      "EPOCH: 91\n",
      "0.391659\n",
      "EPOCH: 92\n",
      "0.325841\n",
      "EPOCH: 93\n",
      "0.372979\n",
      "EPOCH: 94\n",
      "0.289028\n",
      "EPOCH: 95\n",
      "0.271408\n",
      "EPOCH: 96\n",
      "0.301706\n",
      "EPOCH: 97\n",
      "0.376744\n",
      "EPOCH: 98\n",
      "0.336807\n",
      "EPOCH: 99\n",
      "0.246546\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-578021a88470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-587c420e9b4f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(FLAGS)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Save checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "FLAGS = parameters()\n",
    "train(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(FLAGS):\n",
    "\n",
    "    # Change FLAGS parameters\n",
    "    FLAGS.batch_size = 1\n",
    "    FLAGS.src_vocab_size = len(src_vocab_dict)\n",
    "    FLAGS.tar_vocab_size = len(tar_vocab_dict)\n",
    "    FLAGS.tar_max_len = max(src_seq_lens) + 1 # GO token\n",
    "\n",
    "    # Process sample sentence\n",
    "    inference_sentence = [\"I usually eat the very large salad.\"]\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in range(len(inference_sentence)):\n",
    "        tokenized.append(basic_tokenizer(inference_sentence[i]))\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, sample_src_seq_lens = data_to_token_ids(\n",
    "        tokenized, src_vocab_dict,max_seq_len=30 ,normalize_digits=True)\n",
    "\n",
    "    # make dummy_sp_inputs\n",
    "    dummy_tar_inputs = np.array([[GO_ID]*FLAGS.tar_max_len])\n",
    "    sample_tar_seq_lens = np.array([len(dummy_tar_inputs)])\n",
    "\n",
    "    print(data_as_tokens)\n",
    "    print(sample_src_seq_lens)\n",
    "    print(dummy_tar_inputs)\n",
    "    print(sample_tar_seq_lens)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Load trained model\n",
    "        input_feed = {model.encoder_inputs: data_as_tokens,\n",
    "            model.inference_inputs: dummy_tar_inputs,\n",
    "            model.src_seq_lens: batch_src_seq_lens,\n",
    "            model.tar_seq_lens: batch_tar_seq_lens,\n",
    "            model.dropout: 1}\n",
    "        output_feed = [model.inference_logits_flat]\n",
    "        outputs = sess.run(output_feed, input_feed)\n",
    "        print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
